{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs","text":""},{"location":"Debugging%20Documentation/DEBUGGING/","title":"Debugging Node \u2194 RPG \u2194 IBM i Integrations","text":"<p>This guide documents the moving pieces in this repository and outlines how to diagnose failures anywhere along the NestJS \u2192 Bull/BullMQ queue \u2192 RPG programs \u2192 IBM i spooled output flow.</p>"},{"location":"Debugging%20Documentation/DEBUGGING/#1-nodejs-framework-map","title":"1. Node.js Framework Map","text":"Concern Key Modules / Packages Debugging Tips HTTP &amp; DI framework <code>AppModule</code> wires the NestJS modules for static file serving, config, caching, auth, account-payable flows, Bull, and the websocket gateway. \u3010F:src/app.module.ts\u2020L1-L35\u3011 Run <code>npm run start:debug</code> to launch the Nest dev server with an inspector, then set breakpoints inside the relevant module providers. HTTP platform Nest boots on <code>@nestjs/platform-express</code>, and the app serves static assets via <code>@nestjs/serve-static</code>. Socket connections use <code>@nestjs/platform-socket.io</code>. All platform adapters are declared in <code>package.json</code>. \u3010F:package.json\u2020L8-L78\u3011 If HTTP requests stall, confirm the correct platform adapter is installed (<code>node_modules/@nestjs/platform-express</code>) and that static roots resolve to real paths via <code>ServeStaticModule</code>. Queues Bull (<code>@nestjs/bull</code>, <code>bull</code>) handles classic queues; BullMQ (<code>bullmq</code>) is used for streaming uploads. Redis connection info lives in <code>src/shared/queue/bullmq-connection.ts</code>. \u3010F:package.json\u2020L8-L78\u3011\u3010F:src/shared/queue/bullmq-connection.ts\u2020L1-L4\u3011 Failed jobs show up in processor hooks such as <code>onFailed</code> inside each <code>@Processor</code> class. Attach a debugger to those hooks or query BullMQ using <code>bull-board</code>/<code>redis-cli</code> for stuck jobs. ORM &amp; IBM driver Sequelize (<code>sequelize</code>, <code>sequelize-typescript</code>) is paired with the DB2 for i dialect (<code>@sequelize/db2-ibmi</code>) and the native ODBC binding (<code>odbc</code>). \u3010F:package.json\u2020L39-L78\u3011 Enable verbose SQL logging by running in non-prod (<code>NODE_ENV !== 'prod'</code>) and tailing the <code>Database</code> logger output from <code>customSequelizeLogger</code>. Websocket fan-out Websocket gateway/service broadcast upload summaries to browser clients. \u3010F:src/shared/websocket/websocket.service.ts\u2020L1-L118\u3011 If browsers miss events, confirm the gateway is emitting (<code>emitEvent</code>, <code>emitToUser</code>) and that Redis pub/sub (if enabled) is reachable. Observability <code>AppLogger</code> wraps Nest\u2019s logger with timing helpers for slow operations. \u3010F:src/shared/logger/logger.service.ts\u2020L1-L69\u3011 Call <code>logger.timing()</code>/<code>sharedTiming()</code> around long-running sections to surface delays in development logs."},{"location":"Debugging%20Documentation/DEBUGGING/#2-ibm-i-connectivity-driver-layer","title":"2. IBM i Connectivity &amp; Driver Layer","text":"<ol> <li>Container prerequisites \u2013 The Dockerfile installs unixODBC + IBM i Access ODBC driver, copies <code>odbc.ini</code>, then builds the Nest app. Missing packages or an unreadable <code>odbc.ini</code> are the first things to check when DSN resolution fails. \u3010F:Dockerfile\u2020L1-L49\u3011</li> <li>DSN / ODBC configuration \u2013 The project checks an <code>odbc.ini</code> file (copied into <code>/etc/odbc.ini</code>) that contains a baseline <code>AS400DB</code> definition, including the IBM i system IP, default package library, CCSID translation and tracing flags. \u3010F:src/shared/infrastructure/odbc/odbc.ini\u2020L1-L27\u3011</li> <li>Runtime connection builder \u2013 <code>buildDsn()</code> in <code>connection.ts</code> composes the IBM i Access connection string from <code>DB_SYSTEM</code>, <code>DB_USERNAME</code>, <code>DB_PASSWORD</code>, and package overrides. Logging is enabled by default when <code>NODE_ENV !== 'prod'</code>. \u3010F:src/shared/infrastructure/connection.ts\u2020L71-L134\u3011</li> <li>Pool and retry tuning \u2013 <code>connectAs400Db2IBMi()</code> configures Sequelize with explicit pool sizes, benchmark logging, and retry counts, then races authentication against a 15s timeout so hangs surface quickly. \u3010F:src/shared/infrastructure/connection.ts\u2020L194-L233\u3011</li> <li>Dynamic library resolution \u2013 After connecting, the app always runs the <code>GSSETENV</code> stored procedure via <code>DynamicLibraryManager</code>. It dynamically creates <code>QGPL</code> variables, calls the SP, trims outputs (data/work/sp libraries plus error message), updates process env vars, and drops the temp variables. Failures surface in the <code>DynamicLibraryManager</code> logger, and you can re-run the SP via <code>dynamicLibManager.setDynamicLibraries()</code> in a REPL. \u3010F:src/main/global-states/data/stored-procedure/dynamic-library-manager.ts\u2020L1-L213\u3011\u3010F:src/shared/utils/db.utils.ts\u2020L1-L79\u3011</li> <li>Library map per environment \u2013 <code>env-library-config.ts</code> keeps the default schema per environment. If SQL starts referencing the wrong library, confirm <code>NODE_ENV</code> and the derived <code>DB_*</code> env vars match what <code>DynamicLibraryManager.updateEnvironmentVariables()</code> prints. \u3010F:src/shared/config/env-library-config.ts\u2020L4-L83\u3011</li> </ol>"},{"location":"Debugging%20Documentation/DEBUGGING/#diagnosing-connection-errors","title":"Diagnosing connection errors","text":"Symptom Checklist <code>Database authentication timeout</code> or <code>Failed to connect to the database</code> Ensure the Docker image picked up the IBM driver, verify <code>DB_SYSTEM/DB_USERNAME/DB_PASSWORD</code> in the running container, and re-run <code>connectAs400Db2IBMi()</code> to regenerate logs from <code>customSequelizeLogger</code>. \u3010F:src/shared/infrastructure/connection.ts\u2020L194-L257\u3011 <code>GSSETENV stored procedure failed</code> Query <code>QSYS2.SYSROUTINES</code> to confirm the SP exists (the helper already logs a warning/fallback). Check that the current user has authority to <code>CREATE VARIABLE</code> in QGPL because the manager drops/recreates them each call. \u3010F:src/main/global-states/data/stored-procedure/dynamic-library-manager.ts\u2020L40-L123\u3011 Models missing tables/columns <code>initializeModels()</code> enumerates every table that must be initialized after connection. If a model fails, it will throw before <code>isModelsInitialized</code> flips. Inspect the preceding log statements to see which initializer last ran. \u3010F:src/shared/infrastructure/connection.ts\u2020L136-L191\u3011"},{"location":"Debugging%20Documentation/DEBUGGING/#3-sql-diagnostics-model-issues","title":"3. SQL Diagnostics &amp; Model Issues","text":"<ol> <li>Custom SQL logging \u2013 With <code>NODE_ENV !== 'prod'</code>, <code>customSequelizeLogger</code> emits the full SQL statement and timing at <code>warn</code> level, making it easy to correlate with IBM job logs. \u3010F:src/shared/infrastructure/connection.ts\u2020L117-L134\u3011</li> <li>Dynamic tables \u2013 Utility helpers in <code>db.utils.ts</code> build schema-qualified names, guard against duplicate registration, and drop temp variables. When diagnosing incorrect table names, inspect <code>buildTableName()</code>/<code>initializeModelWithSequelize()</code> to see how prefixes/suffixes are derived. \u3010F:src/shared/utils/db.utils.ts\u2020L1-L79\u3011</li> <li>Library-specific failures \u2013 Compare the resolved schema from <code>getTableAndSchemaInfo()</code> / <code>getDynamicTableAndSchemaInfo()</code> with the job log library list to confirm you are hitting the intended environment libraries. \u3010F:src/shared/config/env-library-config.ts\u2020L30-L83\u3011</li> </ol>"},{"location":"Debugging%20Documentation/DEBUGGING/#4-bullbullmq-job-pipelines","title":"4. Bull/BullMQ Job Pipelines","text":"<ul> <li>Queue wiring \u2013 Each processor (Flexi uploads, SOGAS, Clear Checks, etc.) is decorated with <code>@Processor(QUEUE_NAMES.X)</code> and exposes <code>onActive</code>, <code>onCompleted</code>, and <code>onFailed</code> hooks that log the Bull job lifecycle. For example, <code>FlexiProcessor</code> logs every transition, binds user context, limits parallelism, and records batch summaries back into Redis. \u3010F:src/main/account-payable/application/voucher/shared-services/flexi.processor.ts\u2020L1-L175\u3011</li> <li>Redis connection \u2013 <code>redis_connection</code> simply reads <code>REDIS_HOST</code>/<code>REDIS_PORT</code>. If every job immediately fails with <code>ECONNREFUSED</code>, inspect those env vars and confirm Redis is reachable. \u3010F:src/shared/queue/bullmq-connection.ts\u2020L1-L4\u3011</li> <li>Websocket fan-out \u2013 Job processors push interim/final summaries through <code>WebsocketService</code>, which calculates per-status totals and emits user-scoped events (<code>emitUploadStatusToUser</code>). Missing UI updates usually mean the <code>UserContext</code> lacks initials or the gateway is not subscribed. \u3010F:src/shared/websocket/websocket.service.ts\u2020L1-L118\u3011</li> <li>Debug strategy \u2013 Attach a debugger to the <code>handle()</code> method of the relevant processor, replay the job data (from Redis or API logs), and step through <code>validateHeader</code>, <code>saveVoucherData</code>, and the DB writes. Because the processors log invoice numbers, you can cross-reference IBM joblogs for the same invoice ID.</li> </ul>"},{"location":"Debugging%20Documentation/DEBUGGING/#5-spooled-file-output-queue-diagnostics","title":"5. Spooled File &amp; Output Queue Diagnostics","text":"<ol> <li>Metadata captured per report \u2013 The <code>SpooledMetaDataReportEntity</code> stores spool/PDF filenames, report type, job identifiers, output queue name/library, file path, creation timestamp, form type, and any RPG error string. This lets you correlate Node errors with IBM job output queue entries. \u3010F:src/main/account-payable/domain/entities/spooled-meta-data-report.entity.ts\u2020L1-L17\u3011</li> <li>Schema mapping \u2013 <code>SpooledMetadataReportModelSchema</code> maps each property to its IBM i physical file column (<code>MEJOBNM</code>, <code>MEOQNM</code>, etc.). If fields are blank in the API, inspect this mapping to verify the RPG program actually populated the column. \u3010F:src/main/account-payable/data/schemas/schema.ts\u2020L2296-L2339\u3011</li> <li>Default spool config \u2013 <code>spooledReportMetaData</code> defines default spool file name (<code>APREPORT</code>), job user (<code>QUSER</code>), and output queue/library (<code>DAMCOOUTQ</code> / <code>QUSRSYS</code>) used when generating files (e.g., ACH, IRS, 1099 reports). Override these values when testing non-production queues. \u3010F:src/shared/config/generate-report-config.ts\u2020L57-L127\u3011</li> <li>Repository queries \u2013 <code>SpooledMetaDataReportsRepository</code> exposes <code>SpooledMetadataReports()</code> to paginate by <code>reportType</code>, wildcard filename, and date range; it also formats the timestamps/file paths for the UI. Use this repository to confirm whether the IBM job created entries and whether <code>reportsFormatter</code> built a valid download path. \u3010F:src/main/account-payable/data/repositories/spooled-meta-data-reports.repository.ts\u2020L1-L107\u3011\u3010F:src/shared/formatters/dropdown.formatter.ts\u2020L1-L66\u3011</li> <li>Job correlation fields \u2013 Because the schema persists <code>jobName</code>, <code>jobNumber</code>, <code>jobUser</code>, <code>jobSystemName</code>, and <code>error</code>, you can filter the metadata table (via SQL or the repository) for a specific failing IBM job, then jump directly to its output queue entry using the stored <code>outputQueueName</code>/<code>outputQueueLibrary</code>.</li> </ol>"},{"location":"Debugging%20Documentation/DEBUGGING/#debugging-spooloutput-queue-issues","title":"Debugging spool/output queue issues","text":"Issue How to investigate Report exists in RPG queue but not in API Query the <code>SpooledMetadataReport</code> table for the <code>reportType</code> and timestamp window using the repository\u2019s <code>where</code> construction to ensure the filters align with how the UI queries. \u3010F:src/main/account-payable/data/repositories/spooled-meta-data-reports.repository.ts\u2020L24-L94\u3011 PDF download 404s <code>reportsFormatter</code> rewrites <code>filePath</code> using <code>buildFilePath(row.pdfFileName.trim())</code>. Verify the PDF actually exists beneath the static root configured in <code>ServeStaticModule</code> and that <code>filePath</code> aligns with the SMB/NFS mount. \u3010F:src/shared/formatters/dropdown.formatter.ts\u2020L1-L66\u3011\u3010F:src/app.module.ts\u2020L15-L35\u3011 Spool shows wrong queue/library Confirm the defaults from <code>spooledReportMetaData</code> were overridden (if needed) before calling <code>createReports()</code>. When debugging, log the payload passed to <code>SpooledMetaDataReportInterface.createReports()</code> so you can match the queue metadata with IBM <code>WRKSPLF</code> output. \u3010F:src/shared/config/generate-report-config.ts\u2020L57-L127\u3011\u3010F:src/main/account-payable/data/repositories/spooled-meta-data-reports.repository.ts\u2020L97-L107\u3011"},{"location":"Debugging%20Documentation/DEBUGGING/#6-end-to-end-checklist-for-node-rpg-ibm-i-bugs","title":"6. End-to-End Checklist for Node \u2194 RPG \u2194 IBM i Bugs","text":"<ol> <li>API entry point \u2013 Confirm the relevant Nest controller/use case received the expected DTO (enable class-validator errors). Use the inspector or HTTP logs to ensure the request payload matches the RPG program\u2019s expectations.</li> <li>Queue ingestion \u2013 Make sure the job was enqueued in Bull/BullMQ (watch Redis). If it failed immediately, inspect the processor\u2019s <code>onFailed()</code> logs for stack traces. \u3010F:src/main/account-payable/application/voucher/shared-services/flexi.processor.ts\u2020L55-L111\u3011</li> <li>Sequelize transaction \u2013 Track SQL logs for the job window. If a query fails, <code>customSequelizeLogger</code> will show the statement right before the error; match it against IBM joblog statements. \u3010F:src/shared/infrastructure/connection.ts\u2020L117-L134\u3011</li> <li>RPG program execution \u2013 Use the stored job metadata (<code>jobName</code>, <code>jobNumber</code>, etc.) from the spooled metadata entries to open the IBM joblog (<code>DSPJOBLOG</code>) and the output queue (<code>WRKOUTQ</code>). \u3010F:src/main/account-payable/domain/entities/spooled-meta-data-report.entity.ts\u2020L1-L17\u3011</li> <li>Spooled output verification \u2013 Compare the expected spool file name / form type against <code>spooledReportMetaData</code> defaults or overrides for the use case you are testing. \u3010F:src/shared/config/generate-report-config.ts\u2020L57-L127\u3011</li> <li>Front-end surface \u2013 Ensure the formatted report list returned by <code>reportsFormatter</code> uses the same timezone/format as the UI filters; mismatches can hide real reports even when IBM generated them. \u3010F:src/shared/formatters/dropdown.formatter.ts\u2020L1-L66\u3011</li> </ol> <p>Following these steps gives you observability from the Nest request layer, through the queue processors, into the IBM DB2/ODBC layer, all the way to the IBM output queue that hosts the RPG-generated spooled files. When you need a click-by-click workflow for VS Code, IBM Navigator, ACS, or Data Studio, open the debugging workflows directory for ready-to-run checklists that build on this reference.</p>"},{"location":"Debugging%20Documentation/errors/","title":"Error Playbooks","text":"<p>Use this folder whenever a delivery or support engineer needs a repeatable workflow for diagnosing production/runtime errors. Each Markdown guide is scoped to a single symptom so additional issues can be added without rewriting the existing debugging bible.</p>"},{"location":"Debugging%20Documentation/errors/#current-guides","title":"Current Guides","text":"Error When to open Owner(s) 500 Internal Server Errors NestJS responds with 500 for otherwise valid API calls or IBM i raises unexpected SQL/CPF faults. Backend on-call + feature owner 502 Bad Gateway The gateway/ALB cannot reach the API container or rejects its response. Backend on-call 503 Service Unavailable Clients see 503 because the service is overloaded, throttled, or undergoing maintenance. Backend on-call + SRE 504 Gateway Timeouts Any NestJS endpoint called by a front-end use case returns 504 from the API Gateway, ALB, or reverse proxy. Backend on-call + IBM i operator DB2 Connection Failures Logs show SQLSTATE 08001/08004 or Sequelize connection errors preventing DB2 access. Backend on-call + IBM i database admin <p>Contributing: copy the template from any existing guide, change the headings to match the new symptom, and link it here.</p>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/","title":"500 Internal Server Errors \u2013 Diagnostic Playbook","text":"<p>Use this runbook when the API Gateway/proxy returns HTTP 500 for a request that should succeed. A 500 indicates NestJS threw an unhandled exception or the IBM i call stack surfaced an error that was not translated to a user-facing response. Follow each section to capture evidence before applying code fixes.</p>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#1-confirm-the-symptom","title":"1. Confirm the Symptom","text":"<ul> <li>Reproduce with tracing. Trigger the failing endpoint via Postman or <code>curl -v</code> and capture the response headers (including <code>x-request-id</code>) plus the JSON error payload or HTML error page.</li> <li>Check API logs immediately. Run <code>docker logs arg-backend-dev | rg \"reqId=&lt;value&gt;\" -C3</code> to grab the stack trace surrounding the 500 before log rotation occurs.</li> <li>Identify the use case. Map the route to its controller/service (e.g., <code>/ap/vendors/:id</code> \u2192 <code>src/main/account-payable/application/vendors/usecases/get-vendor.usecase.ts</code>).</li> </ul>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#2-containment","title":"2. Containment","text":"<ol> <li>Evaluate blast radius. If the exception fires for every request, drain traffic from the instance (<code>docker compose scale api=0</code> or remove the pod from the load balancer) to stop noisy alerts.</li> <li>Rollback problematic releases. Use the deployment manifest/git tag noted in <code>package.json</code> to redeploy the prior container image while the fix is prepared.</li> <li>Disable new features. If the 500 originated behind a feature flag (see <code>src/config/feature-flags.ts</code>), flip it off so unaffected functionality remains online.</li> </ol>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#3-nestjs-node-diagnostics","title":"3. NestJS + Node Diagnostics","text":"<ul> <li>Enable verbose logging. Temporarily set <code>LOG_LEVEL=debug</code> to capture per-provider logs via Winston (<code>src/shared/infrastructure/logger.ts</code>).</li> <li>Trace interceptors and filters. Inspect global exception filters (<code>src/shared/infrastructure/http/exception.filter.ts</code>) for missing branches that should translate known DB2 or validation errors to 4xx codes.</li> <li>Validate DTOs. When <code>class-validator</code> fails, it may throw before response mapping. Run the corresponding Jest suite (<code>npm run test &lt;module&gt;</code>) to surface schema drift.</li> <li>Inspect async flows. Many 500s stem from rejected promises in repositories. Open the repository under <code>src/main/**/data/repositories</code> and verify every <code>await</code> is wrapped in <code>try/catch</code> or re-thrown with context.</li> </ul>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#4-ibm-i-db2-stack","title":"4. IBM i / DB2 Stack","text":"<ol> <li>Correlate SQLSTATE. From the stack trace, note the SQLSTATE or CPF message. Search IBM Navigator job logs (<code>WRKACTJOB</code> \u2192 option 5) for the same timestamp to confirm whether DB2 aborted the call.</li> <li>Review RPG return data. If the CL/RPG program returns an error structure, ensure the Node adapter (<code>src/shared/infrastructure/ibmi/ibmi.service.ts</code>) maps it to a typed response instead of throwing.</li> <li>Check commitment control. Use IBM Data Studio to confirm the stored procedure commits/rolls back as expected; lingering locks often manifest as CPF or SQL0911 messages bubbled up as 500s.</li> </ol>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#5-resolution-paths","title":"5. Resolution Paths","text":"<ul> <li>Add targeted exception handling. Translate recurring SQL codes (e.g., duplicate keys) into HTTP 409/422 so the UI receives actionable feedback.</li> <li>Patch the offending module. Add tests reproducing the error path (controller \u2192 service \u2192 repository) before refactoring.</li> <li>Coordinate IBM i fixes. If the RPG program is defective, attach the job log, inputs, and SQL payload so the IBM team can recompile and notify when safe to redeploy.</li> </ul>"},{"location":"Debugging%20Documentation/errors/500-internal-server-errors/#6-evidence-checklist","title":"6. Evidence Checklist","text":"<ul> <li>HAR or curl output showing HTTP 500 + headers.</li> <li>Relevant container logs/stack traces tied to the request ID.</li> <li>IBM i job log snippet or SQLSTATE screenshot.</li> <li>Link to the code fix/rollback and automated tests proving the issue is resolved.</li> </ul>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/","title":"502 Bad Gateway \u2013 Diagnostic Playbook","text":"<p>Open this guide when the edge proxy (API Gateway, ALB, ingress) returns HTTP 502 indicating it cannot reach or trust the backend response. 502s usually mean the NestJS container crashed, rejected TLS, or returned malformed headers. Use this sequence to restore service quickly.</p>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#1-confirm-the-symptom","title":"1. Confirm the Symptom","text":"<ul> <li>Capture raw responses. Use <code>curl -v</code> or the UI HAR to record the 502 payload, timestamp, and request ID.</li> <li>Inspect gateway health checks. From Azure Application Gateway or AWS ALB metrics, verify the backend health probe is failing with <code>BadGateway</code>/<code>502</code> errors.</li> <li>Check container status. Run <code>docker compose ps</code> or <code>kubectl get pods</code> to see if the API container restarted or is in <code>CrashLoopBackoff</code>.</li> </ul>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#2-containment","title":"2. Containment","text":"<ol> <li>Fail over to a healthy instance. Remove the failing task/pod from the load balancer until diagnostics finish.</li> <li>Restart dependent services carefully. Bring up Redis, DB2 tunnel, and the NestJS API in the correct order using <code>dev-start.sh</code> or your orchestrator playbook so connection retries do not cascade.</li> <li>Notify front-end teams. Provide status so they can suppress repeat retries that might overload the gateway.</li> </ol>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#3-application-diagnostics","title":"3. Application Diagnostics","text":"<ul> <li>Check boot logs. Run <code>docker logs arg-backend-dev --since=10m</code> for startup stack traces (missing env vars, migration failures, etc.).</li> <li>Validate environment files. Ensure <code>.env</code>/secret mounts include required keys like <code>DB2_HOST</code>, <code>REDIS_URL</code>, and TLS certificates referenced in <code>src/config</code>.</li> <li>Look for memory/CPU exhaustion. Use <code>docker stats</code> or Kubernetes metrics to confirm the process is not being OOM-killed during heavy use cases.</li> <li>Review HTTP adapters. Misconfigured global interceptors (e.g., streaming large files without <code>Content-Length</code>) can cause the gateway to close the connection mid-flight.</li> </ul>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#4-ibm-i-connectivity-checks","title":"4. IBM i / Connectivity Checks","text":"<ol> <li>ODBC reachability. From inside the container, run <code>isql ARGDSN -v</code> (matching <code>odbc.ini</code>) to confirm DB2 is online and TLS negotiation succeeds.</li> <li>Job queue saturation. Use IBM Navigator or 5250 <code>WRKACTJOB</code> to ensure the required subsystem has free <code>QZDASOINIT</code> jobs; no available jobs can make the API appear offline.</li> <li>Stored procedure regression. If a new RPG program was deployed, confirm its signature still matches the Sequelize binding; mismatched parameter counts may crash the job and close the socket.</li> </ol>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#5-resolution-paths","title":"5. Resolution Paths","text":"<ul> <li>Fix configuration drift. Redeploy the container with the correct env/secret bundle or update Helm/docker-compose manifests.</li> <li>Patch dependency updates. Revert or adjust recent package upgrades that changed TLS/HTTP behavior (see <code>package.json</code> changelog).</li> <li>Add health probes. Ensure the NestJS <code>/health</code> endpoint exercises DB2/Redis connectivity so orchestration can detect failures before 502s reach users.</li> </ul>"},{"location":"Debugging%20Documentation/errors/502-bad-gateway/#6-evidence-checklist","title":"6. Evidence Checklist","text":"<ul> <li>Gateway metrics screenshot showing 502 spikes + probe status.</li> <li>Container/pod state output (<code>docker compose ps</code> or <code>kubectl describe pod</code>).</li> <li>Relevant NestJS logs proving the crash or misconfiguration.</li> <li>IBM i/DB2 validation (isql output, job log snippet) if the backend dependency caused the outage.</li> </ul>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/","title":"503 Service Unavailable \u2013 Diagnostic Playbook","text":"<p>Use this runbook when clients receive HTTP 503 responses, usually indicating the backend is overloaded, throttled, or intentionally taken out of service. These steps help determine whether resource exhaustion, planned maintenance, or IBM i dependencies are responsible.</p>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#1-confirm-the-symptom","title":"1. Confirm the Symptom","text":"<ul> <li>Capture the response. Record the 503 payload/body and headers to see if it originates from the gateway (e.g., <code>Retry-After</code> present) or from NestJS itself.</li> <li>Check incident calendar. Verify whether maintenance windows or deploy freezes are in effect before treating it as an outage.</li> <li>Review gateway metrics. Use Application Gateway/ALB dashboards to confirm backend response codes and whether health probes also fail.</li> </ul>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#2-containment","title":"2. Containment","text":"<ol> <li>Scale horizontally if possible. Increase the replica/task count using your orchestrator or <code>docker compose up --scale api=2</code> to relieve pressure.</li> <li>Throttle heavy jobs. Pause batch/BullMQ queues via <code>bull-board</code> or CLI to free CPU for synchronous traffic.</li> <li>Communicate status. Update the status page/Slack with ETA and mitigation steps so users know when to retry.</li> </ol>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#3-api-infrastructure-diagnostics","title":"3. API + Infrastructure Diagnostics","text":"<ul> <li>Inspect resource limits. Check <code>docker stats</code>, Kubernetes HPA metrics, or VM monitoring for CPU/memory saturation.</li> <li>Examine DB connection pools. In <code>src/shared/infrastructure/connection.ts</code>, verify pool settings align with IBM i limits; run <code>SELECT * FROM QSYS2.DATABASE_CONNECTION_POOL_INFO</code> to see active handles during the incident.</li> <li>Redis/backing service availability. Timeouts to Redis or other dependencies (S3, messaging) can bubble up as 503. Review connection error logs in the NestJS container.</li> <li>Deployment health. Ensure the latest release finished migrations and warmed caches; partially rolled deployments often sit behind 503s while readiness probes fail.</li> </ul>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#4-ibm-i-focus","title":"4. IBM i Focus","text":"<ol> <li>Subsystem availability. Use 5250 <code>WRKSBS</code>/<code>WRKACTJOB</code> to confirm the subsystem hosting the RPG programs is active and not held.</li> <li>Job queue backlog. If <code>QSYSOPR</code> shows numerous jobs in <code>JOBQ</code> or <code>MSGW</code>, IBM i may not accept new requests, causing the API to return 503s.</li> <li>Navigator performance metrics. Review CPU and temporary storage usage; heavy RPG batch jobs can starve interactive jobs invoked by Node.</li> </ol>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#5-resolution-paths","title":"5. Resolution Paths","text":"<ul> <li>Tune autoscaling. Adjust HPA or compose resource reservations so the service scales before saturation.</li> <li>Optimize expensive endpoints. Add caching/pagination to large exports that monopolize worker threads.</li> <li>Coordinate maintenance. When IBM i maintenance is planned, return a friendlier maintenance page and set accurate <code>Retry-After</code> headers via NestJS filters.</li> </ul>"},{"location":"Debugging%20Documentation/errors/503-service-unavailable/#6-evidence-checklist","title":"6. Evidence Checklist","text":"<ul> <li>Screenshot/log of 503 responses with timestamps.</li> <li>Metrics proving resource limits (CPU/memory/connection pool) during the event.</li> <li>IBM i subsystem/job status snapshot.</li> <li>Documented mitigation (scaling, throttle, maintenance window) plus follow-up tasks.</li> </ul>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/","title":"504 Gateway Timeouts \u2013 Diagnostic Playbook","text":"<p>This checklist explains how to confirm, isolate, and remediate 504 Gateway Timeout errors raised by ARG-WEB-BACKEND while customer use cases are executed. Follow it sequentially so evidence collected in each layer (gateway, NestJS service, DB2/RPG job, IBM i job logs) remains correlated.</p>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#1-confirm-the-symptom","title":"1. Confirm the Symptom","text":"<ul> <li>Capture raw responses. Use the frontend HAR, Postman, or <code>curl -v https://api.example.com/vendors</code> to show the 504 plus the request ID/trace headers passed through the gateway.</li> <li>Check the upstream duration. If the edge proxy exposes metrics (Azure Application Gateway, AWS ALB, Nginx ingress), confirm the backend timed out (usually &gt;30s) rather than the client disconnecting early.</li> <li>Note the impacted use case. Record which NestJS controller and use case folder handled the request (for example, <code>/account-payable/voucher</code> maps to <code>src/main/account-payable/application/voucher</code>).</li> </ul>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#2-fast-containment-steps","title":"2. Fast Containment Steps","text":"<ol> <li>Re-route traffic if needed. If the health probe also fails, take the pod/task out of rotation with <code>kubectl scale</code> or the compose equivalent to stop cascading 504s.</li> <li>Restart long-running workers. Use <code>npm run dev:status</code> to list containers and restart only the API container (<code>docker compose restart arg-backend-dev</code>) so Redis, DB2, and IBM i connectivity remain warm.</li> <li>Notify IBM i operators. The RPG job queue may be hung; have operations verify if any <code>QZDASOINIT</code> jobs are stuck in MSGW.</li> </ol>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#3-api-layer-diagnostics","title":"3. API Layer Diagnostics","text":"<ul> <li>Enable HTTP trace logging. Set <code>LOG_LEVEL=debug</code> and restart the API so Winston emits per-request spans (see <code>src/shared/infrastructure/logger.ts</code>).</li> <li>Correlate use cases. Use the request ID from the gateway to search <code>docker logs arg-backend-dev | rg \"reqId=&lt;value&gt;\"</code> and confirm which NestJS provider handled the call.</li> <li>Look for synchronous blockers. 504s often occur because a controller calls a repository that performs sequential DB2 reads. Check the relevant use case in <code>src/main/account-payable/application/**/usecases</code> for <code>await</code> chains that could be parallelized with <code>Promise.all</code>.</li> <li>Inspect Bull/BullMQ queues. If the use case enqueues background work, confirm the queue consumer is healthy: <code>docker exec -it arg-backend-dev redis-cli -n 0 LLEN bull:&lt;queue&gt;:wait</code>.</li> </ul>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#4-database-ibm-i-connectivity","title":"4. Database + IBM i Connectivity","text":"<ol> <li>Sequelize connection pool. Open <code>src/shared/infrastructure/connection.ts</code> and verify the configured pool size/timeouts. During the incident, run <code>SELECT * FROM QSYS2.DATABASE_CONNECTION_POOL_INFO</code> in IBM Data Studio to ensure handles are not exhausted.</li> <li>ODBC driver health. From the container shell, run <code>isql ARGDSN -v</code> using the DSN defined in <code>odbc.ini</code> to ensure credentials still work.</li> <li>Job log inspection. Use IBM Navigator (or 5250 <code>WRKACTJOB</code>) to inspect the job handling the request. Look for CPF/CPI messages referencing the stored procedure called from <code>src/main/account-payable/data/repositories</code>.</li> <li>Spooled output + data queues. If the RPG program writes to an output queue, capture the spool file to see if it halted awaiting input.</li> </ol>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#5-performance-profiling","title":"5. Performance Profiling","text":"<ul> <li>Trace NestJS latency. Wrap the problematic use case with the built-in logging interceptor or temporarily add <code>performance.now()</code> measurements per repository call. Keep the diff local while debugging.</li> <li>Measure DB2 statement time. Temporarily enable <code>logging: (sql) =&gt; this.logger.debug(...)</code> in the Sequelize repository (many already exist, e.g., <code>voucher.repository.ts</code> line ~640) to catch slow SQL.</li> <li>Redis cache misses. For endpoints backed by caching (<code>@nestjs/cache-manager</code>), use <code>redis-cli monitor | rg &lt;key&gt;</code> to confirm whether cache churn is forcing DB2 calls every time.</li> </ul>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#6-resolution-paths","title":"6. Resolution Paths","text":"<ul> <li>Increase upstream timeout only after fixes. If a query legitimately needs &gt;30s, ship an SLA adjustment along with proof that the IBM i program cannot be optimized further.</li> <li>Optimize the RPG/DB2 routine. Share captured SQL and job log evidence with the IBM i team so they can add indexes or refactor the CL/RPG procedure.</li> <li>Add pagination or batching. When a use case fetches thousands of rows (common in voucher exports), implement paging at the repository level and expose pagination params in the controller DTO.</li> <li>Backfill monitoring. Create a Grafana alert or Azure Monitor rule watching the <code>504</code> rate plus the NestJS duration histogram so future spikes auto-page the team.</li> </ul>"},{"location":"Debugging%20Documentation/errors/504-gateway-timeouts/#7-evidence-checklist","title":"7. Evidence Checklist","text":"<ul> <li>HAR or curl output with 504 status and headers.</li> <li>Gateway metrics screenshot showing upstream timeout.</li> <li>Container logs filtered by request ID with timestamps for controller \u2192 repository calls.</li> <li>IBM i job log or spool capture tied to the same timestamp.</li> <li>Root-cause summary + remediation PR/CL link.</li> </ul>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/","title":"DB2 Connection Failures (SQLSTATE 08001/08004) \u2013 Diagnostic Playbook","text":"<p>Follow this guide when the API logs show <code>SequelizeConnectionError</code>, <code>SQL0901</code>, <code>SQL30081N</code>, or SQLSTATE 08001/08004 errors indicating the Node process cannot open or retain DB2 connections on IBM i. These issues often manifest as HTTP 500/503 responses upstream, so documenting them separately keeps DB/ODBC actions clear.</p>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#1-confirm-the-symptom","title":"1. Confirm the Symptom","text":"<ul> <li>Collect application logs. Use <code>docker logs arg-backend-dev | rg -n \"SequelizeConnection\" -C3</code> to capture the first error plus retry attempts.</li> <li>Check DB2 availability. From IBM Data Studio or ACS, run a simple query (<code>SELECT CURRENT_DATE FROM SYSIBM.SYSDUMMY1</code>) to confirm the LPAR accepts connections.</li> <li>Identify affected pools. Note which Sequelize configuration (primary vs reporting) failed by checking <code>src/shared/infrastructure/connection.ts</code> or module-specific datasource files.</li> </ul>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#2-containment","title":"2. Containment","text":"<ol> <li>Recycle idle handles. Restart only the NestJS API container (<code>docker compose restart api</code>) so Redis/jobs remain intact while new DB2 sessions are negotiated.</li> <li>Throttle traffic. Temporarily reduce concurrency (HPA min replicas, worker count) so DB2 is not flooded with retries that prolong the outage.</li> <li>Escalate to IBM i ops. If the error references CPF/SQL messages (e.g., CPF9898), alert the IBM operator to inspect <code>QSYSOPR</code> for system-wide issues.</li> </ol>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#3-ibm-i-diagnostics","title":"3. IBM i Diagnostics","text":"<ul> <li>Active job review. Run <code>WRKACTJOB SBS(QUSRWRK)</code> and filter for <code>QZDASOINIT</code> to ensure jobs are not stuck in <code>MSGW</code>. Responding to the message often frees the port.</li> <li>Connection limits. Execute <code>SELECT * FROM QSYS2.NETSTAT_JOB_INFO</code> to see socket states; if the host reached its limit, you may need to end stale jobs with <code>ENDPJ</code> or <code>ENDJOB</code>.</li> <li>Authority changes. SQL08004 commonly signals revoked credentials. Verify the service profile still has the correct library list and authorities.</li> </ul>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#4-application-diagnostics","title":"4. Application Diagnostics","text":"<ul> <li>Pool exhaustion. Confirm <code>max</code>/<code>acquire</code> timeout settings in the Sequelize config are balanced with IBM i capacity (default <code>pool.max=5</code>). Increase only when IBM i operators approve additional jobs.</li> <li>TLS/driver drift. Check <code>odbc.ini</code> and the IBM i Access ODBC driver version used in the container. Recent OS/PTF updates may require a new driver level.</li> <li>Long-running transactions. Use <code>QSYS2.ACTIVE_JOB_INFO</code> to find transactions holding locks for &gt;30s; they prevent new sessions from starting until commit/rollback.</li> </ul>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#5-resolution-paths","title":"5. Resolution Paths","text":"<ul> <li>Restart the host listener. IBM ops can recycle <code>QUSRWRK</code> or DB2 TCP/IP servers if sockets are wedged.</li> <li>Adjust pool/backoff. Implement exponential backoff in <code>src/shared/infrastructure/connection.ts</code> and expose metrics so chronic spikes trigger alerts before saturation.</li> <li>Coordinate credential updates. When passwords/SSL certificates rotate, update Kubernetes secrets or <code>.env</code> simultaneously to avoid rejected logins.</li> </ul>"},{"location":"Debugging%20Documentation/errors/db2-connection-failures/#6-evidence-checklist","title":"6. Evidence Checklist","text":"<ul> <li>Exact log excerpts with SQLSTATE/SQLCODE.</li> <li>isql or Data Studio screenshots proving success/failure after fixes.</li> <li>IBM i job/subsystem status before and after remediation.</li> <li>Link to configuration changes (Sequelize pool, secrets, driver updates) deployed to resolve the incident.</li> </ul>"},{"location":"Debugging%20Documentation/workflows/","title":"Debugging Workflows Index","text":"<p>Use these guides when you need a step-by-step workflow that links VS Code, NestJS, and IBM i tooling. Each playbook assumes you already read <code>docs/DEBUGGING.md</code> for the architecture-level context; the files here focus on getting you hands-on with debuggers and IBM utilities fast.</p> Workflow Use it when Key tools VS Code \u2194 NestJS \u2194 IBM i Bridge You need to reproduce a failing API/queue job locally and trace it all the way to DB2/RPG from VS Code. VS Code debugger, <code>npm run start:debug</code>, Jest inspector, Bull/BullMQ UI IBM Navigator, ACS, Data Studio, and 5250 Toolkit IBM-side evidence (job logs, output queues, SQL packages) is missing or inconsistent with what the API reports. IBM Navigator for i, Access Client Solutions, IBM Data Studio, 5250/PDM Rapid Incident Triage (Full Stack) You are on-call for the backend and must stabilize an outage before escalating. VS Code, <code>docker-compose</code>, IBM Navigator, error playbooks <p>Contributing: Add new Markdown files beside these guides for future tooling workflows (for example, <code>acs-spooled-file-export.md</code>) and remember to link them in this table.</p>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/","title":"IBM Navigator, ACS, Data Studio, and 5250 Toolkit","text":"<p>Use this playbook when an incident requires IBM-side validation in addition to the VS Code debugging workflow.</p>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#1-before-you-leave-vs-code","title":"1. Before you leave VS Code","text":"<ol> <li>Capture the job metadata (job name/number/user, output queue, report type) from the failed API call or queue job using <code>SpooledMetaDataReportEntity</code> and <code>spooledReportMetaData</code>. \u3010F:src/main/account-payable/domain/entities/spooled-meta-data-report.entity.ts\u2020L1-L17\u3011\u3010F:src/shared/config/generate-report-config.ts\u2020L57-L127\u3011</li> <li>Note the Sequelize model + schema involved from the stack trace (for example, lines inside <code>schema.ts</code>). You will use these names inside IBM Data Studio. \u3010F:src/main/account-payable/data/schemas/schema.ts\u2020L2296-L2339\u3011</li> <li>Export the exact DSN and user from <code>connection.ts</code> so IBM Navigator queries the same library list (<code>env-library-config.ts</code>). \u3010F:src/shared/infrastructure/connection.ts\u2020L71-L233\u3011\u3010F:src/shared/config/env-library-config.ts\u2020L4-L83\u3011</li> </ol>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#2-ibm-navigator-for-i-web","title":"2. IBM Navigator for i (Web)","text":"<ol> <li>Open Work Management \u2192 Active Jobs and filter by the captured <code>jobName</code>/<code>jobNumber</code>.</li> <li>Drill into the job log (<code>Messages</code> tab) and compare CPF / SQL codes with the Nest log output; copy the entries into the incident ticket.</li> <li>Open Output Queues and jump directly to <code>outputQueueLibrary/outputQueueName</code>. Confirm the spool file exists, matches the timestamp, and note whether it is <code>HLD</code>, <code>RDY</code>, or <code>SAV</code>.</li> <li>If the spool file is missing, confirm whether the RPG program wrote to the default queue defined in <code>spooledReportMetaData</code> or if overrides were sent with the API request payload.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#3-ibm-i-access-client-solutions-acs","title":"3. IBM i Access Client Solutions (ACS)","text":"<ol> <li>Launch ACS and connect to the same system/DSN as the API container.</li> <li>Use Run SQL Scripts to run sanity queries with the schema/table names recorded earlier:    <pre><code>SELECT MEJOBNM, MEOQNM, MEOQLB, MERPTP, LASTUPDATED\nFROM LIBRARY.SPOOLEDMETADATAREPORT\nWHERE MERPTP = 'APREPORT'\nORDER BY LASTUPDATED DESC FETCH FIRST 20 ROWS ONLY;\n</code></pre>    Replace <code>LIBRARY</code> with the active schema derived from <code>env-library-config.ts</code>.</li> <li>Use System Status to ensure the relevant subsystems (e.g., <code>QHTTPSVR</code>, <code>QBATCH</code>) are not constrained.</li> <li>Export spool files or job logs as PDF/HTML when attaching evidence to tickets.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#4-ibm-data-studio","title":"4. IBM Data Studio","text":"<ol> <li>Create a connection profile pointing to the DSN from <code>odbc.ini</code> / <code>connection.ts</code>.</li> <li>Import the Sequelize model DDL you are validating (for example, <code>SpooledMetaDataReportsRepository</code> queries). \u3010F:src/main/account-payable/data/repositories/spooled-meta-data-reports.repository.ts\u2020L1-L107\u3011</li> <li>Use the Data Output tab to validate record counts, nullability, and any derived columns (like the formatted paths from <code>reportsFormatter</code>). \u3010F:src/shared/formatters/dropdown.formatter.ts\u2020L1-L66\u3011</li> <li>Save SQL snippets that recreate the failing condition so another engineer can replay them without the app.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#5-5250-pdm-session","title":"5. 5250 / PDM session","text":"<ol> <li>Start a 5250 session (ACS \u25b6 5250 Emulator) with the same profile.</li> <li>Run <code>WRKACTJOB SBS(QBATCH)</code> and locate the job number captured earlier to inspect it interactively.</li> <li>Use <code>DSPJOBLOG</code> and <code>WRKOUTQ</code> with the <code>jobNumber/jobUser</code> to inspect the RPG messages.</li> <li>For source-level investigations, open <code>STRPDM</code> \u2192 option 2 (Work with Members) and navigate to the library that owns the RPG program under test. Verify timestamps match the deployment you expect.</li> <li>If you need to recompile, coordinate with the RPG owner and make sure VS Code\u2019s evidence (DTO payloads, SQL statements) is attached to the service request.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/ibmi-tooling/#6-closing-the-loop-back-in-vs-code","title":"6. Closing the loop back in VS Code","text":"<ol> <li>Record the IBM findings in your incident doc and cross-link them with the related source files (<code>flexi.processor.ts</code>, <code>connection.ts</code>, etc.).</li> <li>Update/extend the relevant error playbook (for example, <code>docs/errors/504-gateway-timeouts.md</code>) with the new failure mode if it was not already documented.</li> <li>Re-run the VS Code debugger session using any IBM-side configuration tweaks (library list, user profile) that surfaced during the investigation to ensure the fix is reproducible.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/","title":"Rapid Incident Triage (Full Stack)","text":"<p>This runbook combines the VS Code and IBM i workflows so on-call engineers can restore service fast while gathering the right evidence.</p>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#1-confirm-the-symptom","title":"1. Confirm the symptom","text":"<ol> <li>Check the API logs (<code>npm run dev:logs</code> for docker-compose or your log aggregator) to confirm HTTP status and error signature. \u3010F:package.json\u2020L16-L32\u3011</li> <li>Open the matching error playbook in <code>docs/errors</code> (500/502/503/504 or DB2) to follow its containment steps while you continue the deeper triage.</li> <li>If the incident is user-specific, capture the request payload or queue job contents (Flexi uploads, ACH batches, etc.) from the Nest debugger before retrying. \u3010F:src/main/account-payable/application/voucher/shared-services/flexi.processor.ts\u2020L1-L175\u3011</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#2-stabilize-the-environment","title":"2. Stabilize the environment","text":"<ol> <li>Restart only what\u2019s needed \u2013 use <code>npm run docker:down</code> or restart the worker container whose queue is failing; avoid bouncing DB2 or Redis unless those services are confirmed unhealthy. \u3010F:package.json\u2020L25-L33\u3011</li> <li>If Redis queues are flooded, temporarily pause the processor by commenting out <code>@OnQueueActive()</code> handlers or scaling workers to zero via docker-compose.</li> <li>Capture a snapshot of <code>docker-compose ps</code> and any Kubernetes/VM state so platform teams can see what changed.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#3-debug-in-vs-code","title":"3. Debug in VS Code","text":"<ol> <li>Attach the VS Code debugger (<code>npm run start:debug</code>) and reproduce the failing route or job.</li> <li>Use logpoints to emit DB2 DSN, schema, and invoice IDs so IBM operators can align job logs without waiting for screenshots. \u3010F:src/shared/infrastructure/connection.ts\u2020L117-L233\u3011</li> <li>Inspect Sequelize\u2019s SQL output (<code>customSequelizeLogger</code>) to see if the failure happens before or after the IBM stored procedure runs.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#4-cross-check-on-ibm-i-tools","title":"4. Cross-check on IBM i tools","text":"<ol> <li>Jump into IBM Navigator with the captured <code>jobNumber/jobUser</code> and review the job log + output queue status (see the IBM tooling guide for exact clicks).</li> <li>If SQLSTATE errors appear, rerun the query inside ACS or Data Studio using the same schema from <code>env-library-config.ts</code> to confirm it fails outside the app too. \u3010F:src/shared/config/env-library-config.ts\u2020L4-L83\u3011</li> <li>For RPG runtime errors, pull the source member from PDM and verify its compile timestamp; confirm whether a newer program was supposed to be moved as part of the current release.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#5-communicate-status","title":"5. Communicate status","text":"<ol> <li>Update the incident channel or ticket with:</li> <li>Impacted endpoints/queues and the HTTP status observed.</li> <li>Whether the <code>docs/errors/&lt;symptom&gt;.md</code> steps have been completed.</li> <li>IBM job IDs / spool files reviewed and their status.</li> <li>If a code fix is needed, create a short-term feature flag or revert referencing the modules touched (controllers, processors, repositories) and cite the last known good commit.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/rapid-triage/#6-capture-learning","title":"6. Capture learning","text":"<ol> <li>Add a post-incident note to the relevant docs:</li> <li><code>docs/errors/...</code> for symptom-specific handling.</li> <li><code>docs/debugging/workflows/...</code> if a new tool workflow emerged.</li> <li>Check the code into a branch, add regression tests (<code>npm run test</code> / <code>npm run test:debug</code>), and run <code>npm run lint</code> before opening a PR. \u3010F:package.json\u2020L10-L34\u3011</li> <li>During the PR review, link the incident ticket and call out any IBM i configuration changes so RPG/operations teams can audit them.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/","title":"VS Code \u2194 NestJS \u2194 IBM i Bridge","text":"<p>Follow this checklist whenever you need to debug a backend defect from VS Code all the way to DB2/RPG artifacts.</p>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#1-prep-vs-code-for-this-repo","title":"1. Prep VS Code for this repo","text":"<ol> <li>Install extensions</li> <li>ESLint + Prettier so files match the project linters (<code>eslint.config.mjs</code>, <code>prettier</code>).</li> <li>NestJS Snippets and IBM i (Code for IBM i) to jump between controllers, queues, and IBM job streams.</li> <li>Clone + install \u2013 open the repo folder and run <code>yarn</code> once so VS Code can index the TypeScript sources referenced from <code>tsconfig.json</code>.</li> <li>Trust the workspace so VS Code can read <code>.vscode/launch.json</code> (if checked in) or let you create a new one.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#2-attach-to-the-api-from-vs-code","title":"2. Attach to the API from VS Code","text":"<ol> <li>Start the inspector-friendly Nest process:    <pre><code>npm run start:debug\n</code></pre>    This wraps <code>nest start --debug 0.0.0.0:9229 --watch</code> per <code>package.json</code>, so file changes automatically reload. \u3010F:package.json\u2020L10-L24\u3011</li> <li>Create a <code>launch.json</code> (Run and Debug \u25b6 <code>create a launch.json</code>) with:    <pre><code>{\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"name\": \"NestJS: Attach\",\n  \"port\": 9229,\n  \"restart\": true,\n  \"protocol\": \"inspector\",\n  \"skipFiles\": [\"&lt;node_internals&gt;/**\"]\n}\n</code></pre></li> <li>Set breakpoints inside controllers/use cases (for example <code>src/main/account-payable/application/voucher/shared-services/flexi.processor.ts</code>) and in the DB driver glue (<code>src/shared/infrastructure/connection.ts</code>). \u3010F:src/main/account-payable/application/voucher/shared-services/flexi.processor.ts\u2020L1-L175\u3011\u3010F:src/shared/infrastructure/connection.ts\u2020L117-L257\u3011</li> <li>Trigger the failing request via Thunder Client/REST Client or by replaying the upstream queue message. When execution pauses, inspect the job payload and confirm the DB schema/libraries match expectations from <code>env-library-config.ts</code>. \u3010F:src/shared/config/env-library-config.ts\u2020L4-L83\u3011</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#3-debug-queue-workers-from-vs-code","title":"3. Debug queue workers from VS Code","text":"<ol> <li>Run the local stack:    <pre><code>npm run docker:dev\n</code></pre>    This brings up the API container plus the Bull/BullMQ workers defined in <code>docker-compose.yml</code>. \u3010F:package.json\u2020L25-L33\u3011\u3010F:docker-compose.yml\u2020L1-L80\u3011</li> <li>Attach the debugger to the worker container by forwarding port <code>9229</code> (add <code>NODE_OPTIONS=--inspect=0.0.0.0:9229</code> to the worker service or run it locally with <code>npm run start:debug</code>).</li> <li>Use watch expressions for <code>job.data</code>, <code>job.attemptsMade</code>, and DB transaction state so you can correlate each retry with IBM job logs.</li> <li>Inspect Redis with the Bull UI of your choice or <code>redis-cli</code> to confirm the job is not stuck in <code>waiting</code> after your code resumes.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#4-debug-db2ibm-i-calls-without-leaving-vs-code","title":"4. Debug DB2/IBM i calls without leaving VS Code","text":"<ol> <li>Search for SQL models quickly \u2013 use <code>Go to Symbol</code> (<code>Ctrl+T</code>) to jump to the Sequelize model named in the error. The schema and table name mappings live alongside each model (for example, <code>src/main/account-payable/data/schemas/schema.ts</code>). \u3010F:src/main/account-payable/data/schemas/schema.ts\u2020L2296-L2339\u3011</li> <li>Open the DSN builder \u2013 peek at <code>src/shared/infrastructure/connection.ts</code> while paused to see the exact DSN string and retry config your code is using.</li> <li>Run targeted tests \u2013 configure another <code>launch.json</code> entry:    <pre><code>{\n  \"type\": \"node\",\n  \"request\": \"launch\",\n  \"name\": \"Jest: Debug current file\",\n  \"program\": \"${workspaceFolder}/node_modules/.bin/jest\",\n  \"args\": [\"${relativeFile}\", \"--runInBand\"],\n  \"cwd\": \"${workspaceFolder}\",\n  \"runtimeArgs\": [\"--inspect-brk\"],\n  \"console\": \"integratedTerminal\"\n}\n</code></pre>    Then press <code>F5</code> inside a failing spec (for example, <code>add-voucher-entry.use-case.spec.ts</code>) to replay the business logic without starting the HTTP server.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#5-capture-ibm-evidence-from-vs-code","title":"5. Capture IBM evidence from VS Code","text":"<ol> <li>Copy the job metadata \u2013 the <code>SpooledMetaDataReportEntity</code> exposes <code>jobName</code>, <code>jobNumber</code>, <code>jobUser</code>, and <code>outputQueue</code> so you can paste them into IBM Navigator or ACS without re-running the API. \u3010F:src/main/account-payable/domain/entities/spooled-meta-data-report.entity.ts\u2020L1-L17\u3011</li> <li>Track spool defaults \u2013 <code>spooledReportMetaData</code> defines which queue/library the RPG job will target; keep the file open side-by-side with your debugger to confirm overrides. \u3010F:src/shared/config/generate-report-config.ts\u2020L57-L127\u3011</li> <li>Log evidence inline \u2013 use VS Code\u2019s <code>Add Logpoint\u2026</code> feature to emit structured JSON (job ID, invoice number, SQL state) without stopping the process. Share those snippets in incident tickets alongside IBM screenshots.</li> </ol>"},{"location":"Debugging%20Documentation/workflows/vscode-debugging/#6-shut-down-cleanly","title":"6. Shut down cleanly","text":"<ol> <li>Stop debugging (Shift+F5) so Nest restarts in watch mode without a debugger.</li> <li><code>Ctrl+C</code> the terminal running <code>npm run start:debug</code> or <code>npm run docker:dev</code>.</li> <li>Run <code>npm run dev:logs</code> if you need to preserve the docker-compose output for root-cause analysis. \u3010F:package.json\u2020L16-L32\u3011</li> </ol>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/","title":"Client-Side Grid","text":"<p>Title: Client-Side Grid Filtering, Sorting, and Pagination with Backend Offset-Based Pagination Date: 2025-06-16 Status: Accepted  </p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#context","title":"Context","text":"<p>This decision addresses the approach to managing grid-level data interactions \u2014 specifically filtering, sorting, and pagination \u2014 for data-driven UI components.</p> <p>Two architectural options were considered for handling these features:</p> <ol> <li>Client-Side Handling \u2013 Load the dataset once from the API and allow all interactions (filtering, sorting, pagination) to occur entirely in the browser.  </li> <li>Server-Side Handling \u2013 Rely on backend API endpoints to execute filtering, sorting, and pagination for every user interaction.</li> </ol> <p>The system in question needs to handle datasets that may be large in some cases, but are typically manageable in size after user-driven filtering.</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#decision","title":"Decision","text":"<p>The architecture will implement client-side filtering, sorting, and pagination for grid components.</p> <p>To support this:</p> <ul> <li>The UI will include filter inputs and a \"Search\" button that users must use to initiate data retrieval.</li> <li>Upon clicking \"Search\", the frontend will send a request to the API including any provided filter criteria.</li> <li>The API will return a page of data (default size of 500 records or fewer) along with offset-based pagination metadata.</li> <li>All grid operations (filtering, sorting, pagination) will then be handled entirely client-side using the retrieved data.</li> </ul> <p>In addition:</p> <ul> <li>The API will implement standardized filtering and sorting behavior for all relevant endpoints. This ensures consistency and reusability of query logic across the application.</li> <li>Every API endpoint will support:</li> <li>Sorting via query parameters in ascending/descending order.</li> <li>Filtering on text fields (equality) and numeric fields (equality, greater than, less than, and range).</li> </ul> <p>If the total number of matching records exceeds the initial page size (default limit = 500), the API will not return an error. Instead, it will return a <code>nextPageUrl</code> that can be used to fetch additional pages of data.</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#performance-and-user-experience","title":"Performance and User Experience","text":"<ul> <li>Interactions like filtering and sorting are executed instantly in the browser, eliminating round trips to the server and improving responsiveness.</li> <li>Users control when and how data is retrieved, reducing backend load and improving perceived speed.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#simplicity","title":"Simplicity","text":"<ul> <li>Reduces complexity in the frontend-to-backend interaction model.</li> <li>Decreases backend logic for grid state handling.</li> <li>Encourages standardization of query patterns across endpoints.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#scalable-pagination","title":"Scalable Pagination","text":"<ul> <li>Using <code>offset</code> and <code>limit</code> enables scalable data retrieval while avoiding hard failures due to record caps.</li> <li>Providing metadata like <code>nextPageUrl</code> and <code>totalRecords</code> gives clients full control if they need to fetch more data explicitly.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#rest-api-capabilities","title":"REST API Capabilities","text":"<p>Even though grid operations are handled client-side after data retrieval, the REST API will support consistent and standardized pagination, filtering, and sorting behavior:</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#filtering","title":"Filtering","text":"<ul> <li>Text fields: Supports equality match (<code>eq</code>)</li> <li>Numeric fields: Supports:</li> <li>Equality (<code>eq</code>)</li> <li>Greater than (<code>gt</code>)</li> <li>Less than (<code>lt</code>)</li> <li>Range filtering (e.g., <code>min</code>/<code>max</code> or <code>between</code> operators)</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#sorting","title":"Sorting","text":"<ul> <li>Query parameter format: <code>?sort=field.asc</code> or <code>?sort=field.desc</code></li> <li>Multiple sort keys may be supported where applicable.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#pagination-offset-based","title":"Pagination (Offset-Based)","text":"<ul> <li>Default <code>limit</code>: 500</li> <li>Supports <code>limit</code> and <code>offset</code> parameters</li> <li>The API response includes:</li> </ul> <p>```json {   \"data\": [ / array of records / ],   \"offset\": 0,   \"limit\": 500,   \"totalRecords\": 1342,   \"hasMore\": true,   \"nextPageUrl\": \"/api/items?offset=500&amp;limit=500\",   \"previousPageUrl\": null,   \"filtersApplied\": {     \"status\": \"active\"   },   \"sort\": \"createdAt.desc\" }</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/","title":"File Upload Workflow","text":"<p>Title: File Upload Workflow with Asynchronous Batch Processing Date: 2025-10-26 Status: Accepted</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#context","title":"Context","text":"<p>The application requires a robust file upload system that can handle large Excel/CSV files containing business data. The system must address several key challenges:</p> <p>Key Requirements: - Process large files (up to 10MB) containing structured business data - Validate file structure and data integrity before processing - Provide real-time feedback to users during long-running operations - Handle thousands of records efficiently without HTTP timeouts - Ensure data accuracy with proper Excel date and accounting number parsing - Maintain security through proper authentication and user isolation</p> <p>Constraints: - HTTP request timeout limitations for synchronous processing - Need for immediate user feedback while processing continues - File format complexity (Excel serial dates, accounting number formats) - Resource cleanup requirements for temporary file storage</p> <p>Options Considered: 1. Synchronous HTTP-only processing - Simple but causes timeouts on large files 2. Pure asynchronous processing with polling - Requires constant client polling, inefficient 3. Hybrid HTTP/WebSocket with queue-based processing - Balances immediate feedback with async processing</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#decision","title":"Decision","text":"<p>We will implement a hybrid HTTP/WebSocket architecture with asynchronous batch processing using BullMQ for the file upload workflow.</p> <p>Key Components:</p> <ol> <li>HTTP Layer (Synchronous)</li> <li>Endpoint: <code>POST /{module}/upload</code></li> <li>Protocol: HTTP with multipart/form-data</li> <li> <p>Handles: File upload, validation, and initial processing setup</p> </li> <li> <p>WebSocket Layer (Asynchronous)</p> </li> <li>Protocol: WebSocket with Socket.IO</li> <li>Authentication: JWT token-based</li> <li> <p>Handles: Real-time progress updates and final results</p> </li> <li> <p>Queue Layer (Background Processing)</p> </li> <li>Technology: BullMQ with Redis</li> <li>Handles: Asynchronous batch processing of uploaded data</li> </ol> <p>Processing Workflow:</p> <pre><code>1. File Upload &amp; Validation (HTTP)\n   \u2193\n2. Data Parsing &amp; Batch Creation\n   \u2193\n3. Queue Job Creation (Parent + Child Jobs)\n   \u2193\n4. Asynchronous Batch Processing (BullMQ)\n   \u2193\n5. Real-time Updates (WebSocket)\n   \u2193\n6. Final Summary &amp; Cleanup\n</code></pre> <p>Technical Specifications: - File types supported: .xlsx, .xls, .csv - Maximum file size: 10MB (configurable) - Storage location: <code>uploads/csv/{MODULE_NAME}/</code> - Upload ID format: <code>{PREFIX}-{timestamp}-{uuid}</code> - User-specific WebSocket rooms: <code>user-{userIdentifier}</code> - Job structure: Parent job orchestrating multiple child batch jobs</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#scalability","title":"Scalability","text":"<ul> <li>Asynchronous processing prevents HTTP timeouts: By decoupling file upload from data processing, large files can be processed without hitting typical 30-60 second HTTP timeout limits</li> <li>Batch processing handles large datasets efficiently: Configurable batch sizes allow tuning for optimal memory usage and processing speed</li> <li>Redis-backed queue supports horizontal scaling: Multiple worker processes can consume jobs from the same queue, enabling easy scaling as load increases</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#user-experience","title":"User Experience","text":"<ul> <li>Immediate upload confirmation: Users receive instant HTTP response confirming file acceptance, preventing perceived delays</li> <li>Real-time progress updates: WebSocket events provide continuous feedback on processing status, reducing user anxiety during long operations</li> <li>Comprehensive error reporting: Specific, actionable error messages at multiple validation levels help users correct issues quickly</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#reliability","title":"Reliability","text":"<ul> <li>Automatic resource cleanup: Temporary files are removed after processing (success or failure), preventing disk space exhaustion</li> <li>Robust error handling: Multi-level validation catches issues early; processing errors don't crash the system</li> <li>Job tracking and monitoring: Parent/child job structure enables detailed observability and debugging</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#security","title":"Security","text":"<ul> <li>JWT-based authentication for both protocols: Consistent authentication across HTTP and WebSocket prevents unauthorized access</li> <li>File type and size validation: Prevents malicious file uploads and resource exhaustion attacks</li> <li>User-specific data isolation: WebSocket rooms ensure users only receive updates for their own uploads</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#trade-offs-accepted","title":"Trade-offs Accepted","text":"<ul> <li>Increased system complexity: Three-layer architecture (HTTP/WebSocket/Queue) requires more infrastructure and monitoring compared to simple synchronous approach</li> <li>Redis dependency: Adds operational overhead of maintaining Redis instance</li> <li>WebSocket connection management: Requires handling connection lifecycle, reconnection logic, and authentication</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#consequences","title":"Consequences","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#positive","title":"Positive","text":"<ul> <li>Handles files of any size within limits: Large files with thousands of records process smoothly without timeouts</li> <li>Excellent user experience: Users can navigate away and return; progress is preserved and resumable</li> <li>Production-ready scalability: Can handle increased load by adding queue workers horizontally</li> <li>Detailed observability: Upload IDs correlate actions across all system layers for effective debugging</li> <li>Clean separation of concerns: HTTP handles upload, WebSocket handles communication, Queue handles processing</li> <li>Reusable pattern: Architecture can be applied to multiple modules requiring file upload functionality</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#negative","title":"Negative","text":"<ul> <li>Increased infrastructure complexity: Requires Redis server, WebSocket server, and queue workers in addition to standard HTTP API</li> <li>More difficult local development: Developers need Redis and multiple processes running to test the full workflow</li> <li>WebSocket connection management: Must handle disconnections, reconnections, and stale connections</li> <li>Eventual consistency model: Brief delay between upload confirmation and processing completion may confuse some users</li> <li>Higher operational overhead: More services to monitor, log aggregation across multiple components required</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#implementation-notes","title":"Implementation Notes","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#file-validation-pipeline","title":"File Validation Pipeline","text":"<pre><code>1. File type validation: .xlsx, .xls, .csv only\n2. File size validation: Configurable maximum (default 10MB)\n3. Header validation: Exact column order and count matching\n4. Data validation: At least 1 non-empty row after header\n5. Format validation: Excel date conversion, accounting number parsing\n</code></pre>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#response-schemas","title":"Response Schemas","text":"<p>HTTP Success Response: <pre><code>interface FileUploadResponseDto {\n  message: string;           // \"File accepted, split into batches\"\n  uploadId: string;         // \"{PREFIX}-{timestamp}-{uuid}\"\n  totalRecords: number;     // Total records processed\n  totalBatches: number;     // Number of batches created\n  parentJobId: string;      // Parent job ID for tracking\n  childJobIds: string[];    // Array of child job IDs\n}\n</code></pre></p> <p>WebSocket Batch Progress: <pre><code>{\n  \"event\": \"batch-progress\",\n  \"data\": {\n    \"uploadId\": \"UPLOAD-1234567890-uuid\",\n    \"batchId\": \"batch-1\",\n    \"status\": \"completed\",\n    \"processed\": 5,\n    \"errors\": 0,\n    \"warnings\": 0\n  }\n}\n</code></pre></p> <p>WebSocket Final Summary: <pre><code>{\n  \"event\": \"upload-complete\",\n  \"data\": {\n    \"uploadId\": \"UPLOAD-1234567890-uuid\",\n    \"summary\": {\n      \"totalProcessed\": 10,\n      \"successCount\": 7,\n      \"errorCount\": 2,\n      \"warningCount\": 1,\n      \"additionalMetrics\": {}  // Module-specific metrics\n    }\n  }\n}\n</code></pre></p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#error-response-types","title":"Error Response Types","text":"<p>File Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"Only Excel (.xlsx, .xls) and CSV files are allowed\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Header Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"Header mismatch at column 3. Expected 'COLUMN_NAME' but found 'ColumnName'\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Data Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"File must contain at least 1 non-empty data row(s) after the header row. Found only 0 non-empty data row(s).\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Authentication Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"User authentication required\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>1. parseFile() - Extract data with proper formatting\n2. filterAndNormalizeRows() - Clean and filter empty rows\n3. chunkArray() - Split into configurable batches\n4. createQueueJobs() - Create parent/child job structure\n5. processInBackground() - Execute batch processing\n</code></pre>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>Logging Strategy: - Structured logging with upload IDs for correlation - Process duration tracking at each stage - Error logging with full stack traces</p> <p>Key Metrics: - Upload success/failure rates by module - Processing time per batch - Queue depth and processing rates - WebSocket connection count and stability</p> <p>Tracing: - Upload ID correlation across HTTP/WebSocket/Queue layers - Job ID tracking for debugging - User context preservation throughout workflow</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#module-specific-configuration","title":"Module-Specific Configuration","text":"<p>Each module can customize: - Upload ID prefix (e.g., \"CC\" for Clear Checks, \"INV\" for Invoices) - Expected file headers and column order - Batch size for processing - Module-specific validation rules - Custom business logic for data processing - Summary metrics format</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#related-decisions","title":"Related Decisions","text":"<p>[To be added as related ADRs are created for specific module implementations]</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#references","title":"References","text":"<ul> <li>BullMQ Documentation: https://docs.bullmq.io/</li> <li>Socket.IO Documentation: https://socket.io/docs/</li> <li>Excel Serial Date Format: Microsoft Excel date system documentation</li> <li>Multipart Form Data Specification: RFC 7578</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/","title":"LRO Long Running Operations","text":"<p>Title: Long-Running Operations (LRO) Pattern with HTTP/WebSocket/Queue Architecture Date: 2025-10-26 Status: Accepted</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#context","title":"Context","text":"<p>The application requires a pattern for handling long-running operations (LROs) that exceed typical HTTP timeout limits. These operations include file uploads, batch validations, data migrations, complex calculations, and other processing tasks that take more than a few seconds to complete.</p> <p>Key Requirements: - Execute operations that may take minutes to complete - Provide immediate acknowledgment to users when operations start - Deliver real-time progress updates during execution - Support operations across multiple modules (file uploads, validations, data processing, etc.) - Maintain security through proper authentication - Enable horizontal scaling for increased load</p> <p>Constraints: - HTTP request timeout limitations (typically 30-60 seconds) - Need for immediate user feedback while processing continues - Browser connection limitations for long-polling approaches - Resource cleanup requirements for failed operations - User expectation of real-time progress visibility</p> <p>Options Considered: 1. Synchronous HTTP-only processing - Simple but causes timeouts on long operations 2. HTTP with polling - Requires constant client requests, inefficient and increases server load 3. Server-Sent Events (SSE) - One-way communication, limited browser support, connection management complexity 4. Hybrid HTTP/WebSocket with queue-based processing - Balances immediate feedback with async processing and bidirectional communication</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#decision","title":"Decision","text":"<p>We will implement a Long-Running Operations (LRO) pattern using a hybrid HTTP/WebSocket architecture with asynchronous processing via BullMQ for any operation that exceeds typical HTTP timeout thresholds.</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#core-pattern-components","title":"Core Pattern Components","text":"<ol> <li>HTTP Layer (Synchronous Initiation)</li> <li>Endpoint Pattern: <code>POST /{module}/{operation}</code></li> <li>Protocol: HTTP (REST)</li> <li>Responsibility: Initiate operation, perform fast validations, return operation ID</li> <li> <p>Timeout: &lt; 5 seconds for initial response</p> </li> <li> <p>WebSocket Layer (Asynchronous Communication)</p> </li> <li>Protocol: WebSocket with Socket.IO</li> <li>Authentication: JWT token-based</li> <li>Responsibility: Real-time progress updates, completion notifications, error reporting</li> <li> <p>Connection Model: Persistent, bidirectional</p> </li> <li> <p>Queue Layer (Background Execution)</p> </li> <li>Technology: BullMQ with Redis</li> <li>Responsibility: Execute long-running operations asynchronously</li> <li>Job Model: Parent/child job hierarchy for complex operations</li> </ol>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#generic-lro-workflow","title":"Generic LRO Workflow","text":"<pre><code>Client Request (HTTP)\n   \u2193\n[Fast Validation &amp; Setup]\n   \u2193\nOperation ID Generated \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Immediate HTTP Response (200 OK)\n   \u2193\nQueue Job Created\n   \u2193\n[Background Processing]\n   \u2193\nProgress Updates \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba WebSocket Events\n   \u2193\nCompletion/Failure \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba WebSocket Final Event\n   \u2193\n[Resource Cleanup]\n</code></pre>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#pattern-application-examples","title":"Pattern Application Examples","text":"<p>File Upload &amp; Processing: - HTTP: Accept file, validate format, return upload ID - Queue: Parse file, validate data, process batches - WebSocket: Batch progress, validation results, completion summary</p> <p>Bulk Data Validation: - HTTP: Accept validation request, return validation ID - Queue: Execute validation rules across datasets - WebSocket: Progress percentage, rule violations, final report</p> <p>Data Migration/Transformation: - HTTP: Initiate migration, return migration ID - Queue: Transform records, handle dependencies - WebSocket: Records processed, errors encountered, completion status</p> <p>Complex Calculations: - HTTP: Submit calculation request, return calculation ID - Queue: Execute computation, handle intermediate results - WebSocket: Calculation progress, partial results, final output</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#decoupling-scalability","title":"Decoupling &amp; Scalability","text":"<ul> <li>Decouples client from processing lifecycle: HTTP response returns immediately, allowing users to continue working while operation executes</li> <li>Prevents timeout cascades: Long operations don't hold HTTP connections open, avoiding timeout failures and connection pool exhaustion</li> <li>Horizontal scaling capability: Queue workers can be added independently of API servers, allowing targeted scaling for processing bottlenecks</li> <li>Load distribution: Redis-backed queue distributes work across multiple workers automatically</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#user-experience","title":"User Experience","text":"<ul> <li>Immediate acknowledgment: Users get instant feedback that their operation was accepted</li> <li>Real-time visibility: Continuous progress updates reduce anxiety and uncertainty during long operations</li> <li>Resumable operations: Users can disconnect and reconnect; progress state is maintained server-side</li> <li>Detailed feedback: Granular progress and error information helps users understand what's happening</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#reliability-resilience","title":"Reliability &amp; Resilience","text":"<ul> <li>Automatic retry mechanisms: BullMQ provides built-in retry logic for transient failures</li> <li>Job persistence: Operations survive server restarts; Redis stores job state</li> <li>Error isolation: Failed operations don't crash the API server</li> <li>Resource cleanup: Systematic cleanup on success or failure prevents resource leaks</li> <li>Observability: Operation IDs correlate events across all system layers</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#developer-experience","title":"Developer Experience","text":"<ul> <li>Consistent pattern: Same architecture applies to all LROs across modules</li> <li>Clear separation of concerns: HTTP for initiation, WebSocket for communication, Queue for execution</li> <li>Testable components: Each layer can be tested independently</li> <li>Reusable infrastructure: Queue workers, WebSocket handlers, and HTTP controllers follow standard patterns</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#trade-offs-accepted","title":"Trade-offs Accepted","text":"<ul> <li>Increased system complexity: Three-layer architecture requires more infrastructure than simple synchronous approach</li> <li>Eventual consistency: Brief delay between initiation and completion</li> <li>Infrastructure dependencies: Redis and WebSocket server required</li> <li>Connection management overhead: Must handle WebSocket lifecycle (connect, disconnect, reconnect)</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#consequences","title":"Consequences","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#positive","title":"Positive","text":"<ul> <li>Handles operations of any duration: No artificial limits imposed by HTTP timeouts</li> <li>Production-ready scalability: Proven pattern handles high load with horizontal scaling</li> <li>Enhanced user experience: Users stay informed throughout operation lifecycle</li> <li>Flexible application: Pattern works for uploads, validations, migrations, calculations, and more</li> <li>Operational visibility: Complete tracing from initiation through completion</li> <li>Clean architecture: Well-defined boundaries between layers</li> <li>Graceful degradation: Operations continue even if WebSocket temporarily disconnects</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#negative","title":"Negative","text":"<ul> <li>Infrastructure complexity: Requires Redis, WebSocket server, and queue workers</li> <li>Development overhead: More moving parts to develop, test, and debug</li> <li>Operational burden: More services to monitor, maintain, and troubleshoot</li> <li>Learning curve: Developers must understand async patterns and queue mechanics</li> <li>Local development complexity: Full stack requires multiple services running locally</li> <li>Network reliability dependency: WebSocket connections can be disrupted by network issues</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#upload-status-response-object-documentation","title":"Upload Status Response Object Documentation","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#overview","title":"Overview","text":"<p>This response object represents the result of a batch invoice upload operation, providing comprehensive status information, validation results, and error tracking for each uploaded invoice.</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#structure","title":"Structure","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#root-array","title":"Root Array","text":"<p>The response is a tuple containing: 1. Type identifier (string): <code>\"upload-status\"</code> 2. Status payload (object): Contains all upload processing details</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#status-payload-properties","title":"Status Payload Properties","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#id-string","title":"<code>id</code> (string)","text":"<p>Unique identifier for the upload batch operation. - Format: <code>U-{timestamp}-{uuid}</code> - Example: <code>\"U-1761481107580-6b830793-371b-4fc3-b94d-bbd40b24d5fa\"</code></p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#summary-object","title":"<code>summary</code> (object)","text":"<p>Aggregated statistics for the entire upload batch.</p> Field Type Description <code>totalAmount</code> string Formatted sum of all invoice amounts (includes currency symbol) <code>countE</code> number Count of items with Error status <code>countW</code> number Count of items with Warning status <code>countS</code> number Count of items with Success status <code>totalUploads</code> number Total number of invoices uploaded"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#items-array","title":"<code>items</code> (array)","text":"<p>Detailed results for each uploaded invoice.</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#item-object-properties","title":"Item Object Properties","text":"Field Type Description <code>invoiceNo</code> string Invoice identifier <code>invoiceAmount</code> number Invoice monetary value (numeric, no formatting) <code>status</code> string Processing status: <code>\"S\"</code> (Success), <code>\"E\"</code> (Error), <code>\"W\"</code> (Warning) <code>headerValidation</code> object Validation results for invoice header fields <code>detailValidation</code> object Validation results for invoice line items/details <code>dbError</code> string | null Database-level error message, if any"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#validation-object-structure","title":"Validation Object Structure","text":"<p>Both <code>headerValidation</code> and <code>detailValidation</code> contain: - <code>errors</code> (string[]): Array of error messages preventing successful processing - <code>warnings</code> (string[]): Array of non-blocking warning messages</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#unprocesseditems-array","title":"<code>unprocessedItems</code> (array)","text":"<p>Items that could not be processed, with error details.</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#unprocessed-item-object","title":"Unprocessed Item Object","text":"Field Type Description <code>vendorNo</code> string Vendor identifier associated with the failed item <code>invoiceNo</code> string Invoice identifier <code>error</code> object Structured error information"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#error-object","title":"Error Object","text":"Field Type Description <code>code</code> string Machine-readable error code (e.g., <code>\"VENDOR_NOT_FOUND\"</code>) <code>message</code> string Human-readable error description <code>field</code> string Field name that caused the error"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#status-string","title":"<code>status</code> (string)","text":"<p>Overall processing status of the upload batch. - Possible values: <code>\"Completed\"</code>, <code>\"Processing\"</code>, <code>\"Failed\"</code> (implementation-specific)</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#status-code-meanings","title":"Status Code Meanings","text":"<ul> <li><code>S</code> (Success): Invoice passed all validations and was processed successfully</li> <li><code>E</code> (Error): Invoice failed critical validations and was not processed</li> <li><code>W</code> (Warning): Invoice has non-critical issues but was processed</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#key-observations","title":"Key Observations","text":"<ol> <li>Duplicate Handling: The same <code>invoiceNo</code> can appear multiple times with different statuses (e.g., <code>\"RAJ-FLEXI-002\"</code> appears with both <code>S</code> and <code>E</code> statuses)</li> <li>Validation Separation: Header and detail validations are tracked independently</li> <li>Error Redundancy: Failed items appear in both <code>items</code> (with status <code>E</code>) and <code>unprocessedItems</code> arrays</li> <li>Amount Formatting: <code>summary.totalAmount</code> is formatted with currency symbol, while <code>items[].invoiceAmount</code> is numeric only</li> </ol>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/","title":"Upload Status Response Object Documentation","text":""},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#overview","title":"Overview","text":"<p>This response object represents the result of a batch invoice upload operation, providing comprehensive status information, validation results, and error tracking for each uploaded invoice.</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#structure","title":"Structure","text":""},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#root-array","title":"Root Array","text":"<p>The response is a tuple containing: 1. Type identifier (string): <code>\"upload-status\"</code> 2. Status payload (object): Contains all upload processing details</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#status-payload-properties","title":"Status Payload Properties","text":""},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#id-string","title":"<code>id</code> (string)","text":"<p>Unique identifier for the upload batch operation. - Format: <code>U-{timestamp}-{uuid}</code> - Example: <code>\"U-1761481107580-6b830793-371b-4fc3-b94d-bbd40b24d5fa\"</code></p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#summary-object","title":"<code>summary</code> (object)","text":"<p>Aggregated statistics for the entire upload batch.</p> Field Type Description <code>totalAmount</code> string Formatted sum of all invoice amounts (includes currency symbol) <code>countE</code> number Count of items with Error status <code>countW</code> number Count of items with Warning status <code>countS</code> number Count of items with Success status <code>totalUploads</code> number Total number of invoices uploaded"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#items-array","title":"<code>items</code> (array)","text":"<p>Detailed results for each uploaded invoice.</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#item-object-properties","title":"Item Object Properties","text":"Field Type Description <code>invoiceNo</code> string Invoice identifier <code>invoiceAmount</code> number Invoice monetary value (numeric, no formatting) <code>status</code> string Processing status: <code>\"S\"</code> (Success), <code>\"E\"</code> (Error), <code>\"W\"</code> (Warning) <code>headerValidation</code> object Validation results for invoice header fields <code>detailValidation</code> object Validation results for invoice line items/details <code>dbError</code> string | null Database-level error message, if any"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#validation-object-structure","title":"Validation Object Structure","text":"<p>Both <code>headerValidation</code> and <code>detailValidation</code> contain: - <code>errors</code> (string[]): Array of error messages preventing successful processing - <code>warnings</code> (string[]): Array of non-blocking warning messages</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#unprocesseditems-array","title":"<code>unprocessedItems</code> (array)","text":"<p>Items that could not be processed, with error details.</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#unprocessed-item-object","title":"Unprocessed Item Object","text":"Field Type Description <code>vendorNo</code> string Vendor identifier associated with the failed item <code>invoiceNo</code> string Invoice identifier <code>error</code> object Structured error information"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#error-object","title":"Error Object","text":"Field Type Description <code>code</code> string Machine-readable error code (e.g., <code>\"VENDOR_NOT_FOUND\"</code>) <code>message</code> string Human-readable error description <code>field</code> string Field name that caused the error"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#status-string","title":"<code>status</code> (string)","text":"<p>Overall processing status of the upload batch. - Possible values: <code>\"Completed\"</code>, <code>\"Processing\"</code>, <code>\"Failed\"</code> (implementation-specific)</p>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#status-code-meanings","title":"Status Code Meanings","text":"<ul> <li><code>S</code> (Success): Invoice passed all validations and was processed successfully</li> <li><code>E</code> (Error): Invoice failed critical validations and was not processed</li> <li><code>W</code> (Warning): Invoice has non-critical issues but was processed</li> </ul>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#key-observations","title":"Key Observations","text":"<ol> <li>Duplicate Handling: The same <code>invoiceNo</code> can appear multiple times with different statuses (e.g., <code>\"RAJ-FLEXI-002\"</code> appears with both <code>S</code> and <code>E</code> statuses)</li> <li>Validation Separation: Header and detail validations are tracked independently</li> <li>Error Redundancy: Failed items appear in both <code>items</code> (with status <code>E</code>) and <code>unprocessedItems</code> arrays</li> <li>Amount Formatting: <code>summary.totalAmount</code> is formatted with currency symbol, while <code>items[].invoiceAmount</code> is numeric only</li> </ol>"},{"location":"Front%20End%20ADR%27s/Uproccesed%20Records%20Response/#concrete-example","title":"Concrete Example","text":"<pre><code>[\n    \"upload-status\",\n    {\n        \"id\": \"U-1761481107580-6b830793-371b-4fc3-b94d-bbd40b24d5fa\",\n        \"summary\": {\n            \"totalAmount\": \"$800.00\",\n            \"countE\": 2,\n            \"countW\": 0,\n            \"countS\": 2,\n            \"totalUploads\": 4\n        },\n        \"items\": [\n            {\n                \"invoiceNo\": \"RAJ-FLEXI-001\",\n                \"invoiceAmount\": 100,\n                \"status\": \"S\",\n                \"headerValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"detailValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"dbError\": null\n            },\n            {\n                \"invoiceNo\": \"RAJ-FLEXI-002\",\n                \"invoiceAmount\": 200,\n                \"status\": \"S\",\n                \"headerValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"detailValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"dbError\": null\n            },\n            {\n                \"invoiceNo\": \"RAJ-FLEXI-002\",\n                \"invoiceAmount\": 200,\n                \"status\": \"E\",\n                \"headerValidation\": {\n                    \"errors\": [\n                        \"Vendor with vendorNo: 0 and companyNo: 10 not found.\",\n                        \"Vendor 0 not found for company 10\"\n                    ],\n                    \"warnings\": [\n                        \"discountDueDate: Missed Discount: Voucher entered on or after discount due date\"\n                    ]\n                },\n                \"detailValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"dbError\": null\n            },\n            {\n                \"invoiceNo\": \"RAJ-FLEXI-003\",\n                \"invoiceAmount\": 300,\n                \"status\": \"E\",\n                \"headerValidation\": {\n                    \"errors\": [\n                        \"Vendor with vendorNo: 0 and companyNo: 10 not found.\",\n                        \"Vendor 0 not found for company 10\"\n                    ],\n                    \"warnings\": [\n                        \"discountDueDate: Missed Discount: Voucher entered on or after discount due date\"\n                    ]\n                },\n                \"detailValidation\": {\n                    \"errors\": [],\n                    \"warnings\": []\n                },\n                \"dbError\": null\n            }\n        ],\n        \"unprocessedItems\": [\n            {\n                \"vendorNo\": \"0\",\n                \"invoiceNo\": \"RAJ-FLEXI-002\",\n                \"error\": {\n                    \"code\": \"VENDOR_NOT_FOUND\",\n                    \"message\": \"WE DIDN'T FIND ANY VENDOR WITH 0 AND INVOICE NO RAJ-FLEXI-002\",\n                    \"field\": \"vendorNo\"\n                }\n            },\n            {\n                \"vendorNo\": \"0\",\n                \"invoiceNo\": \"RAJ-FLEXI-003\",\n                \"error\": {\n                    \"code\": \"VENDOR_NOT_FOUND\",\n                    \"message\": \"WE DIDN'T FIND ANY VENDOR WITH 0 AND INVOICE NO RAJ-FLEXI-003\",\n                    \"field\": \"vendorNo\"\n                }\n            }\n        ],\n        \"status\": \"Completed\"\n    }\n]\n</code></pre>"},{"location":"Process%20Documentation/","title":"Delivery, QA, and IBM i Playbooks","text":"<p>Use this table of contents to jump straight to the checklist you need. Every document is written as simple bullet points so you can execute the task without rereading the full codebase each time.</p>"},{"location":"Process%20Documentation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Legacy Table to API Resource Mapping</li> <li>Integrations with External Apps</li> <li>Sign Off User Stories and BRD</li> <li>Approve QA Test Cases from Acceptance Criteria</li> <li>Code Review \u2013 Node.js</li> <li>Code Review \u2013 RPG</li> <li>RPG Program Signoff (Syntax/Compile)</li> <li>RPG Testing End-to-End</li> <li>Sign Off on UAT (Stage)</li> <li>Transfer RPG Programs to Power 10</li> <li>Own Go-Live Tasks</li> <li>Issues After Deployment</li> <li>Feature Change Requests</li> </ol>"},{"location":"Process%20Documentation/#need-tool-by-tool-instructions","title":"Need tool-by-tool instructions?","text":"<ul> <li>Head over to the Tool-Specific Delivery Playbooks for walkthroughs that call out the exact steps inside VS Code, IBM Navigator, IBM Data Studio, 5250/PDM, and related utilities for each task above.</li> </ul>"},{"location":"Process%20Documentation/#want-to-see-a-completed-example","title":"Want to see a completed example?","text":"<ul> <li>Review the End-to-End Example Runs for realistic scenarios that show how each checklist is executed, which artifacts to collect, and how to close the loop with IBM i teams.</li> </ul>"},{"location":"Process%20Documentation/approve-qa-test-cases-from-acceptance-criteria/","title":"Approve QA Test Cases from Acceptance Criteria","text":"<p>Run through this checklist before you sign off on any manual or automated QA plan.</p>"},{"location":"Process%20Documentation/approve-qa-test-cases-from-acceptance-criteria/#prep","title":"Prep","text":"<ul> <li>Open the relevant Jest specs (e.g., <code>src/modules/voucher/voucher.controller.spec.ts</code>).</li> <li>Ensure you can run the common scripts: <code>yarn test</code>, <code>yarn test:cov</code>, and <code>yarn lint</code>.</li> <li>Have the IBM validation queries ready, such as repository calls in <code>SpooledMetaDataReportsRepository</code>.</li> </ul>"},{"location":"Process%20Documentation/approve-qa-test-cases-from-acceptance-criteria/#steps","title":"Steps","text":"<ol> <li>Trace each acceptance criterion. For every bullet in the user story, confirm there is at least one test case referencing the same inputs and expected outputs.</li> <li>Verify automation coverage. Identify which scenarios are already handled by Jest or integration tests. Mark any gaps that still need manual execution and explain why automation is not feasible.</li> <li>Check data setup. Document how to seed required DB2 tables (fixtures, scripts, or RPG jobs). Include file/library names so QA can prep the environment quickly.</li> <li>List execution commands. Provide the exact CLI command(s) QA should run (<code>yarn test voucher</code>, <code>yarn test --watch</code>, IBM CL commands, etc.).</li> <li>Define pass/fail evidence. Describe what proof QA must attach (console output, screenshots, spool file IDs) to show the test case passed.</li> <li>Review regression impact. Ensure smoke/regression suites are triggered whenever applicable. Note which pipelines (Azure, Jenkins) must show green checks before approval.</li> </ol>"},{"location":"Process%20Documentation/approve-qa-test-cases-from-acceptance-criteria/#sign-off","title":"Sign-off","text":"<ul> <li>Record who approved the test cases and the date.</li> <li>Store links to the final test plan and any exported reports for traceability.</li> </ul>"},{"location":"Process%20Documentation/code-review-nodejs/","title":"Code Review \u2013 Node.js","text":"<p>Use these bullets to complete a consistent review of NestJS pull requests.</p>"},{"location":"Process%20Documentation/code-review-nodejs/#prep","title":"Prep","text":"<ul> <li>Skim <code>docs/DEBUGGING.md</code> so you remember the logging and queue hooks that must remain intact.</li> <li>Run the author\u2019s branch locally and execute <code>yarn lint</code>, <code>yarn test</code>, and <code>yarn build</code> if they did not attach output.</li> <li>Keep the relevant module files open (controllers, services, repositories, DTOs).</li> </ul>"},{"location":"Process%20Documentation/code-review-nodejs/#steps","title":"Steps","text":"<ol> <li>Check architectural boundaries. Ensure modules only import from allowed layers (controllers \u2192 services \u2192 repositories). Flag any cross-module shortcuts or circular imports.</li> <li>Validate dependency injection. Confirm providers are registered in their modules and that constructors only inject what they use. Remove dead injections.</li> <li>Review DTOs and validation. Make sure new endpoints declare DTO classes with <code>class-validator</code> decorators and Swagger metadata.</li> <li>Inspect database changes. Compare Sequelize model edits with the table registry. Confirm migrations or IBM updates are documented if column names/lengths changed.</li> <li>Evaluate background jobs. For Bull/BullMQ handlers, check retry settings, concurrency, and logging so operations can trace failures.</li> <li>Enforce tooling gates. Require proof that linting, tests, and builds pass. If the PR adds scripts, ensure they are wired into <code>package.json</code> and CI.</li> <li>Request documentation updates. If behavior, configuration, or debugging paths changed, confirm the README or relevant guide was updated.</li> </ol>"},{"location":"Process%20Documentation/code-review-nodejs/#sign-off","title":"Sign-off","text":"<ul> <li>Leave a summary comment highlighting any follow-up work.</li> <li>Approve only when all checklist items pass and CI is green.</li> </ul>"},{"location":"Process%20Documentation/code-review-rpg/","title":"Code Review \u2013 RPG","text":"<p>This checklist keeps RPG reviewers aligned with the Node-facing contracts.</p>"},{"location":"Process%20Documentation/code-review-rpg/#prep","title":"Prep","text":"<ul> <li>Gather the RPG source or CL scripts under review plus any IBM project references.</li> <li>Open <code>src/modules/spooled-metadata-report/entities/spooled-metadata-report.entity.ts</code> to see the expected fields and data types.</li> <li>Keep <code>src/modules/spooled-metadata-report/schema.ts</code> handy for column names and lengths.</li> </ul>"},{"location":"Process%20Documentation/code-review-rpg/#steps","title":"Steps","text":"<ol> <li>Validate inputs/outputs. Confirm the RPG program reads and writes every field required by the entity (job name, queue, report type, etc.). Note any missing assignments.</li> <li>Check library bindings. Ensure the program references the libraries resolved by <code>GSSETENV</code>. If a different library is required, document the override and reason.</li> <li>Review error handling. Confirm the program returns clear status codes or messages via <code>errmsg_out</code> so Node logs show meaningful failures.</li> <li>Inspect queue/report usage. Compare the output queue names and report defaults with <code>generate-report-config.ts</code>. Flag discrepancies so downstream processes keep working.</li> <li>Examine performance/locking. Look for unnecessary exclusive locks, missing commitment control, or loops that could cause long-running jobs.</li> <li>Confirm compile instructions. Note the exact commands (CRTBNDRPG, CRTRPGMOD, etc.), required options, and any post-compile steps so build engineers can reproduce.</li> </ol>"},{"location":"Process%20Documentation/code-review-rpg/#sign-off","title":"Sign-off","text":"<ul> <li>Document review findings in the ticket and mention any required IBM promotions.</li> <li>Approve only after compile instructions and error handling expectations are clear.</li> </ul>"},{"location":"Process%20Documentation/feature-change-requests/","title":"Feature Change Requests","text":"<p>Use this checklist to evaluate and plan new or modified functionality.</p>"},{"location":"Process%20Documentation/feature-change-requests/#prep","title":"Prep","text":"<ul> <li>Keep <code>README.md</code> open for the architecture map.</li> <li>Review <code>docs/DEBUGGING.md</code> to understand existing observability and IBM touchpoints.</li> <li>Pull the relevant module code so you can reference files directly in your notes.</li> </ul>"},{"location":"Process%20Documentation/feature-change-requests/#steps","title":"Steps","text":"<ol> <li>Clarify the request. Write a short summary of the feature, its users, and success metrics. Include any regulatory or SLA drivers.</li> <li>Identify impacted modules. List controllers, services, repositories, queues, and RPG programs that will change. Provide file paths for quick lookup.</li> <li>Map data dependencies. Note every DB2 table or spool file affected. Reference the table registry entries and explain whether schema changes are required.</li> <li>Assess external integrations. Document any partner systems, file formats, or APIs that must be updated and which team owns each integration.</li> <li>Plan testing and debugging. Specify which automated suites will be extended, what manual IBM validation is needed, and how existing debugging hooks must evolve.</li> <li>Estimate effort and risks. Provide T-shirt sizes or story points plus a risk list (performance, security, data migrations). Suggest mitigations for each risk.</li> <li>Record approvals. Capture product, architecture, and IBM signoffs before implementation begins.</li> </ol>"},{"location":"Process%20Documentation/feature-change-requests/#sign-off","title":"Sign-off","text":"<ul> <li>Store the completed checklist with the feature ticket or in <code>docs/delivery</code>.</li> <li>Update the delivery roadmap or BRD once the request is accepted.</li> </ul>"},{"location":"Process%20Documentation/integrations-with-external-apps/","title":"Integrations with External Apps","text":"<p>Follow these bullets to onboard or troubleshoot any third-party system that exchanges files or API calls with ARG.</p>"},{"location":"Process%20Documentation/integrations-with-external-apps/#prep","title":"Prep","text":"<ul> <li>Open <code>src/modules/voucher/voucher.controller.ts</code> to see every exposed route plus Swagger decorators.</li> <li>Review the processor under <code>src/modules/voucher/processors</code> (e.g., <code>flexi.processor.ts</code>) to understand how background jobs run.</li> <li>Skim <code>docs/DEBUGGING.md</code> for the end-to-end NestJS \u2194 IBM troubleshooting flow.</li> </ul>"},{"location":"Process%20Documentation/integrations-with-external-apps/#steps","title":"Steps","text":"<ol> <li>Describe the entry point. List the HTTP method, URL, and DTO for each integration. Example bullet: \u201c<code>POST /voucher/flexi/upload</code> expects <code>FlexiUploadDto</code> with fields \u2026\u201d.</li> <li>Show authentication expectations. Mention how the partner should authenticate (API key, OAuth, VPN allow list) and point to the middleware or guard that enforces it.</li> <li>Outline the processing pipeline. Create a mini-sequence such as \u201cUpload hits controller \u2192 Bull queue (<code>flexi</code>) \u2192 <code>FlexiProcessor</code> batches and validates \u2192 repository saves data \u2192 websocket notifies UI\u201d.</li> <li>Call out required IBM assets. Note which RPG programs, libraries, or spool files the integration ultimately touches so IBM teams can prep promotions.</li> <li>Provide monitoring hooks. Include the loggers, metrics, or Bull board to watch (<code>/admin/queues</code>, Kibana index, etc.) along with the command to tail logs (<code>yarn start:dev</code> locally, <code>kubectl logs</code> in prod).</li> <li>List debugging steps. Link the relevant section of <code>docs/DEBUGGING.md</code> plus any SQL scripts or joblog retrieval commands unique to this integration.</li> </ol>"},{"location":"Process%20Documentation/integrations-with-external-apps/#sign-off","title":"Sign-off","text":"<ul> <li>Share the document with the external partner so they can validate the payload shape and SLA.</li> <li>Capture any agreed retry/alerting rules and store them next to the integration ticket.</li> </ul>"},{"location":"Process%20Documentation/issues-after-deployment/","title":"Issues After Deployment","text":"<p>Use these bullets to triage and resolve incidents quickly after a release.</p>"},{"location":"Process%20Documentation/issues-after-deployment/#prep","title":"Prep","text":"<ul> <li>Open <code>docs/DEBUGGING.md</code> for the Node \u2194 RPG troubleshooting workflow.</li> <li>Ensure you have access to production logs, Bull queues, and IBM job logs/output queues.</li> <li>Gather the deployment details (build number, timestamp, change list).</li> </ul>"},{"location":"Process%20Documentation/issues-after-deployment/#steps","title":"Steps","text":"<ol> <li>Stabilize the environment. Decide whether to rollback, hotfix, or continue monitoring. Document the decision and stakeholders involved.</li> <li>Capture evidence. Record error messages, screenshots, API payloads, and IBM job numbers as soon as the issue is reported.</li> <li>Trace the flow. Follow the debugging guide from HTTP request \u2192 queue \u2192 processor \u2192 database \u2192 RPG job \u2192 spool file to pinpoint the failing hop.</li> <li>Inspect persisted metadata. Query <code>SpooledMetaDataReportsRepository</code> or relevant tables to verify whether jobs completed and what filenames/queues they used.</li> <li>Collect IBM artifacts. Download job logs, spool files, or dumps referenced by the incident for later analysis.</li> <li>Communicate status. Provide regular updates to the incident channel, including ETA for fixes and any customer impact.</li> <li>Apply fix or rollback. Execute the chosen remediation (config change, code patch, revert) and note every command or deployment performed.</li> <li>Verify resolution. Re-run the failing scenario plus a small regression set. Confirm monitoring alerts have cleared.</li> </ol>"},{"location":"Process%20Documentation/issues-after-deployment/#sign-off","title":"Sign-off","text":"<ul> <li>Publish a post-incident summary with root cause, fix, and follow-up tasks.</li> <li>Store all logs and evidence in the incident ticket for audit purposes.</li> </ul>"},{"location":"Process%20Documentation/legacy-table-to-api-resource-mapping/","title":"Legacy Table to API Resource Mapping","text":"<p>Use this checklist any time you have to prove how a REST endpoint maps to RPG or DB2 tables.</p>"},{"location":"Process%20Documentation/legacy-table-to-api-resource-mapping/#prep","title":"Prep","text":"<ul> <li>Open <code>src/shared/config/constants/table-registry.ts</code> so you can reference the authoritative list of physical file names and descriptions.</li> <li>Keep <code>src/shared/config/env-library-config.ts</code> handy to see which schema/library each table uses per environment (<code>dev</code>, <code>test</code>, <code>uat</code>, <code>prod</code>).</li> <li>Locate the Sequelize model(s) for the API you are mapping (for example, <code>src/modules/spooled-metadata-report/schema.ts</code>).</li> </ul>"},{"location":"Process%20Documentation/legacy-table-to-api-resource-mapping/#steps","title":"Steps","text":"<ol> <li>Identify the resource. Note the controller or resolver that owns the API (e.g., <code>VoucherController</code>) and list every DTO field that needs a backing column.</li> <li>Match the table name. Use the table registry to find the physical file(s) mentioned in the use case. Highlight the matching entry so reviewers know the official spelling and description (<code>APTRANH</code>, <code>SPLFMETA</code>, etc.).</li> <li>Lock in the schema per environment. Call out the output of <code>getTableAndSchemaInfo()</code> for each environment involved in the BRD so implementers know which library to query or promote.</li> <li>Map every field. Walk through the Sequelize schema definition and create a bullet mapping such as \u201c<code>reportType</code> \u2192 <code>MERPTNAM</code> (char 10)\u201d or \u201c<code>jobNumber</code> \u2192 <code>JOBNBR</code> (numeric)\u201d.</li> <li>Document transforms or joins. If an API field is derived (for example, concatenating job identifiers), add a note about the helper or SQL that performs the transformation.</li> <li>Share validation steps. Include the Jest spec or Postman collection that exercises the mapping so QA can confirm the table/resource link with a single command (<code>yarn test</code> for automated suites).</li> </ol>"},{"location":"Process%20Documentation/legacy-table-to-api-resource-mapping/#sign-off","title":"Sign-off","text":"<ul> <li>Confirm every REST field now lists a DB2 column and table.</li> <li>Attach the mapping doc to the BRD or ticket so future developers do not need to repeat this investigation.</li> </ul>"},{"location":"Process%20Documentation/own-go-live-tasks/","title":"Own Go-Live Tasks","text":"<p>Use this checklist to lead a production release from start to finish.</p>"},{"location":"Process%20Documentation/own-go-live-tasks/#prep","title":"Prep","text":"<ul> <li>Confirm the release scope, change tickets, and rollback plan.</li> <li>Ensure Docker images or builds are ready (run <code>yarn build</code>, <code>docker build</code>, etc.).</li> <li>Coordinate timing with IBM teams responsible for RPG promotions and spool queues.</li> </ul>"},{"location":"Process%20Documentation/own-go-live-tasks/#steps","title":"Steps","text":"<ol> <li>Validate artifacts. Run automated tests (<code>yarn lint</code>, <code>yarn test</code>, <code>yarn build</code>) and confirm CI is green. Record build IDs and Docker tags.</li> <li>Review deployment script. Walk through <code>deploy.sh</code> (login, pull image, stop old container, start new). Update environment variables or secrets if needed.</li> <li>Announce freeze window. Notify stakeholders of the deployment time, expected impact, and communication channel (Teams, Slack, bridge line).</li> <li>Execute deployment. Follow the script, capturing console output and timestamps for each command.</li> <li>Run smoke tests. Hit key APIs and user journeys, plus verify IBM queue/library connectivity (<code>DynamicLibraryManager</code> output, report generation, etc.).</li> <li>Monitor systems. Watch logs, APM dashboards, and Bull queue UIs for anomalies during the launch window.</li> <li>Coordinate IBM steps. Ensure RPG promotions, spool queue routing, or CL commands finish successfully. Capture job logs if issues appear.</li> <li>Communicate status. Provide periodic updates until the release is stable, including any hotfix or rollback decisions.</li> </ol>"},{"location":"Process%20Documentation/own-go-live-tasks/#sign-off","title":"Sign-off","text":"<ul> <li>Document the final status, including production verification results and any incidents.</li> <li>Archive deployment logs and share them with the operations team.</li> </ul>"},{"location":"Process%20Documentation/rpg-program-signoff-syntax-compile/","title":"RPG Program Signoff (Syntax/Compile)","text":"<p>Use this checklist to confirm an RPG program is ready to promote after compiling cleanly.</p>"},{"location":"Process%20Documentation/rpg-program-signoff-syntax-compile/#prep","title":"Prep","text":"<ul> <li>Obtain the latest source member and compile commands from the developer.</li> <li>Ensure access to the target IBM environment (DEV/TEST/UAT/PROD) plus the correct library list.</li> <li>Keep <code>DynamicLibraryManager</code> documentation handy for reference on library resolution.</li> </ul>"},{"location":"Process%20Documentation/rpg-program-signoff-syntax-compile/#steps","title":"Steps","text":"<ol> <li>Verify environment setup. Run <code>GSSETENV</code> or the equivalent CL to confirm the program sees the correct data, work, and stored procedure libraries. Record the output values.</li> <li>Compile with tracing enabled. Execute the provided CRTBNDRPG/CRTRPGMOD commands. Capture the compile listing and highlight any warnings that still need signoff.</li> <li>Check error messaging. Ensure the program writes failures to <code>errmsg_out</code> or another agreed data structure so Node logs surface compile/runtime errors.</li> <li>Validate external dependencies. Confirm all called procedures, service programs, and files exist in the target libraries. Update the checklist if stubs or mocks were used.</li> <li>Align with Sequelize schema. Cross-check field names and lengths against the Node schemas to avoid runtime truncation or mismatch.</li> <li>Document deployment steps. Write down the exact commands (library overrides, binding directories, authorization lists) required so operations can reproduce the compile.</li> </ol>"},{"location":"Process%20Documentation/rpg-program-signoff-syntax-compile/#sign-off","title":"Sign-off","text":"<ul> <li>Attach the successful compile listing and command history to the ticket.</li> <li>Note any outstanding warnings and who accepted them.</li> </ul>"},{"location":"Process%20Documentation/rpg-testing-e2e/","title":"RPG Testing End-to-End","text":"<p>Follow this checklist to validate an RPG flow from API call through spool file creation.</p>"},{"location":"Process%20Documentation/rpg-testing-e2e/#prep","title":"Prep","text":"<ul> <li>Review <code>docs/DEBUGGING.md</code>, especially the Node \u2192 Bull \u2192 RPG \u2192 spool verification section.</li> <li>Confirm access to the target IBM environment, job logs, and output queues.</li> <li>Prepare the REST client or automated script that will trigger the RPG job (Postman, Jest, curl).</li> </ul>"},{"location":"Process%20Documentation/rpg-testing-e2e/#steps","title":"Steps","text":"<ol> <li>Trigger the scenario. Call the appropriate API or queue event and record the request payload, time, and environment.</li> <li>Monitor Node processing. Tail the service logs (<code>yarn start:dev</code>, <code>kubectl logs</code>, etc.) to ensure the request entered the correct queue and processor.</li> <li>Confirm RPG execution. Capture the job name, number, and user from IBM job logs. Note any messages or warnings.</li> <li>Verify database writes. Query the repositories (e.g., <code>SpooledMetaDataReportsRepository</code>) or DB2 tables to confirm rows were inserted/updated.</li> <li>Check spool/output queues. Locate the generated reports, download them if needed, and compare to expected content/format.</li> <li>Document variances. Record any mismatched fields, missing files, or timing issues plus the log excerpts that prove the problem.</li> <li>Repeat for edge cases. Run negative and boundary scenarios listed in the acceptance criteria.</li> </ol>"},{"location":"Process%20Documentation/rpg-testing-e2e/#sign-off","title":"Sign-off","text":"<ul> <li>Attach screenshots or text dumps of job logs, spool files, and API responses.</li> <li>Record who performed the test, the date, and the environment.</li> </ul>"},{"location":"Process%20Documentation/sign-off-on-uat-stage/","title":"Sign Off on UAT (Stage)","text":"<p>Use these bullets to release a build into the stage/UAT environment confidently.</p>"},{"location":"Process%20Documentation/sign-off-on-uat-stage/#prep","title":"Prep","text":"<ul> <li>Verify the <code>.env</code> or pipeline variables reference the <code>uat</code> section in <code>env-library-config.ts</code>.</li> <li>Review the Swagger output for the build (<code>/api-docs</code>) to confirm endpoints and DTOs.</li> <li>Ensure access to the shared output queues and data libraries used in UAT.</li> </ul>"},{"location":"Process%20Documentation/sign-off-on-uat-stage/#steps","title":"Steps","text":"<ol> <li>Confirm configuration. Double-check service URLs, credentials, and library overrides match the UAT plan. Document any differences from PROD.</li> <li>Deploy the build. Run the approved deployment steps (<code>yarn build</code>, Docker image push, <code>deploy.sh</code>, etc.) and capture console output.</li> <li>Smoke test APIs. Execute a short list of high-value requests (voucher upload/download, report generation). Record status codes and payload snippets.</li> <li>Validate IBM outputs. Trigger at least one report or job that touches IBM i resources. Confirm spool files appear in the expected queues/libraries.</li> <li>Review monitoring/alerts. Ensure logging, metrics, and alert rules are pointing to the correct environment (e.g., UAT-specific Log Analytics workspace).</li> <li>Collect tester readiness info. Share URLs, credentials, and known limitations with the UAT testers so they can start immediately.</li> </ol>"},{"location":"Process%20Documentation/sign-off-on-uat-stage/#sign-off","title":"Sign-off","text":"<ul> <li>Document the deployment timestamp, build SHA, and who performed the release.</li> <li>Obtain written approval from QA/business before promoting further.</li> </ul>"},{"location":"Process%20Documentation/sign-off-user-stories-and-brd/","title":"Sign Off User Stories and BRD","text":"<p>Use these bullets to ensure each business request has complete technical coverage before it leaves discovery.</p>"},{"location":"Process%20Documentation/sign-off-user-stories-and-brd/#prep","title":"Prep","text":"<ul> <li>Keep the repository <code>README.md</code> open for the architecture overview and module list.</li> <li>Reference <code>docs/DEBUGGING.md</code> to remind stakeholders how Node \u2194 RPG \u2194 IBM flows operate.</li> <li>Pull <code>src/shared/config/env-library-config.ts</code> so you can cite the exact IBM libraries touched by each story.</li> </ul>"},{"location":"Process%20Documentation/sign-off-user-stories-and-brd/#steps","title":"Steps","text":"<ol> <li>Confirm scope against architecture. Walk module by module (voucher, purchase journal, shared libs) and note whether the story affects each layer. If yes, jot down the impacted files or directories.</li> <li>List dependencies per story. For every acceptance criterion, note which APIs, queues, RPG programs, or DB tables it touches. Use filenames (<code>voucher.controller.ts</code>, <code>SpooledMetaDataReportsRepository</code>) so engineers can jump directly to the code.</li> <li>Record IBM i considerations. Identify required libraries, CL commands, or spool outputs. Example bullet: \u201cNeeds <code>SPLFMETA</code> write access in <code>DB_DATA_LIB_UAT</code>\u201d.</li> <li>Validate acceptance criteria. Ensure each criterion specifies observable behavior (response payload, spool file name, etc.) and reference the DTO or schema that proves it.</li> <li>Capture testing expectations. Add a note about the automated suites (Jest specs to run, <code>yarn test</code>) plus any manual IBM validation the business expects.</li> <li>Document signers and dates. List who approved the BRD, product signoff, architecture review, and IBM review so the history is auditable.</li> </ol>"},{"location":"Process%20Documentation/sign-off-user-stories-and-brd/#sign-off","title":"Sign-off","text":"<ul> <li>Share the completed checklist with product/engineering leadership.</li> <li>Store the document in the ticket or <code>docs/delivery</code> folder for future audits.</li> </ul>"},{"location":"Process%20Documentation/transfer-rpg-programs-to-power-10/","title":"Transfer RPG Programs to Power 10","text":"<p>Use this checklist when moving RPG assets to new Power 10 hardware.</p>"},{"location":"Process%20Documentation/transfer-rpg-programs-to-power-10/#prep","title":"Prep","text":"<ul> <li>Gather the list of programs, service programs, and files being migrated.</li> <li>Confirm the target Power 10 library structure and any new naming conventions.</li> <li>Coordinate downtime or promotion windows with operations.</li> </ul>"},{"location":"Process%20Documentation/transfer-rpg-programs-to-power-10/#steps","title":"Steps","text":"<ol> <li>Inventory current libraries. Run <code>GSSETENV</code> (or review <code>DynamicLibraryManager</code> output) in the source environment and record the data, work, and stored procedure libraries for each program.</li> <li>Validate target libraries. Compare the recorded list with the Power 10 libraries. Document any renames so Node configs (<code>DB_DATA_LIB_*</code>, etc.) can be updated.</li> <li>Migrate source members. Copy the source (SAVLIB/RSTLIB, RCLSTG) and verify member counts and timestamps after transfer.</li> <li>Recompile on Power 10. Execute the compile scripts in the new environment. Capture listings and confirm there are no hardware-specific warnings.</li> <li>Update application configs. Modify environment variables, table registries, or Sequelize schemas if library names or file paths changed.</li> <li>Run validation tests. Execute representative API flows or RPG harnesses to confirm the new programs behave identically. Record job numbers and spool outputs.</li> <li>Communicate completion. Notify Node and IBM teams of the cutover, including any clean-up tasks on legacy hardware.</li> </ol>"},{"location":"Process%20Documentation/transfer-rpg-programs-to-power-10/#sign-off","title":"Sign-off","text":"<ul> <li>Store migration logs, compile listings, and config diffs in the ticket.</li> <li>Confirm rollback instructions exist before closing the task.</li> </ul>"},{"location":"Process%20Documentation/examples/","title":"Delivery Examples \u2013 End-to-End Runs","text":"<p>Use these example walkthroughs when you need to see how a full task execution looks in practice. Each guide:</p> <ul> <li>Describes a realistic scenario from this NestJS + IBM i stack.</li> <li>Lists every artifact touched (code modules, RPG sources, IBM tools, tickets).</li> <li>Shows the evidence captured for audit and sign-off.</li> </ul>"},{"location":"Process%20Documentation/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Legacy Table to API Resource Mapping</li> <li>Integrations with External Apps</li> <li>Sign Off User Stories and BRD</li> <li>Approve QA Test Cases from Acceptance Criteria</li> <li>Code Review \u2013 Node.js</li> <li>Code Review \u2013 RPG</li> <li>RPG Program Signoff (Syntax/Compile)</li> <li>RPG Testing End-to-End</li> <li>Sign Off on UAT (Stage)</li> <li>Transfer RPG Programs to Power 10</li> <li>Own Go-Live Tasks</li> <li>Issues After Deployment</li> <li>Feature Change Requests</li> </ol>"},{"location":"Process%20Documentation/examples/approve-qa-test-cases-from-acceptance-criteria/","title":"Example \u2013 Approve QA Test Cases from Acceptance Criteria","text":""},{"location":"Process%20Documentation/examples/approve-qa-test-cases-from-acceptance-criteria/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: approve QA regression tests for \"AP Vendor Hold\" story before UAT.</li> <li>Inputs: Acceptance criteria from Azure Boards, test cases drafted in Azure Test Plans, API contract for <code>/vendors/:id/holds</code>, IBM i job notes for <code>APSGACH</code> table.</li> <li>Output: QA suite marked \"Ready\" with explicit traceability to criteria and data sources.</li> </ul>"},{"location":"Process%20Documentation/examples/approve-qa-test-cases-from-acceptance-criteria/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Extract acceptance criteria. Paste each bullet into a checklist so every QA case maps to a single behavior.</li> <li>Review drafted tests. In Test Plans, ensure there is at least one API test (via Postman or automation) and one IBM i validation (verifying the hold row in <code>dataLib.APSGACH</code>).</li> <li>Validate data prep. Confirm QA provided seed scripts or IBM Navigator instructions to create the vendor states needed for the tests.</li> <li>Check automation hooks. Ensure the Jest/E2E folder contains the updated spec (e.g., <code>vendor-hold.e2e-spec.ts</code>) and that it calls the NestJS endpoint with sample data.</li> <li>Provide feedback. Comment directly in Azure Test Plans or Teams, flagging any missing negative scenario (e.g., hold removal without authorization).</li> <li>Approve suite. Once gaps close, change the Test Suite status to \"Ready\", note the acceptance criteria numbers it covers, and tag the QA lead.</li> </ol>"},{"location":"Process%20Documentation/examples/approve-qa-test-cases-from-acceptance-criteria/#evidence-package","title":"Evidence Package","text":"<ul> <li>Screenshot of Azure Test Plans suite showing status = Ready.</li> <li>Link to the Jest E2E test file in Git with the commit hash.</li> <li>Comment snippet demonstrating traceability back to the acceptance criteria IDs.</li> </ul>"},{"location":"Process%20Documentation/examples/code-review-nodejs/","title":"Example \u2013 Code Review (Node.js)","text":""},{"location":"Process%20Documentation/examples/code-review-nodejs/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: review PR #412 that introduces <code>/vouchers/:id/cancel</code> endpoint changes.</li> <li>Inputs: Git diff, Jest results, ESLint output, Swagger updates, IBM i stored procedure references.</li> <li>Output: review comments plus an approval summarizing coverage, risk, and IBM dependencies.</li> </ul>"},{"location":"Process%20Documentation/examples/code-review-nodejs/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Prep environment. Fetch the branch, run <code>yarn</code> to sync dependencies, and start Redis via <code>docker-compose up -d redis</code>.</li> <li>Run automated checks. Execute <code>yarn lint</code>, <code>yarn test</code>, and <code>yarn test:e2e</code>; paste results into the PR so Devs know the baseline.</li> <li>Trace new logic. Inspect <code>cancel-voucher-entry.use-case.ts</code> and related controller/DTO files to ensure validation pipes align with the new payload.</li> <li>Validate IBM calls. Confirm the repository still calls the correct stored procedure (<code>AP200PRC</code>) and that the parameters align with DB2 column types.</li> <li>Check observability. Ensure new branches log context using Winston per <code>docs/DEBUGGING.md</code> so on-call can trace cancellations.</li> <li>Approve or request changes. Summarize key findings (e.g., \"Approved after verifying tests and DB2 mappings\") or block until fixes arrive.</li> </ol>"},{"location":"Process%20Documentation/examples/code-review-nodejs/#evidence-package","title":"Evidence Package","text":"<ul> <li>Terminal output for lint/test/e2e runs.</li> <li>Screenshot or permalink referencing the stored procedure invocation.</li> <li>PR review comment summarizing the IBM dependency validation.</li> </ul>"},{"location":"Process%20Documentation/examples/code-review-rpg/","title":"Example \u2013 Code Review (RPG)","text":""},{"location":"Process%20Documentation/examples/code-review-rpg/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: review RPGLE member <code>AP200R</code> changes that sync with the <code>/vouchers</code> NestJS endpoint.</li> <li>Inputs: Source in <code>QRPGLESRC/AP200R</code>, compile listing, IBM Navigator diff tool, Node.js integration contract.</li> <li>Output: review log covering logic, data structures, and IBM job scheduling impact.</li> </ul>"},{"location":"Process%20Documentation/examples/code-review-rpg/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Open the member. Use RDi or VS Code + Code for IBM i to download <code>AP200R</code> from the DEV library and compare against the previous release.</li> <li>Check data structures. Confirm any new fields map to the API DTO (e.g., <code>CancelReason</code>) and exist in the DB2 tables referenced in <code>table-registry.ts</code>.</li> <li>Validate subprocedure calls. Ensure service programs or stored procedures (like <code>AP200PRC</code>) still receive the correct parameters and that timestamps match Node expectations (UTC vs local).</li> <li>Compile in TEST library. Trigger a <code>CRTBNDRPG</code> or the project build script, capturing the listing with <code>*SRCSTMT</code> enabled.</li> <li>Analyze performance. Review the generated listing for new SQL statements or loops that could impact batch jobs; flag any record locks.</li> <li>Sign the review. Email or comment on the work item summarizing findings and attach the compile listing plus diff snippet.</li> </ol>"},{"location":"Process%20Documentation/examples/code-review-rpg/#evidence-package","title":"Evidence Package","text":"<ul> <li>Annotated diff from RDi/Code for IBM i.</li> <li>Successful compile listing stored in the team SharePoint or attached to the story.</li> <li>Reviewer note referencing the associated NestJS PR to prove parity.</li> </ul>"},{"location":"Process%20Documentation/examples/feature-change-requests/","title":"Example \u2013 Feature Change Requests","text":""},{"location":"Process%20Documentation/examples/feature-change-requests/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: process a change request to add <code>PaymentTerms</code> to the <code>/vendors</code> API and downstream RPG processes.</li> <li>Inputs: CR form, backlog entry, architecture diagrams, DB2 column definitions (<code>APVEND</code>), Node + RPG codebases.</li> <li>Output: approved change summary with implementation/validation plan.</li> </ul>"},{"location":"Process%20Documentation/examples/feature-change-requests/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Capture the request. Log the CR in Azure Boards with justification, affected systems, and desired delivery date.</li> <li>Assess impact. Identify NestJS touchpoints (DTO, Sequelize model, mapper) plus RPG programs (<code>AP200R</code>, <code>APVENY</code> maintenance) needing updates.</li> <li>Estimate effort. Work with Dev leads to size Node vs RPG changes, considering testing + deployment sequencing.</li> <li>Plan validation. Define QA, integration, and IBM regression tests (including spool output updates) required before release.</li> <li>Obtain approvals. Present the plan during CAB/change meeting; capture sign-offs from business owner, Dev lead, and IBM ops.</li> <li>Track execution. Break the CR into actionable tasks/PRs, update statuses weekly, and store all documents in the CR record.</li> </ol>"},{"location":"Process%20Documentation/examples/feature-change-requests/#evidence-package","title":"Evidence Package","text":"<ul> <li>Completed change request form with signatures.</li> <li>Architecture/impact diagram showing affected flows.</li> <li>Link to the epic or feature branch tracking implementation progress.</li> </ul>"},{"location":"Process%20Documentation/examples/integrations-with-external-apps/","title":"Example \u2013 Integrations with External Apps","text":""},{"location":"Process%20Documentation/examples/integrations-with-external-apps/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: push voucher approvals from ARG Web Backend into a partner system via REST webhooks.</li> <li>Inputs: NestJS <code>VoucherController</code>, outbound integration service in <code>src/shared/infrastructure/connection.ts</code>, queue workers defined under <code>src/main/account-payable/application/voucher/usecases</code>.</li> <li>Output: documented handshake covering authentication, payload contract, and IBM i references.</li> </ul>"},{"location":"Process%20Documentation/examples/integrations-with-external-apps/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Gather specs. Download the partner's OpenAPI contract and compare it against <code>src/api-schema/voucher.yml</code> to understand field overlaps.</li> <li>Map auth requirements. Confirm the integration user/secret are stored in Azure Key Vault or <code>.env</code> and referenced in the NestJS <code>ConfigService</code> before coding.</li> <li>Prototype the call. Use VS Code REST Client or Postman to hit the partner sandbox with a payload generated from <code>voucher.dto.ts</code>.</li> <li>Connect to IBM i triggers. Trace how voucher approvals originate from RPG by reviewing the job log for program <code>AP200</code> and ensuring the <code>AP200PRC</code> stored procedure entry in <code>table-registry.ts</code> matches.</li> <li>Implement the webhook. Update the outbound service to call the partner API after a successful <code>AddVoucherEntryUseCase</code>. Add retries via BullMQ if the partner is down.</li> <li>Prove end-to-end flow. Run the NestJS app locally (<code>yarn start:dev</code>), approve a voucher, and show the outbound HTTP 200 plus the IBM job log entry referencing the same voucher number.</li> </ol>"},{"location":"Process%20Documentation/examples/integrations-with-external-apps/#evidence-package","title":"Evidence Package","text":"<ul> <li>Postman collection export with the tested webhook request/response.</li> <li>Screenshot of the IBM Navigator job log referencing the voucher event.</li> <li>Link to the merge request showing the outbound service changes and associated unit test updates.</li> </ul>"},{"location":"Process%20Documentation/examples/issues-after-deployment/","title":"Example \u2013 Issues After Deployment","text":""},{"location":"Process%20Documentation/examples/issues-after-deployment/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: triage a production incident where <code>/vendors</code> returns 500 after release.</li> <li>Inputs: Application Insights logs, IBM job logs, rollback plan, hotfix branch.</li> <li>Output: incident timeline with root cause, fix, and prevention items.</li> </ul>"},{"location":"Process%20Documentation/examples/issues-after-deployment/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Acknowledge alert. Update ticket or respond to user, include RPG Team.</li> <li>Gather data. Pull logs from <code>docs/DEBUGGING.md</code> instructions (Winston correlation IDs) and run <code>DSPJOBLOG</code> for any related IBM batch jobs.</li> <li>Reproduce safely. Hit endpoint in Stage / reproduce, with the same payload to see if the issue repeats; capture the API response and DB queries. Using the Stage or sandbox environment avoids impacting production while debugging. Also, use F12 tools to capture and view errors, or network requests if applicable. </li> <li>Identify root cause. Compare the failing code to the previous commit; e.g., missing column mapping referencing <code>X</code> table.</li> <li>Issue fix. Create a hotfix branch, patch the repository, run <code>yarn test</code> and targeted RPG tests if required, then deploy via emergency pipeline.</li> <li>Close incident. Document the timeline, attach logs, note the production verification steps, and file follow-up actions (e.g., add regression test).</li> </ol>"},{"location":"Process%20Documentation/examples/issues-after-deployment/#viewing-docker-logs","title":"Viewing Docker Logs","text":"<p>In some cases, you may need to access Docker container logs directly to troubleshoot issues. The backend, frontend, and other services run inside Docker containers, and all use the default Docker logging driver, which stores logs in JSON format on the host machine.</p> <ol> <li>Logs are stored at:</li> </ol> <pre><code>/var/snap/docker/common/var-lib-docker/containers/`&lt;container-id&gt;`/`&lt;container-id&gt;`-json.log\n</code></pre> <p>This path should be the same across most Linux distributions using Docker installed via Snap, as should be the same for all ARG servers. Replace <code>&lt;container-id&gt;</code> with the actual ID of your container.</p> <ol> <li>How to find your container's log file</li> </ol> <p>Get container ID:  </p> <pre><code>sudo docker ps\n</code></pre> <p>Navigate to its folder:  </p> <pre><code>cd /var/snap/docker/common/var-lib-docker/containers/`&lt;container-id&gt;`\n</code></pre> <p>View the log file:  </p> <pre><code>sudo tail -f *-json.log\n</code></pre>"},{"location":"Process%20Documentation/examples/issues-after-deployment/#evidence-package","title":"Evidence Package","text":"<ul> <li>Incident log with timestamps/actions.</li> <li>Before/after API responses plus correlation IDs.</li> <li>Hotfix PR link and deployment confirmation screenshot.</li> </ul>"},{"location":"Process%20Documentation/examples/legacy-table-to-api-resource-mapping/","title":"Example \u2013 Legacy Table to API Resource Mapping","text":""},{"location":"Process%20Documentation/examples/legacy-table-to-api-resource-mapping/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: expose legacy <code>APVEND</code> vendor data through the <code>/vendors</code> NestJS endpoint.</li> <li>Inputs: DB2 catalog entry from <code>src/shared/config/constants/table-registry.ts</code>, Sequelize model in <code>src/main/account-payable/data/models/vendor.model.ts</code>, Swagger contract in <code>src/api-schema/vendor.yml</code>.</li> <li>Output: documented mapping table and PR evidence showing DTO, repository, and mapper alignment.</li> </ul>"},{"location":"Process%20Documentation/examples/legacy-table-to-api-resource-mapping/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Confirm table metadata. Open <code>table-registry.ts</code> and copy the schema/name pair for <code>Vendor</code> so the mapping doc references the exact IBM i object.</li> <li>Trace the NestJS flow. In VS Code, follow <code>vendor.controller.ts \u2192 vendor.service.ts \u2192 vendor.repository.ts</code> to list every column touched by the API.</li> <li>Compare ORM \u2192 DTO. Use split view on <code>vendor.model.ts</code> and <code>voucher.dto.ts</code> (if the endpoint returns vouchers) or <code>vendor.dto.ts</code> to make sure each Sequelize attribute is represented in the DTO.</li> <li>Document the mapping. Update your mapping spreadsheet/Confluence with three columns: legacy column, Sequelize attribute, DTO field. Paste file links from GitLens for evidence.</li> <li>Validate against IBM i. Run a <code>SELECT</code> in IBM Data Studio against <code>dataLib.APVEND</code> to prove each column exists and note any packed/decimal formats for the consumer team.</li> <li>Attach evidence to the story. Drop the mapping artifact plus VS Code screenshots into the Azure Boards work item and note the commit hash containing any DTO/model changes.</li> </ol>"},{"location":"Process%20Documentation/examples/legacy-table-to-api-resource-mapping/#evidence-package","title":"Evidence Package","text":"<ul> <li>Screenshot of the VS Code diff covering DTO/model updates.</li> <li>Data Studio query output filtered to a single vendor row.</li> <li>Link to the Azure Boards attachment containing the finalized mapping table.</li> </ul>"},{"location":"Process%20Documentation/examples/own-go-live-tasks/","title":"Example \u2013 Own Go-Live Tasks","text":""},{"location":"Process%20Documentation/examples/own-go-live-tasks/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: execute all production cutover actions for release 2024.11 of the ARG Web Backend + IBM i batch changes.</li> <li>Inputs: Cutover checklist, deployment pipeline, IBM CL scripts, monitoring runbook.</li> <li>Output: Go-live report showing each step, timestamp, and owner.</li> </ul>"},{"location":"Process%20Documentation/examples/own-go-live-tasks/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Freeze scope. Confirm no new PRs are merged after the go/no-go meeting and note the final commit SHA.</li> <li>Prep environments. Scale staging to production-like size, warm caches via <code>yarn seed:prod</code>, and notify IBM ops about the RPG deployment window.</li> <li>Deploy Node backend. Trigger the Azure DevOps release pipeline; monitor container logs until <code>/health</code> is green in production.</li> <li>Promote RPG objects. Run the approved <code>SAVOBJ/RSTOBJ</code> or change management scripts to move RPG programs into PRODLIB and capture job logs.</li> <li>Execute smoke tests. Call <code>/vouchers</code>, <code>/vendors</code>, and run an IBM Navigator query to confirm data flows end-to-end. Record correlation IDs.</li> <li>Communicate status. Update the go-live Teams channel with start/end times, note any incidents, and attach evidence before closing the change record.</li> </ol>"},{"location":"Process%20Documentation/examples/own-go-live-tasks/#evidence-package","title":"Evidence Package","text":"<ul> <li>Completed cutover checklist with timestamps.</li> <li>Azure pipeline logs plus IBM job log PDFs.</li> <li>Smoke test summary including API IDs and DB2 query outputs.</li> </ul>"},{"location":"Process%20Documentation/examples/rpg-program-signoff-syntax-compile/","title":"Example \u2013 RPG Program Signoff (Syntax/Compile)","text":""},{"location":"Process%20Documentation/examples/rpg-program-signoff-syntax-compile/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: certify that RPGLE member <code>APINVR</code> compiles cleanly before promoting to TEST.</li> <li>Inputs: Source member, compile command <code>CRTBNDRPG</code>, joblog, comparison with previous module level.</li> <li>Output: signoff note referencing compile logs and object versions.</li> </ul>"},{"location":"Process%20Documentation/examples/rpg-program-signoff-syntax-compile/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Load source. Use PDM or Code for IBM i to edit <code>APINVR</code> in <code>DEVLIB/QRPGLESRC</code> and ensure comments note the change ticket.</li> <li>Run compile. Submit <code>SBMJOB CMD(CRTBNDRPG PGM(TESTLIB/APINVR) SRCFILE(DEVLIB/QRPGLESRC)) LOG(*JOBLOGSVR)</code>.</li> <li>Review joblog. In IBM Navigator, check the submitted job for any RNF/RPG messages; copy them into OneNote even if informational.</li> <li>Capture listing. Save the spool file <code>QSYSPRT</code> from the compile job, export as PDF, and attach to the story.</li> <li>Verify object level. Use <code>DSPOBJD OBJ(TESTLIB/APINVR) DETAIL(*SERVICE)</code> to confirm the creation timestamp and compiler level.</li> <li>Post signoff. Update the release checklist with compile date/time, job number, and spool file path.</li> </ol>"},{"location":"Process%20Documentation/examples/rpg-program-signoff-syntax-compile/#evidence-package","title":"Evidence Package","text":"<ul> <li>PDF of the compile listing with zero severity &gt; 20 errors.</li> <li>Screenshot of IBM Navigator joblog filtered to the compile job.</li> <li>Release checklist entry referencing object level/time stamp.</li> </ul>"},{"location":"Process%20Documentation/examples/rpg-testing-e2e/","title":"Example \u2013 RPG Testing End-to-End","text":""},{"location":"Process%20Documentation/examples/rpg-testing-e2e/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: validate that voucher postings triggered by <code>/vouchers</code> API create balanced journal entries through RPG batch job <code>APJRN</code>.</li> <li>Inputs: Node API, RPG program, DB2 tables (<code>APTRANH</code>, <code>APTRAND</code>), spool files.</li> <li>Output: signed test log showing API \u2192 RPG \u2192 DB2 consistency.</li> </ul>"},{"location":"Process%20Documentation/examples/rpg-testing-e2e/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Seed data. Use Data Studio to insert a test voucher header/detail row or run the NestJS endpoint to create it via <code>AddVoucherEntryUseCase</code>.</li> <li>Kick off RPG batch. Submit <code>SBMJOB CMD(CALL PGM(TESTLIB/APJRN) PARM('TESTVCHR'))</code> so the IBM job processes the voucher.</li> <li>Monitor job. Watch <code>WRKACTJOB</code> or IBM Navigator to ensure the job completes without MSGW status; capture the job log if warnings occur.</li> <li>Verify DB2 impact. Query <code>APTRANH</code> and <code>APTRAND</code> to confirm the voucher status changed and balancing amounts match the API payload.</li> <li>Check spooled output. Download the balancing report spool file (e.g., <code>APJRNLOG</code>) as PDF and compare totals to the API response.</li> <li>Record results. Update the E2E test case in Azure Test Plans with pass/fail, attach the spool PDF, and mention the API request ID.</li> </ol>"},{"location":"Process%20Documentation/examples/rpg-testing-e2e/#evidence-package","title":"Evidence Package","text":"<ul> <li>API request/response log from NestJS (include correlation ID).</li> <li>DB2 query screenshot proving status transitions.</li> <li>Spool file PDF showing balanced totals.</li> </ul>"},{"location":"Process%20Documentation/examples/sign-off-on-uat-stage/","title":"Example \u2013 Sign Off on UAT (Stage)","text":""},{"location":"Process%20Documentation/examples/sign-off-on-uat-stage/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: certify release 2024.11 passed Stage after business testers ran regression suite.</li> <li>Inputs: Stage deployment logs, Azure Test Plans UAT suite, IBM Navigator job traces, monitoring dashboards.</li> <li>Output: sign-off email/post summarizing scope, blockers, and readiness.</li> </ul>"},{"location":"Process%20Documentation/examples/sign-off-on-uat-stage/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Verify deployment. Confirm the Stage slot shows the new container tag via <code>kubectl get deploy</code> or Azure App Service logs and that NestJS health endpoint <code>/health</code> returns 200.</li> <li>Review UAT cases. In Test Plans, check that every test tied to the release is executed with Pass/Fail recorded; chase any \"Blocked\" entries.</li> <li>Spot-check data. Run a few <code>/vouchers</code> and <code>/vendors</code> calls against Stage plus IBM Navigator queries to ensure the Stage DB2 library contains the expected records.</li> <li>Monitor jobs. Ensure scheduled RPG jobs (e.g., nightly AP batch) run in Stage by reviewing job logs/spooled files after testers complete flows.</li> <li>Consolidate evidence. Compile API logs, job numbers, and QA metrics into a short summary deck or wiki entry.</li> <li>Announce sign-off. Email stakeholders or update Teams/Confluence with the summary, list of known issues, and explicit \"Stage Approved\" statement.</li> </ol>"},{"location":"Process%20Documentation/examples/sign-off-on-uat-stage/#evidence-package","title":"Evidence Package","text":"<ul> <li>Screenshot of Stage health endpoint returning OK.</li> <li>Azure Test Plans export showing completion percentages.</li> <li>IBM Navigator job log snippet proving RPG batch success.</li> </ul>"},{"location":"Process%20Documentation/examples/sign-off-user-stories-and-brd/","title":"Example \u2013 Sign Off User Stories and BRD","text":""},{"location":"Process%20Documentation/examples/sign-off-user-stories-and-brd/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: approve BRD-142 for \"Vendor Remittance Search\" after verifying requirements meet the delivered NestJS endpoints and RPG jobs.</li> <li>Inputs: Azure Boards story, BRD PDF, swagger doc for <code>/remittance</code>, RPG spec for <code>APCHKR</code> program, updated docs in <code>docs/delivery</code>.</li> <li>Output: sign-off note that references code, documentation, and business validation artifacts.</li> </ul>"},{"location":"Process%20Documentation/examples/sign-off-user-stories-and-brd/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Read the BRD deltas. Highlight acceptance criteria tied to API responses, UI filters, and IBM i programs.</li> <li>Trace implementation. Confirm the <code>/remittance</code> controller and DTO exist under <code>src/main/account-payable/application</code> and the corresponding repository queries <code>APCHKR</code>/<code>APHSTHB</code> tables listed in <code>table-registry.ts</code>.</li> <li>Review documentation. Ensure <code>docs/DELIVERY_TASKS.md</code> (or the per-task guides) mention the new behavior so downstream teams have guidance.</li> <li>Validate demo evidence. Watch the latest sprint review or run <code>yarn start:dev</code> and use Swagger (<code>/api-docs</code>) to reproduce the story's happy path.</li> <li>Check RPG alignment. Using IBM Navigator, review the last compile listing for the RPG program or service program powering the remittance query.</li> <li>Record sign-off. Post a comment on the Azure Boards story citing the commit hash, Swagger screenshot, and RPG compile date before moving the story to \"Accepted\".</li> </ol>"},{"location":"Process%20Documentation/examples/sign-off-user-stories-and-brd/#evidence-package","title":"Evidence Package","text":"<ul> <li>Screenshot of Swagger showing the <code>/remittance</code> call returning the expected fields.</li> <li>IBM Navigator compile report snippet referencing the updated RPG source member.</li> <li>Azure Boards comment ID/time stamp documenting the approval.</li> </ul>"},{"location":"Process%20Documentation/examples/transfer-rpg-programs-to-power-10/","title":"Example \u2013 Transfer RPG Programs to Power 10","text":""},{"location":"Process%20Documentation/examples/transfer-rpg-programs-to-power-10/#scenario-snapshot","title":"Scenario Snapshot","text":"<ul> <li>Goal: migrate <code>AP200R</code> and <code>APJRN</code> objects from the Power 8 DEV partition to the new Power 10 TEST partition.</li> <li>Inputs: SAVOBJ library save, FTP/SFTP transfer, RSTOBJ on target, validation queries.</li> <li>Output: confirmation that binaries exist on Power 10 with matching levels.</li> </ul>"},{"location":"Process%20Documentation/examples/transfer-rpg-programs-to-power-10/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Prep save files. On Power 8, run <code>SAVOBJ OBJ(AP200R APJRN) LIB(DEVLIB) DEV(*SAVF) SAVF(DEVLIB/APMIG)</code>.</li> <li>Transfer save files. Use FTP or Navigator \"Download Save File\" to move <code>APMIG</code> to your workstation, then upload to Power 10 <code>QGPL/APMIG</code>.</li> <li>Restore objects. On Power 10, execute <code>RSTOBJ OBJ(AP200R APJRN) SAVLIB(DEVLIB) DEV(*SAVF) SAVF(QGPL/APMIG) RSTLIB(TESTLIB)</code>.</li> <li>Verify authorities. Run <code>DSPOBJAUT TESTLIB/AP200R</code> to confirm profiles match the original security matrix.</li> <li>Smoke test. Call each program with a harmless parameter set (<code>CALL TESTLIB/AP200R PARM('CHECKONLY')</code>) and ensure job logs show success.</li> <li>Document transfer. Update the migration tracker with save/restore job numbers, timestamps, and checksum values if used.</li> </ol>"},{"location":"Process%20Documentation/examples/transfer-rpg-programs-to-power-10/#evidence-package","title":"Evidence Package","text":"<ul> <li>SAVOBJ/RSTOBJ job logs exported as PDF.</li> <li>Screenshot of <code>WRKOBJ</code> on Power 10 proving objects exist in TESTLIB.</li> <li>Migration tracker entry referencing checksum or object level numbers.</li> </ul>"},{"location":"Process%20Documentation/tools/","title":"Tool-Specific Delivery Playbooks","text":"<p>Use these guides when you need step-by-step instructions tied to VS Code, IBM Navigator, 5250 sessions, IBM Data Studio, and other platform tools.</p>"},{"location":"Process%20Documentation/tools/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Legacy Table to API Resource Mapping</li> <li>Integrations with External Apps</li> <li>Sign Off User Stories and BRD</li> <li>Approve QA Test Cases from Acceptance Criteria</li> <li>Code Review \u2013 Node.js</li> <li>Code Review \u2013 RPG</li> <li>RPG Program Signoff (Syntax/Compile)</li> <li>RPG Testing End-to-End</li> <li>Sign Off on UAT (Stage)</li> <li>Transfer RPG Programs to Power 10</li> <li>Own Go-Live Tasks</li> <li>Issues After Deployment</li> <li>Feature Change Requests</li> </ol>"},{"location":"Process%20Documentation/tools/approve-qa-test-cases-from-acceptance-criteria/","title":"Approve QA Test Cases \u2013 Tool Walkthrough","text":"<p>Tie each acceptance criterion to an executable artifact using the tooling below.</p>"},{"location":"Process%20Documentation/tools/approve-qa-test-cases-from-acceptance-criteria/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 open the acceptance-criteria Markdown and compare to automated test specs in <code>src/**/**/*.spec.ts</code>.</li> <li>VS Code Test Explorer \u2013 run or debug Jest specs tied to the story.</li> <li>IBM Data Studio \u2013 craft SQL test data scripts referenced by QA.</li> <li>5250 emulator \u2013 preload IBM i files with <code>RUNSQLSTM</code> or <code>CPYF</code> if QA scenarios need seeded data.</li> </ul>"},{"location":"Process%20Documentation/tools/approve-qa-test-cases-from-acceptance-criteria/#guided-steps","title":"Guided Steps","text":"<ol> <li>Load the acceptance criteria in VS Code. Use a split view with the matching Jest spec so each bullet is visible while you verify coverage.</li> <li>Run the automated tests through Test Explorer. Capture pass/fail output and note any missing criteria.</li> <li>Prepare DB2 datasets in Data Studio. Save parameterized SQL scripts QA can run before each execution to reset state.</li> <li>Seed IBM i records if needed. Use a 5250 session to run <code>RUNSQLSTM</code> or <code>CPYF</code> commands that mirror the Data Studio script on the host.</li> <li>Log approval. In VS Code, append a checklist to the acceptance criteria doc referencing the exact test files (<code>*.spec.ts</code>) and SQL scripts used.</li> </ol>"},{"location":"Process%20Documentation/tools/approve-qa-test-cases-from-acceptance-criteria/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Test Explorer screenshot or CLI output showing Jest suites tied to the story.</li> <li>SQL script filenames or Git hashes for seeded data.</li> <li>5250 command history proving host data preparation occurred.</li> </ul>"},{"location":"Process%20Documentation/tools/code-review-nodejs/","title":"Code Review \u2013 Node.js (Tool Playbook)","text":"<p>Mirror the standard checklist while leaning on VS Code tooling.</p>"},{"location":"Process%20Documentation/tools/code-review-nodejs/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 use split editors for controller/service/repository diffs.</li> <li>VS Code GitLens &amp; SCM panel \u2013 inspect commit history and staged changes.</li> <li>VS Code ESLint + Jest extensions \u2013 run lint/tests inline.</li> <li>Docker/Terminal inside VS Code \u2013 reproduce the service locally.</li> <li>IBM Data Studio \u2013 validate SQL changes reflected in Sequelize models.</li> </ul>"},{"location":"Process%20Documentation/tools/code-review-nodejs/#guided-steps","title":"Guided Steps","text":"<ol> <li>Fetch the branch and open in VS Code. Use the Source Control pane to review file-by-file diffs, focusing on DTOs, modules, and queue handlers.</li> <li>Run lint/tests from VS Code. Trigger <code>yarn lint</code>, <code>yarn test</code>, and <code>yarn build</code> via the integrated terminal or Tasks, saving the output in the PR conversation.</li> <li>Inspect database mappings. When models change, open Data Studio to verify the DB2 table/column definitions line up with the new TypeScript attributes.</li> <li>Verify configuration links. Use search in VS Code to confirm any new environment variables are documented in <code>.env.example</code> and referenced by <code>config/</code> providers.</li> <li>Review logs/debug hooks. Set VS Code breakpoints or run the NestJS app via the debugger to ensure new handlers emit the logs described in <code>docs/DEBUGGING.md</code>.</li> <li>Summarize findings. Use GitLens to copy permalinks to any lines needing follow-up and paste them into the PR review.</li> </ol>"},{"location":"Process%20Documentation/tools/code-review-nodejs/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>VS Code terminal snippets for lint/test/build.</li> <li>Data Studio screenshot or query result verifying schema alignment.</li> <li>Debug console output proving log statements fire as expected.</li> </ul>"},{"location":"Process%20Documentation/tools/code-review-rpg/","title":"Code Review \u2013 RPG (Tool Playbook)","text":"<p>Blend SEU/PDM, VS Code, and Navigator views to review RPG updates thoroughly.</p>"},{"location":"Process%20Documentation/tools/code-review-rpg/#core-tools","title":"Core Tools","text":"<ul> <li>5250 emulator with PDM/SEU \u2013 open members, compare versions, and run syntax checks (<code>STRSEU</code> Option 14).</li> <li>RDi or VS Code with Code for IBM i \u2013 optional modern editor for inline diffs and linting.</li> <li>IBM Navigator for i \u2013 inspect job logs or spool files after test compiles.</li> <li>VS Code \u2013 read companion TypeScript changes if the RPG program feeds the API.</li> </ul>"},{"location":"Process%20Documentation/tools/code-review-rpg/#guided-steps","title":"Guided Steps","text":"<ol> <li>Fetch the source member. In 5250, run <code>STRPDM</code> \u2192 Option 2 (Work with members) to open the library/file the change targets. Use Option 5 to display and Option 14 to syntax-check.</li> <li>Compare revisions. Use Option 3 (Copy) with <code>F15=Browse</code> or RDi/VS Code diff tools to compare the updated member against the previous production version stored in source control.</li> <li>Validate interfaces. Cross-check field layouts with VS Code DTOs if the program writes to API-facing files or data queues.</li> <li>Compile and inspect logs. Issue a test compile (<code>CRTRPGMOD</code>/<code>CRTBNDRPG</code>) and view the joblog via Navigator or <code>WRKJOB OPTION(10)</code> to ensure no severity &gt; 0 messages persist.</li> <li>Sign off. Record the compile listing path and any changed service programs in the PR or task comments.</li> </ol>"},{"location":"Process%20Documentation/tools/code-review-rpg/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Screenshot of the SEU syntax check summary or RDi Problems view.</li> <li>Navigator joblog snippet with <code>Compilation successful</code> message.</li> <li>Notes showing DTO alignment if data is consumed by Node.js code.</li> </ul>"},{"location":"Process%20Documentation/tools/feature-change-requests/","title":"Feature Change Requests \u2013 Tool Playbook","text":"<p>Capture the technical impact of requested changes using the standard toolset.</p>"},{"location":"Process%20Documentation/tools/feature-change-requests/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 edit the feature request template (<code>docs/delivery/feature-change-requests.md</code>) and prototype code spikes.</li> <li>VS Code extensions (REST Client, Thunder Client) \u2013 mock new API calls or payloads.</li> <li>IBM Navigator for i \u2013 list existing objects that might need changes (files, data queues, service programs).</li> <li>IBM Data Studio \u2013 inspect table growth, column availability, and query plans affected by the request.</li> <li>5250/PDM \u2013 estimate impact to RPG members via <code>WRKOBJ</code> and <code>DSPFD</code>.</li> </ul>"},{"location":"Process%20Documentation/tools/feature-change-requests/#guided-steps","title":"Guided Steps","text":"<ol> <li>Document the request in VS Code. Open the feature template, fill in motivation, and link to impacted modules/files.</li> <li>Prototype payloads. Use VS Code REST tools to draft sample requests/responses that illustrate the change.</li> <li>Assess database impact. In Data Studio, review schema details and determine whether new columns/indexes are required.</li> <li>Inventory IBM i dependencies. Run <code>WRKOBJ</code> or Navigator object searches to find RPG programs, service programs, or files touched by the change.</li> <li>Summarize scope. Update the request doc with complexity notes (Node modules + IBM assets) and attach Navigator/Data Studio exports.</li> </ol>"},{"location":"Process%20Documentation/tools/feature-change-requests/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>VS Code diff or snippet showing the documented request.</li> <li>Data Studio schema screenshots.</li> <li>Navigator object listings or 5250 command output referencing impacted IBM assets.</li> </ul>"},{"location":"Process%20Documentation/tools/integrations-with-external-apps/","title":"Integrations with External Apps \u2013 Tool Walkthrough","text":"<p>Follow these VS Code, IBM i, and Navigator actions to validate each integration point end-to-end.</p>"},{"location":"Process%20Documentation/tools/integrations-with-external-apps/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 inspect NestJS integration modules, <code>.env</code> examples, and queue processors.</li> <li>Docker Compose terminal / VS Code tasks \u2013 spin up stubs or mock servers locally.</li> <li>IBM Navigator for i \u2013 confirm remote database or MQ credentials stored in system values or service entries.</li> <li>IBM Data Studio \u2013 verify DB2 views or staging tables that feed external systems.</li> <li>5250 emulator \u2013 run CL commands (<code>WRKSRVTBLE</code>, <code>WRKENVVAR</code>) to confirm IBM i-side configuration.</li> </ul>"},{"location":"Process%20Documentation/tools/integrations-with-external-apps/#guided-steps","title":"Guided Steps","text":"<ol> <li>Open the integration module in VS Code. Use the file Explorer to review the NestJS provider, HTTP client, and DTO definitions. Note required secrets.</li> <li>Start local dependencies. Launch VS Code\u2019s integrated terminal to run <code>docker-compose up integration-mocks</code> or the service-specific mock script so you can replay callbacks.</li> <li>Validate IBM i credentials. In Navigator, open Configuration and Service \u2192 Service Entries to verify the user profile, endpoint, and certificate assigned to the integration. Export the entry details if updates are needed.</li> <li>Check DB2 staging artifacts with Data Studio. Run diagnostics queries to ensure data is landing in the expected staging tables or views that the integration consumes.</li> <li>Confirm IBM i environment variables via 5250. Execute <code>WRKENVVAR LEVEL(*SYS)</code> or <code>WRKSRVTBLE</code> to verify keys that should align with the <code>.env</code> entries used in VS Code.</li> <li>Exercise the flow. From VS Code, issue the relevant API call (via Thunder Client/REST Client) and monitor both the NestJS logs and Navigator job logs for errors.</li> </ol>"},{"location":"Process%20Documentation/tools/integrations-with-external-apps/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Screenshot of Navigator service entry parameters.</li> <li>CLI output from <code>WRKENVVAR</code> showing matching keys.</li> <li>Local log excerpt proving the mock/external call succeeded.</li> </ul>"},{"location":"Process%20Documentation/tools/issues-after-deployment/","title":"Issues After Deployment \u2013 Tool Playbook","text":"<p>Use these diagnostics to triage regressions quickly.</p>"},{"location":"Process%20Documentation/tools/issues-after-deployment/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 tail application logs via SSH, run <code>kubectl logs</code> (if applicable), and inspect recent commits.</li> <li>VS Code Debug Console \u2013 attach to local reproductions or run targeted scripts.</li> <li>IBM Navigator for i \u2013 review QSYSOPR, job logs, and message queues tied to the failing process.</li> <li>5250 emulator \u2013 query message queues (<code>WRKMSGF</code>, <code>DSPMSG</code>), cancel jobs, or rerun CL utilities.</li> <li>IBM Data Studio \u2013 run point-in-time queries to compare pre/post data states.</li> </ul>"},{"location":"Process%20Documentation/tools/issues-after-deployment/#guided-steps","title":"Guided Steps","text":"<ol> <li>Capture context in VS Code. Pull the latest tag, review the diff for the component at fault, and open log streaming tasks (<code>tail -f logs/app.log</code>).</li> <li>Inspect IBM i alerts. In Navigator, open Basic Operations \u2192 Messages and download relevant QSYSOPR/QSYSMSG entries; cross-check from a 5250 session if Navigator is lagging.</li> <li>Check jobs/subsystems. Use Navigator or <code>WRKACTJOB</code> to identify looping/failed jobs. Drill into job logs (<code>Option 10</code>) and export them.</li> <li>Validate data. From Data Studio, run targeted queries to confirm whether tables were partially updated or locked.</li> <li>Reproduce locally. Back in VS Code, run the failing API or script with prod-like <code>.env</code> overrides to replicate the issue.</li> <li>Document RCA inputs. Store links to Navigator logs, SQL results, and VS Code repro steps inside the <code>docs/delivery/issues-after-deployment.md</code> checklist.</li> </ol>"},{"location":"Process%20Documentation/tools/issues-after-deployment/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Log excerpts showing timestamps of failures.</li> <li>Joblog PDFs from Navigator/5250.</li> <li>SQL query outputs demonstrating data corruption or confirming integrity.</li> </ul>"},{"location":"Process%20Documentation/tools/legacy-table-to-api-resource-mapping/","title":"Legacy Table to API Resource Mapping \u2013 Tool Walkthrough","text":"<p>Turn the conceptual mapping checklist into concrete IDE and IBM i actions.</p>"},{"location":"Process%20Documentation/tools/legacy-table-to-api-resource-mapping/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 open <code>src/main/**</code> modules and DTOs, and keep <code>docs/DEBUGGING.md</code> handy for data-contract nuances.</li> <li>IBM Navigator for i \u2013 browse DB2 schemas, table descriptions, and indexes through the web console.</li> <li>IBM Data Studio \u2013 run exploratory SQL (column discovery, sample payloads, joins) against the same DB2 catalog.</li> <li>5250 emulator + PDM/SEU \u2013 inspect RPG copybooks or physical files that still drive the table layout.</li> </ul>"},{"location":"Process%20Documentation/tools/legacy-table-to-api-resource-mapping/#guided-steps","title":"Guided Steps","text":"<ol> <li>Review the API resource shape in VS Code. Use the Explorer search (<code>Ctrl+Shift+F</code>) to locate the DTO and Sequelize model that serve the endpoint. Note field names, types, and enumerations.</li> <li>Confirm table metadata in IBM Navigator. Launch Navigator \u2192 Schemas \u2192 locate the legacy library \u2192 open the table \u2192 capture column names, CCSID, and constraints. Export the column list for reference.</li> <li>Pull live samples with IBM Data Studio. Run <code>SELECT</code> statements using the Navigator metadata to verify data density, nullability, and lookup tables. Save the result set as CSV for the API team.</li> <li>Check RPG copybooks in 5250/PDM. Use <code>STRPDM</code> \u2192 Option 3 (Work with members) to open copybooks referenced by RPG programs that populate the table. Ensure packed/decimal definitions align with DTO fields.</li> <li>Document the mapping. Back in VS Code, create or update the mapping note under <code>docs/delivery/legacy-table-to-api-resource-mapping.md</code>, embedding the column screenshots or SQL output you gathered.</li> </ol>"},{"location":"Process%20Documentation/tools/legacy-table-to-api-resource-mapping/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Column export from IBM Navigator.</li> <li>SQL result samples from Data Studio showing edge cases.</li> <li>Screenshots or notes from PDM showing packed/decimal definitions when they differ from API expectations.</li> </ul>"},{"location":"Process%20Documentation/tools/own-go-live-tasks/","title":"Own Go-Live Tasks \u2013 Tool Playbook","text":"<p>Coordinate release weekend activities across IDEs and IBM i consoles.</p>"},{"location":"Process%20Documentation/tools/own-go-live-tasks/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 run deployment scripts, edit <code>.env</code> overrides, and capture release notes.</li> <li>Terminal/Docker in VS Code \u2013 execute <code>deploy.sh</code> or Azure DevOps pipeline steps manually.</li> <li>IBM Navigator for i \u2013 stop/start subsystems, view QSYSOPR messages, and monitor job queues.</li> <li>5250 emulator \u2013 run CL commands (<code>ENDSBS</code>, <code>STRSBS</code>, <code>SBMJOB</code>) when Navigator access is constrained.</li> <li>IBM Data Studio \u2013 run migration SQL and verify post-go-live data sanity checks.</li> </ul>"},{"location":"Process%20Documentation/tools/own-go-live-tasks/#guided-steps","title":"Guided Steps","text":"<ol> <li>Review the runbook in VS Code. Open <code>docs/delivery/own-go-live-tasks.md</code> and annotate the timeline with assignees.</li> <li>Prepare environment switches. Use VS Code to edit <code>.env.production</code> or pipeline variables, committing changes if required.</li> <li>Execute application deployment. Run <code>deploy.sh</code> or the relevant CI task inside VS Code\u2019s terminal; watch for failures and rerun if necessary.</li> <li>Flip IBM i jobs. Via Navigator or 5250, stop the legacy subsystem, deploy new programs, then <code>STRSBS</code> or <code>SBMJOB</code> to resume operations.</li> <li>Run validation SQL. In Data Studio, execute the go-live verification queries (row counts, default flags) and store outputs.</li> <li>Log status. Update the go-live tracker in VS Code/DevOps with timestamps, Navigator screenshots, and SQL results.</li> </ol>"},{"location":"Process%20Documentation/tools/own-go-live-tasks/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Terminal output from deployment scripts.</li> <li>Navigator or 5250 logs showing subsystem restarts.</li> <li>Data Studio query exports for post-deploy validation.</li> </ul>"},{"location":"Process%20Documentation/tools/rpg-program-signoff-syntax-compile/","title":"RPG Program Signoff (Syntax/Compile) \u2013 Tool Playbook","text":"<p>Ensure every compile uses the correct IBM i tooling and artifacts.</p>"},{"location":"Process%20Documentation/tools/rpg-program-signoff-syntax-compile/#core-tools","title":"Core Tools","text":"<ul> <li>5250 emulator \u2013 run <code>STRSEU</code> Option 14 or <code>CRTBNDRPG</code> commands.</li> <li>PDM \u2013 track members and capture compile history via Option 8 (Attributes) or 14 (Compile).</li> <li>IBM Navigator for i \u2013 review job logs/spooled compile listings.</li> <li>VS Code / RDi \u2013 store notes about compile options (<code>DFTACTGRP</code>, binding directories, etc.).</li> </ul>"},{"location":"Process%20Documentation/tools/rpg-program-signoff-syntax-compile/#guided-steps","title":"Guided Steps","text":"<ol> <li>Open the source in SEU. <code>STRPDM</code> \u2192 library/file \u2192 Option 2 \u2192 member \u2192 Option 14 for syntax. Resolve any flagged lines before continuing.</li> <li>Kick off the compile from PDM. Use Option 14 and pick the appropriate command (<code>CRTBNDRPG</code>, <code>CRTRPGMOD</code>, or <code>CRTSQLRPGI</code>). Document the options in a VS Code scratch file so you can repeat the build.</li> <li>Monitor job status. Press <code>F1</code> on any compile message, then review the joblog via Navigator (Jobs \u2192 Active \u2192  \u2192 Messages). <li>Capture spool output. In Navigator, download the <code>QSYSPRT</code> or <code>QRPGLESRC</code> listing for attachment to the ticket.</li> <li>Sign off. Update the tool-specific checklist in <code>docs/delivery/rpg-program-signoff-syntax-compile.md</code> noting the library, object, and compile command.</li>"},{"location":"Process%20Documentation/tools/rpg-program-signoff-syntax-compile/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Screenshot of SEU syntax check summary.</li> <li>Joblog excerpt with <code>Errors = 0</code> and <code>Severity &lt; 30</code>.</li> <li>Spool file (PDF/TXT) attached to the work item.</li> </ul>"},{"location":"Process%20Documentation/tools/rpg-testing-e2e/","title":"RPG Testing End-to-End \u2013 Tool Playbook","text":"<p>Use 5250 tooling plus VS Code and Navigator observers to execute a full workflow.</p>"},{"location":"Process%20Documentation/tools/rpg-testing-e2e/#core-tools","title":"Core Tools","text":"<ul> <li>5250 emulator \u2013 run CL commands, submit batch jobs, and view interactive screens.</li> <li>PDM/SEU \u2013 tweak test data copybooks or driver programs.</li> <li>IBM Navigator for i \u2013 monitor submitted jobs (<code>WRKACTJOB</code> equivalent) and capture job logs/spool files.</li> <li>VS Code / REST client \u2013 trigger API calls if Node.js endpoints are part of the flow.</li> <li>IBM Data Studio \u2013 verify DB2 tables before/after the RPG process runs.</li> </ul>"},{"location":"Process%20Documentation/tools/rpg-testing-e2e/#guided-steps","title":"Guided Steps","text":"<ol> <li>Reset data state. Run Data Studio scripts or <code>RUNSQLSTM</code> (via 5250) to seed prerequisite records.</li> <li>Launch the driver program. In 5250, start the transaction (e.g., <code>CALL PGM(LIB/PGM)</code> or submit the batch job) and record the job name.</li> <li>Monitor execution in Navigator. Watch the job under Active Jobs; if it spawns child jobs, open each joblog and export messages.</li> <li>Verify downstream API effects. If the RPG routine feeds the Node API, issue the relevant request from VS Code\u2019s REST client or Thunder Client and compare payloads against expected values.</li> <li>Validate tables. Use Data Studio to run <code>SELECT</code> queries before and after execution, ensuring delta counts match requirements.</li> <li>Document pass/fail. Update the <code>docs/delivery/rpg-testing-e2e.md</code> checklist with job names, run timestamps, and SQL result summaries.</li> </ol>"},{"location":"Process%20Documentation/tools/rpg-testing-e2e/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Data Studio screenshots showing before/after row counts.</li> <li>Navigator joblog or spool download for the RPG run.</li> <li>REST client output verifying API parity when applicable.</li> </ul>"},{"location":"Process%20Documentation/tools/sign-off-on-uat-stage/","title":"Sign Off on UAT (Stage) \u2013 Tool Playbook","text":"<p>Use the same interface stack testers rely on so approvals mirror reality.</p>"},{"location":"Process%20Documentation/tools/sign-off-on-uat-stage/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code / REST client \u2013 fire API smoke tests against the Stage environment endpoints listed in <code>.env.stage</code>.</li> <li>IBM Navigator for i \u2013 monitor Stage job queues, subsystem status, and logs while UAT runs.</li> <li>IBM Data Studio \u2013 query the Stage DB2 schema to verify data migrations.</li> <li>5250 emulator \u2013 check Stage library lists, data queues, or CL monitors.</li> </ul>"},{"location":"Process%20Documentation/tools/sign-off-on-uat-stage/#guided-steps","title":"Guided Steps","text":"<ol> <li>Load Stage configuration in VS Code. Review <code>.env.stage</code> (or Azure DevOps variable group) to confirm URLs, ports, and IBM i hosts.</li> <li>Trigger smoke tests. Use Thunder Client/REST Client collections stored in the repo to hit the Stage API, capturing responses and latency metrics.</li> <li>Monitor IBM i activity. In Navigator, filter Active Jobs by the Stage subsystem (e.g., <code>QHTTPSVR</code>) and ensure no messages accumulate in the queue.</li> <li>Validate data persistence. Run Data Studio queries against Stage libraries/tables affected by the release. Compare counts to pre-release baselines.</li> <li>Check on-host indicators. From a 5250 session, verify Stage data queues, message queues, or batch jobs (e.g., <code>WRKACTJOB SBS(STAGESBS)</code>) look healthy.</li> <li>Log approval. Update the UAT checklist with API response IDs, Navigator screenshots, and SQL outputs.</li> </ol>"},{"location":"Process%20Documentation/tools/sign-off-on-uat-stage/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>REST client export showing Stage responses.</li> <li>Navigator job or subsystem snapshots.</li> <li>Data Studio query results referencing Stage libraries.</li> </ul>"},{"location":"Process%20Documentation/tools/sign-off-user-stories-and-brd/","title":"Sign Off User Stories and BRD \u2013 Tool Walkthrough","text":"<p>Use these tool-specific instructions to validate documentation before green-lighting development.</p>"},{"location":"Process%20Documentation/tools/sign-off-user-stories-and-brd/#core-tools","title":"Core Tools","text":"<ul> <li>VS Code \u2013 preview Markdown BRDs stored under <code>docs/</code> and compare against implementation notes in <code>src/</code>.</li> <li>VS Code extensions (Markdown Preview, GitLens) \u2013 render acceptance criteria and trace commits that reference each story.</li> <li>IBM Data Studio \u2013 confirm that referenced DB2 objects exist or are planned.</li> <li>5250 emulator / Navigator \u2013 look up existing IBM i programs or queues cited in the BRD.</li> </ul>"},{"location":"Process%20Documentation/tools/sign-off-user-stories-and-brd/#guided-steps","title":"Guided Steps","text":"<ol> <li>Open the BRD files in VS Code. Use the Markdown preview split view to ensure tables, flow diagrams, and requirements render correctly.</li> <li>Cross-reference user stories with code owners. Use GitLens blame on the relevant modules to verify that impacted teams have been consulted.</li> <li>Validate data sources. In Data Studio, run <code>DESCRIBE</code> or <code>SELECT 1 FROM &lt;TABLE&gt;</code> statements for every DB object the BRD promises to reuse.</li> <li>Check IBM i dependencies. Use Navigator or <code>WRKOBJ</code> in 5250 to confirm that referenced RPG programs, queues, or libraries exist and are versioned.</li> <li>Record decisions. Update the story/BRD status line in VS Code and push a commit or comment in Azure DevOps linking to the validated evidence.</li> </ol>"},{"location":"Process%20Documentation/tools/sign-off-user-stories-and-brd/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>VS Code screenshot of the rendered BRD with revision tag.</li> <li>SQL snippets proving each DB object exists.</li> <li><code>WRKOBJ</code> or Navigator object details for every IBM i dependency.</li> </ul>"},{"location":"Process%20Documentation/tools/transfer-rpg-programs-to-power-10/","title":"Transfer RPG Programs to Power 10 \u2013 Tool Playbook","text":"<p>Coordinate tooling between source and target IBM i partitions.</p>"},{"location":"Process%20Documentation/tools/transfer-rpg-programs-to-power-10/#core-tools","title":"Core Tools","text":"<ul> <li>5250 emulator \u2013 package objects (<code>SAVOBJ</code>, <code>SAVLIB</code>) and restore them on the Power 10 LPAR.</li> <li>IBM Navigator for i \u2013 manage network file shares (IFS) and monitor save/restore jobs.</li> <li>VS Code / Code for IBM i \u2013 push source members via Git to the Power 10 repo workspace.</li> <li>IBM Data Studio \u2013 confirm Power 10 DB2 catalogs contain required tables before moving programs.</li> </ul>"},{"location":"Process%20Documentation/tools/transfer-rpg-programs-to-power-10/#guided-steps","title":"Guided Steps","text":"<ol> <li>Verify prerequisites. In Data Studio, connect to the Power 10 DB2 catalog and run sanity queries (<code>SELECT COUNT(*) FROM &lt;table&gt;</code>) to ensure dependencies already exist.</li> <li>Package the objects on the source LPAR. Using 5250, run <code>SAVOBJ</code> or <code>SAVLIB</code> to a save file in QGPL (or a transport library). Note the save file name.</li> <li>Move the save file. From Navigator, download the save file or copy it through an IFS share/SFTP to the Power 10 system.</li> <li>Restore on Power 10. In a Power 10 5250 session, run <code>RSTOBJ</code>/<code>RSTLIB</code>, pointing to the transferred save file. Monitor job progress in Navigator.</li> <li>Sync source members. Use VS Code\u2019s Code for IBM i extension (or Git) to push updated source into the Power 10 source library for long-term maintenance.</li> <li>Validate objects. Run <code>DSPPGM</code> or <code>WRKOBJ</code> on Power 10 to confirm attributes (owner, activation group) and document the results in the transfer checklist.</li> </ol>"},{"location":"Process%20Documentation/tools/transfer-rpg-programs-to-power-10/#evidence-to-capture","title":"Evidence to Capture","text":"<ul> <li>Data Studio output verifying target tables.</li> <li><code>SAVOBJ</code>/<code>RSTOBJ</code> command snippets and joblog exports.</li> <li>VS Code screenshot of the Code for IBM i deploy log (if used).</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/","title":"App Reports Structure","text":"<p>Title: Legacy AS/400 Reporting System Modernization Date: 2025-06-27 Status: In Progress  </p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#context","title":"Context","text":"<p>This decision is about how we implement global reporting structure in the new application while integrating with legacy system. We are modernizing a legacy AS/400 (IBM i) system that currently uses SpoolFlex, an add-on application providing a 5250 terminal interface for spool file management. The existing system allows users to view/work with AS/400 spool files, convert to PDF/Excel formats, email reports, save to shared drives, and automate report generation through pre-defined jobs. </p> <p>The modernization effort aims to replace the outdated 5250 terminal interface completely with a modern web-based user interface while maintaining equivalent functionality.</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#decision","title":"Decision","text":"<p>We will implement a hybrid reporting architecture that bridges the legacy AS/400 system with modern web technologies. We will keep all existing functionality in-place and simply wrap the current system. This structure will also support our second phase of re-writing the existing RPG reports in a modern framework within our Node.js REST API backend.</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#core-components","title":"Core Components:","text":"<ol> <li>Tabular Report Log SQL Table - Central metadata repository:</li> <li> <p>Fields: ReportName, CreatedBy, Timestamp,FileName, LocationPath.</p> </li> <li> <p>Shared Drive Storage - File system location for converted report outputs</p> </li> <li> <p>*Note: the client and the applications moving reports to the share will need the appropriate security access.</p> </li> <li> <p>Legacy System Wrapper Program - Bridge component that accepts parameters (SpoolFileLocation, User, OutputPath, ConversionType), retrieves spool files, converts to desired format, generates timestamped filenames, saves to output path, and records metadata in SQL table</p> </li> <li> <p>Modern Backend API - RESTful service with GET endpoints for report log retrieval, filtering by report type/user permissions, and shared/private access control</p> </li> <li> <p>Web Client Interface - Browser-based UI component that displays available reports and opens them directly from shared drive locations. This should allow user to open the file folder location, open the report in a separate window, or popup download the report from the browser.</p> </li> </ol>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#reasoning","title":"Reasoning","text":""},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#report-log-benefits","title":"Report Log Benefits","text":"<p>The centralized report log provides a scalable foundation for future modernization. As we gradually migrate reports from the legacy system to native backend implementations, we can maintain the same process: write to the report log and save files to shared drive locations. This consistent approach ensures:</p> <ul> <li>Unified reporting interface regardless of report source (legacy or modern)</li> <li>Error tracking capabilities - failed report generations can be logged with error details</li> <li>Audit trail for all report activities across the entire system</li> <li>Seamless migration path without disrupting user workflows</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#alternatives-considered","title":"Alternatives Considered","text":"<p>Alternative 1: Direct File System Access - Pros: Users work directly with output files in storage locations - Cons: Requires page-specific mappings to share drive locations (not scalable), vulnerable to storage location changes, no global repository of report activities - Rejected: Duplicates Windows File Explorer functionality without added value</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#consequences","title":"Consequences","text":""},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#positive","title":"Positive","text":"<ul> <li>Gradual migration path without operational disruption</li> <li>Preserved functionality with modern user experience</li> <li>Scalable architecture supporting future report implementations</li> <li>Centralized error handling and audit capabilities</li> <li>Flexible output formats (PDF, HTML, CSV)</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#negative","title":"Negative","text":"<ul> <li>Maintains dependency on legacy AS/400 system</li> <li>Additional system complexity and potential failure points</li> <li>Ongoing shared drive storage management requirements</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#implementation-notes","title":"Implementation Notes","text":"<ol> <li>Implement Report Log SQL table and wrapper program</li> <li>Build RESTful backend service with filtering capabilities</li> <li>Develop web client interface</li> <li>Migrate automated jobs and optimize performance</li> </ol> <p>Review Date: Now</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/","title":"RPG Program and Database File Development Standards","text":"<p>Title: RPG Program and Database File Development Standards Date: 2025-07-14 Status: In Progress  </p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#context","title":"Context","text":"<p>Architectural Problem: Our current RPG application system relies on temporary flat files that need to be converted to externally defined permanent tables as part of our migration to a Power 10 server architecture. The existing system uses two distinct types of temporary files that require different handling approaches.</p> <p>Key Factors and Constraints: - Migration from temporary flat files to permanent database tables - Two types of temporary files requiring different conversion strategies:   1. Job-scoped tables (implementation approach TBD)   2. User-scoped temporary files (specific conversion requirements defined) - Multi-environment deployment across Power 8 (dev/test/UAT) and Power 10 (production) servers - Need for proper library management and naming conventions - Requirement to maintain data isolation and security</p> <p>Options Considered: - Continue with temporary files (rejected due to data persistence needs) - Convert all temporary files to permanent tables with same structure (rejected due to security concerns) - Implement differentiated approach based on file scope (selected)</p> <p>System Context: IBM i (AS/400) environment with RPG programs accessing database files across multiple environments, with production running on Power 10 and lower environments on Power 8.</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#decision","title":"Decision","text":"<p>Architectural Decision: Implement a tiered database architecture with externally defined permanent tables, environment-specific library management, and differentiated handling of temporary file types.</p> <p>Implementation Approach:</p> <p>User-Scoped Temporary Files Conversion: - Convert to externally defined permanent tables - Add USER_NAME field to enable user-based data isolation - Store physical files in Working Data Library with \"G\" prefix - Create logical files/views in Data Library without prefix for RPG program access - Require user filtering in all INSERT/DELETE operations</p> <p>Job-Scoped Tables: - Status: To Be Determined - Permanent data sorts will be replaced with logical files (SQL views)</p> <p>Library Structure by Environment:</p> Environment Data Library Working Data Library Stored Procedure Library Server Development DATADEV QS36FDEV GSSLIBDEV Power 8 Testing DATATEST QS36FTEST GSSLIBTEST Power 8 UAT DATAUAT QS36FUAT GSSLIBUAT Power 8 Production DATA QS36F GSSLIB Power 10 <p>Technical Requirements: - All database files must be externally defined - RPG programs reference logical files in Data Library only - Physical files stored in Working Data Library with \"G\" prefix - User filtering mandatory for user-scoped table access</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#reasoning","title":"Reasoning","text":"<p>Data Persistence and Integrity - Permanent tables provide data persistence beyond job/session scope - Externally defined tables ensure better data integrity and documentation - Structured approach to user data isolation improves security</p> <p>Maintainability and Scalability - Logical files provide abstraction layer for easier future modifications - Environment-specific libraries enable proper deployment control - Standardized naming conventions improve system maintainability - Power 10 architecture provides improved performance and capacity</p> <p>Security and Data Isolation - User-based filtering prevents cross-user data access - Separate library structure ensures environment isolation - Controlled access patterns through logical file abstraction</p> <p>Trade-offs Considered: - Increased storage requirements vs. improved data persistence - Additional development effort vs. long-term maintainability benefits - User filtering complexity vs. enhanced security</p> <p>Risk Mitigation: - Phased approach allows for testing and refinement - Deferred decision on job-scoped tables reduces immediate complexity - Environment separation minimizes deployment risks</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#consequences","title":"Consequences","text":""},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#positive","title":"Positive","text":"<ul> <li>Data Persistence: User-scoped data persists beyond individual sessions</li> <li>Improved Data Management: Externally defined tables provide better integrity and documentation</li> <li>Environment Isolation: Separate libraries ensure clean environment management and deployment control</li> <li>Enhanced Performance: Power 10 server provides improved performance and capacity for production</li> <li>Better Maintainability: Logical files provide abstraction layer for easier future modifications</li> <li>Enhanced Security: User-based filtering prevents unauthorized data access</li> </ul>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#negative","title":"Negative","text":"<ul> <li>Development Effort: All programs referencing temporary files require modification for user filtering</li> <li>Storage Requirements: Permanent tables consume more storage than temporary files</li> <li>Increased Complexity: Additional user filtering logic required in all affected programs</li> <li>Migration Risk: Potential for data inconsistency during transition period</li> <li>Cross-Server Complexity: Different server architectures for development vs. production environments</li> </ul>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#implementation-notes","title":"Implementation Notes","text":"<p>User-Scoped Table Conversion Process: 1. Create physical table in Working Data Library with \"G\" prefix (QS36FDEV/QS36FTEST/QS36FUAT/QS36F) 2. Add USER_NAME field to table structure 3. Create corresponding logical file/view in Data Library (DATADEV/DATATEST/DATAUAT/DATA) 4. Identify all programs referencing the temporary file 5. Modify programs to include user filtering logic 6. Test data access and filtering functionality 7. Deploy to appropriate environment libraries following promotion process</p> <p>RPG Program Modification Pattern: <pre><code>// Before (temporary file access)\nREAD TEMPFILE;\n\n// After (permanent table with user filtering)\nCLEAR CUSTFILE;\nUSER_NAME = %USERID();\nSETLL (USER_NAME) CUSTFILE;\nREAD CUSTFILE;\n</code></pre></p> <p>Library Usage Guidelines: - Data Library: Contains logical files/views (non-prefixed names) that RPG programs reference - Working Data Library: Contains physical tables with \"G\" prefix for actual data storage - Stored Procedure Library: Contains database stored procedures and functions</p> <p>Server Infrastructure: - Power 8 Server: Hosts Development, Testing, and UAT environments - Power 10 Server: Hosts Production environment only - Cross-Server Considerations: Development and testing occur on Power 8, with final deployment to Power 10 for production</p> <p>Deployment Process: - New programs must be developed with the new library structure - Existing programs migrated to appropriate environment libraries during deployment - Cross-environment consistency maintained through standardized naming conventions</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#related-decisions","title":"Related Decisions","text":"<p>[To be added as additional ADRs are created]</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#references","title":"References","text":"<p>[To be added as external documentation and standards are referenced]</p> <p>Future Considerations: - Job-scoped table implementation strategy needs definition - Performance monitoring requirements for user-filtered queries - Indexing strategy for USER_NAME fields - Data archiving approach for user-scoped permanent tables - Backup and recovery procedure updates for new table structure</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/","title":"Authorization Workflow","text":"<p>Title: Role-Based Access Control (RBAC) Authorization Workflow</p> <p>Date: 2025-10-26</p> <p>Status: Accepted</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#context","title":"Context","text":"<p>The ARG Web system requires a secure and flexible authorization mechanism to control user access to various routes and endpoints. The system needed to address several key requirements:</p> <ul> <li>Validate user requests before they reach backend services</li> <li>Support both group-based and individual user-based permissions</li> <li>Allow fine-grained control where specific rights can be disabled for individual users even if their groups have those rights</li> <li>Enable custom rights assignment directly to users outside of group memberships</li> <li>Maintain separation of concerns between authentication and authorization</li> <li>Support many-to-many relationships between routes and rights</li> </ul> <p>The authorization layer needed to work with JWT tokens and enforce access control in a consistent, auditable manner across all API endpoints. Additionally, the system required flexibility to handle complex scenarios where users belong to multiple groups but may need certain permissions blocked or custom permissions added.</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#decision","title":"Decision","text":"<p>Implement a middleware-based authorization workflow that validates user access through a dedicated ARG Web Auth service using a multi-table RBAC model.</p> <p>Key Components:</p> <ol> <li>Middleware Layer: Intercepts all requests in the ARG Web Backend and forwards authorization checks to the Auth service</li> <li>JWT Token Processing: Decodes access tokens to extract UserId for permission lookups</li> <li> <p>Database Schema: <code>Group</code> - Stores all groups      <code>Rights</code> - Stores all rights/permissions      <code>GroupRights</code> - Maps rights to groups      <code>UserRights</code> - Stores user-specific right overrides (enable/disable)      <code>UserGroupMapping</code> - Associates users with groups (many-to-many)      <code>RouterMaster</code> - Stores all available routes/endpoints      <code>RouteRightsMapping</code> - Maps routes to required rights (many-to-many)</p> </li> <li> <p>Authorization Flow: Middleware sends: Access Token, MetaData, Path URL to Auth service     Auth service decodes token to extract UserId     System queries UserGroupMapping to get all user's groups     Derives rights from GroupRights for all user's groups     Applies UserRights overrides (disabled rights are blocked, custom rights are added)     Checks RouteRightsMapping to verify if user has required rights for the route     Returns authorized/unauthorized response</p> </li> </ol>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#reasoning","title":"Reasoning","text":"<p>Security &amp; Separation of Concerns - Centralized authorization logic in a dedicated Auth service prevents inconsistent security enforcement across the application - Middleware pattern ensures no endpoint can bypass authorization checks - JWT token validation provides stateless authentication that scales well</p> <p>Flexibility &amp; Granularity - Group-based permissions provide efficient management for common access patterns - UserRights table enables precise control by allowing administrators to disable specific group rights or add custom rights for individual users - This hybrid approach handles both standard and exceptional access requirements without creating numerous single-purpose groups</p> <p>Scalability &amp; Maintainability - Many-to-many relationships (UserGroupMapping, RouteRightsMapping) allow flexible assignment without data duplication - Users can belong to multiple groups, inheriting rights from all memberships - Routes can require multiple rights, enabling complex authorization rules - Normalized database structure makes it easy to audit permissions and add new routes or rights</p> <p>Performance Considerations - Single authorization check per request minimizes database queries - UserId-based lookup is efficient and can be cached - Trade-off: Additional network call to Auth service adds latency, but gains security and maintainability</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#consequences","title":"Consequences","text":""},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#positive","title":"Positive","text":"<ul> <li>Fine-grained access control: System can handle complex permission scenarios including group inheritance with individual overrides</li> <li>Audit trail: Clear permission structure makes it easy to determine why a user has or doesn't have access</li> <li>Centralized security: Single point of authorization enforcement reduces security vulnerabilities</li> <li>Flexibility: New routes, rights, and groups can be added without code changes</li> <li>Scalability: Stateless JWT validation allows horizontal scaling of Auth service</li> </ul>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#negative","title":"Negative","text":"<ul> <li>Additional latency: Every request requires a call to the Auth service, adding network overhead</li> <li>Complexity: Multi-table design requires understanding of the entire permission model for troubleshooting</li> <li>Potential inconsistency: UserRights overrides can create confusion if not well-documented (users blocked from group rights they should have)</li> <li>Database load: Authorization checks may create significant database load under high traffic</li> <li>Single point of failure: Auth service unavailability blocks all requests (requires high availability setup)</li> </ul>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#implementation-notes","title":"Implementation Notes","text":"<p>Authorization Request Format: <pre><code>{\n  \"accessToken\": \"JWT_TOKEN\",\n  \"metaData\": [\"key1\", \"key2\"],\n  \"pathUrl\": \"/api/resource/endpoint\"\n}\n</code></pre></p> <p>Authorization Response: - Success: Request forwarded to ARG Web Backend - Failure: Return \"Not Authorized\" error response</p> <p>Permission Resolution Algorithm: 1. Extract UserId from JWT 2. Query UserGroupMapping for all groups where UserId exists 3. Query GroupRights for all rights associated with those groups 4. Query UserRights for user-specific overrides 5. Apply logic:    - Start with all group rights    - Remove any rights marked as disabled in UserRights (IsActive = false)    - Add any custom rights marked as enabled in UserRights (IsActive = true) 6. Query RouteRightsMapping to get required rights for requested route 7. Check if user's final rights set includes all required rights</p> <p>Database Relationships: - User \u2192 UserGroupMapping (1:N) \u2192 Group - Group \u2192 GroupRights (1:N) \u2192 Rights - User \u2192 UserRights (1:N) \u2192 Rights (with IsActive flag) - Route \u2192 RouteRightsMapping (1:N) \u2192 Rights</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#related-decisions","title":"Related Decisions","text":"<p>[No related ADRs referenced in the source document]</p>"},{"location":"Security%20ADR%27s/Authorization%20Workflow%20/#references","title":"References","text":"<ul> <li>Internal document: \"Authorization Workflow in ARG Web System\"</li> <li>JWT (JSON Web Tokens) standard for authentication</li> <li>RBAC (Role-Based Access Control) design patterns</li> </ul>"},{"location":"Testing%20ARD%27s/UAT%20Process/","title":"User Acceptance Testing (UAT) Process","text":""},{"location":"Testing%20ARD%27s/UAT%20Process/#objective","title":"Objective","text":"<p>Ensure delivered functionality meets the Business Requirements Document (BRD) and is free from critical defects before go-live.</p>"},{"location":"Testing%20ARD%27s/UAT%20Process/#roles-responsibilities","title":"Roles &amp; Responsibilities","text":"<ul> <li>Business Users \u2013 Execute test cases, report issues, provide feedback.</li> <li>Internal ARG-QA \u2013 Bridge between Users and Consultant; verifies features before UAT, filters bugs, manages communication, and performs triage classification.</li> <li>Consultant QA (DAMCO) \u2013 Logs validated bugs into JIRA, investigates, resolves, and updates status.</li> </ul>"},{"location":"Testing%20ARD%27s/UAT%20Process/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Smoke Test (DAMCO QA)    DAMCO QA verifies the environment is stable and major functionality works. Critical blockers are fixed before proceeding.</p> </li> <li> <p>Pre-UAT Verification (ARG-QA)    ARG-QA performs a quick functional check to confirm:  </p> </li> <li>Feature is accessible in UAT environment  </li> <li>Main buttons/links work  </li> <li>No blocking errors occur  </li> <li> <p>Core workflows are usable for testing  </p> </li> <li> <p>User Testing &amp; Logging Issues    Users execute test scenarios and log all issues (bugs, usability concerns, questions) into a shared OneDrive spreadsheet with:  </p> </li> <li>Steps taken  </li> <li>Expected vs. actual results  </li> <li> <p>Screenshots/data references</p> </li> <li> <p>ARG-QA Review &amp; Triage    ARG-QA reviews entries to:  </p> </li> <li>Remove duplicates  </li> <li>Identify misunderstandings  </li> <li> <p>Classify using the Triage Classification System (see below)</p> </li> <li> <p>UAT Review Meeting </p> </li> <li> <p>ARG-QA and Users meet to confirm classifications and finalize the issue list based on triage categories.</p> </li> <li> <p>Issue Resolution Based on Triage    Issues are handled according to their triage classification:</p> </li> <li>Clear and Communicate Immediately: <ul> <li>Category 1 (User Mis-Interpretation)</li> </ul> </li> <li>Send Directly to DAMCO: <ul> <li>Category 2 (Design Adherence)</li> <li>Category 3 (Clear Defect)</li> </ul> </li> <li> <p>Requires ARG Internal Review: </p> <ul> <li>Category 4 (Business Rule Implementation Error)</li> <li>Category 5 (Enhancement Request)</li> </ul> </li> <li> <p>Bug Resolution &amp; Tracking    DAMCO QA fixes issues, updates JIRA status, and coordinates with ARG-QA for validation.</p> </li> <li> <p>Bug Validation &amp; UAT Sign-Off    ARG-QA and Users re-test resolved items. UAT completes when:  </p> </li> <li>All critical/high defects are fixed and verified  </li> <li>No open medium/low defects block go-live  </li> <li>User sign-off is received</li> </ol>"},{"location":"Testing%20ARD%27s/UAT%20Process/#triage-classification-system","title":"Triage Classification System","text":"Classification Description Action Required 1 - User Mis-Interpretation User misunderstood functionality or process Clear Immediately with user education 2 - Design Adherence System not following approved design specifications Send Directly to DAMCO 3 - Clear Defect Obvious system malfunction or error Send Directly to DAMCO 4 - Business Rule Implementation Error System not implementing business rules correctly Requires ARG Internal Review 5 - Enhancement Request New functionality or improvement request Requires ARG Internal Review"},{"location":"Testing%20ARD%27s/UAT%20Process/#jira-status-tracking","title":"JIRA Status Tracking","text":"JIRA Status Description Required Action In-Progress DAMCO actively working on the issue Monitor progress ARG-QA Input Required DAMCO requires clarification from ARG ARG provides requested information Implemented Fix deployed to UAT environment REQUIRES ACTION: - ARG-QA Re-test and validate Resolved No Implementation Issue resolved without code changes ARG-QA Needs to confirm and mark done Done ARG has validated and closed the issue Update tracking spreadsheet"},{"location":"Testing%20ARD%27s/UAT%20Process/#tools-communication","title":"Tools &amp; Communication","text":"<ul> <li>OneDrive Spreadsheet \u2013 Centralized logging of all user-reported issues with triage classifications.</li> <li>JIRA \u2013 DAMCO QA's bug tracking system with standardized status workflow.</li> <li>QA Meetings / Teams \u2013 Communication between ARG-QA, Users, and DAMCO QA.</li> </ul>"},{"location":"Testing%20ARD%27s/UAT%20Process/#workflow-summary-by-triage-category","title":"Workflow Summary by Triage Category","text":"<p>Category 1 (User Mis-Interpretation): - OneDrive Entry \u2192 ARG-QA Review \u2192 Clear Immediately \u2192 User Communication</p> <p>Categories 2 &amp; 3 (Design Adherence / Clear Defect): - OneDrive Entry \u2192 ARG-QA Review \u2192 Direct DAMCO Submission \u2192 JIRA Tracking \u2192 Resolution \u2192 User Validation</p> <p>Categories 4 &amp; 5 (Business Rule Error / Enhancement):  - OneDrive Entry \u2192 ARG-QA Review \u2192 ARG Internal Review \u2192 Decision \u2192 Potential DAMCO Submission \u2192 JIRA Tracking (if applicable)</p>"}]}