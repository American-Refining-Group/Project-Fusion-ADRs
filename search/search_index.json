{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs","text":""},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/","title":"Client-Side Grid","text":"<p>Title: Client-Side Grid Filtering, Sorting, and Pagination with Backend Offset-Based Pagination Date: 2025-06-16 Status: Accepted  </p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#context","title":"Context","text":"<p>This decision addresses the approach to managing grid-level data interactions \u2014 specifically filtering, sorting, and pagination \u2014 for data-driven UI components.</p> <p>Two architectural options were considered for handling these features:</p> <ol> <li>Client-Side Handling \u2013 Load the dataset once from the API and allow all interactions (filtering, sorting, pagination) to occur entirely in the browser.  </li> <li>Server-Side Handling \u2013 Rely on backend API endpoints to execute filtering, sorting, and pagination for every user interaction.</li> </ol> <p>The system in question needs to handle datasets that may be large in some cases, but are typically manageable in size after user-driven filtering.</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#decision","title":"Decision","text":"<p>The architecture will implement client-side filtering, sorting, and pagination for grid components.</p> <p>To support this:</p> <ul> <li>The UI will include filter inputs and a \"Search\" button that users must use to initiate data retrieval.</li> <li>Upon clicking \"Search\", the frontend will send a request to the API including any provided filter criteria.</li> <li>The API will return a page of data (default size of 500 records or fewer) along with offset-based pagination metadata.</li> <li>All grid operations (filtering, sorting, pagination) will then be handled entirely client-side using the retrieved data.</li> </ul> <p>In addition:</p> <ul> <li>The API will implement standardized filtering and sorting behavior for all relevant endpoints. This ensures consistency and reusability of query logic across the application.</li> <li>Every API endpoint will support:</li> <li>Sorting via query parameters in ascending/descending order.</li> <li>Filtering on text fields (equality) and numeric fields (equality, greater than, less than, and range).</li> </ul> <p>If the total number of matching records exceeds the initial page size (default limit = 500), the API will not return an error. Instead, it will return a <code>nextPageUrl</code> that can be used to fetch additional pages of data.</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#performance-and-user-experience","title":"Performance and User Experience","text":"<ul> <li>Interactions like filtering and sorting are executed instantly in the browser, eliminating round trips to the server and improving responsiveness.</li> <li>Users control when and how data is retrieved, reducing backend load and improving perceived speed.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#simplicity","title":"Simplicity","text":"<ul> <li>Reduces complexity in the frontend-to-backend interaction model.</li> <li>Decreases backend logic for grid state handling.</li> <li>Encourages standardization of query patterns across endpoints.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#scalable-pagination","title":"Scalable Pagination","text":"<ul> <li>Using <code>offset</code> and <code>limit</code> enables scalable data retrieval while avoiding hard failures due to record caps.</li> <li>Providing metadata like <code>nextPageUrl</code> and <code>totalRecords</code> gives clients full control if they need to fetch more data explicitly.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#rest-api-capabilities","title":"REST API Capabilities","text":"<p>Even though grid operations are handled client-side after data retrieval, the REST API will support consistent and standardized pagination, filtering, and sorting behavior:</p>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#filtering","title":"Filtering","text":"<ul> <li>Text fields: Supports equality match (<code>eq</code>)</li> <li>Numeric fields: Supports:</li> <li>Equality (<code>eq</code>)</li> <li>Greater than (<code>gt</code>)</li> <li>Less than (<code>lt</code>)</li> <li>Range filtering (e.g., <code>min</code>/<code>max</code> or <code>between</code> operators)</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#sorting","title":"Sorting","text":"<ul> <li>Query parameter format: <code>?sort=field.asc</code> or <code>?sort=field.desc</code></li> <li>Multiple sort keys may be supported where applicable.</li> </ul>"},{"location":"Front%20End%20ADR%27s/Client%20Side%20Grid%20Handling/#pagination-offset-based","title":"Pagination (Offset-Based)","text":"<ul> <li>Default <code>limit</code>: 500</li> <li>Supports <code>limit</code> and <code>offset</code> parameters</li> <li>The API response includes:</li> </ul> <p>```json {   \"data\": [ / array of records / ],   \"offset\": 0,   \"limit\": 500,   \"totalRecords\": 1342,   \"hasMore\": true,   \"nextPageUrl\": \"/api/items?offset=500&amp;limit=500\",   \"previousPageUrl\": null,   \"filtersApplied\": {     \"status\": \"active\"   },   \"sort\": \"createdAt.desc\" }</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/","title":"File Upload Workflow","text":"<p>Title: File Upload Workflow with Asynchronous Batch Processing Date: 2025-10-26 Status: Accepted</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#context","title":"Context","text":"<p>The application requires a robust file upload system that can handle large Excel/CSV files containing business data. The system must address several key challenges:</p> <p>Key Requirements: - Process large files (up to 10MB) containing structured business data - Validate file structure and data integrity before processing - Provide real-time feedback to users during long-running operations - Handle thousands of records efficiently without HTTP timeouts - Ensure data accuracy with proper Excel date and accounting number parsing - Maintain security through proper authentication and user isolation</p> <p>Constraints: - HTTP request timeout limitations for synchronous processing - Need for immediate user feedback while processing continues - File format complexity (Excel serial dates, accounting number formats) - Resource cleanup requirements for temporary file storage</p> <p>Options Considered: 1. Synchronous HTTP-only processing - Simple but causes timeouts on large files 2. Pure asynchronous processing with polling - Requires constant client polling, inefficient 3. Hybrid HTTP/WebSocket with queue-based processing - Balances immediate feedback with async processing</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#decision","title":"Decision","text":"<p>We will implement a hybrid HTTP/WebSocket architecture with asynchronous batch processing using BullMQ for the file upload workflow.</p> <p>Key Components:</p> <ol> <li>HTTP Layer (Synchronous)</li> <li>Endpoint: <code>POST /{module}/upload</code></li> <li>Protocol: HTTP with multipart/form-data</li> <li> <p>Handles: File upload, validation, and initial processing setup</p> </li> <li> <p>WebSocket Layer (Asynchronous)</p> </li> <li>Protocol: WebSocket with Socket.IO</li> <li>Authentication: JWT token-based</li> <li> <p>Handles: Real-time progress updates and final results</p> </li> <li> <p>Queue Layer (Background Processing)</p> </li> <li>Technology: BullMQ with Redis</li> <li>Handles: Asynchronous batch processing of uploaded data</li> </ol> <p>Processing Workflow:</p> <pre><code>1. File Upload &amp; Validation (HTTP)\n   \u2193\n2. Data Parsing &amp; Batch Creation\n   \u2193\n3. Queue Job Creation (Parent + Child Jobs)\n   \u2193\n4. Asynchronous Batch Processing (BullMQ)\n   \u2193\n5. Real-time Updates (WebSocket)\n   \u2193\n6. Final Summary &amp; Cleanup\n</code></pre> <p>Technical Specifications: - File types supported: .xlsx, .xls, .csv - Maximum file size: 10MB (configurable) - Storage location: <code>uploads/csv/{MODULE_NAME}/</code> - Upload ID format: <code>{PREFIX}-{timestamp}-{uuid}</code> - User-specific WebSocket rooms: <code>user-{userIdentifier}</code> - Job structure: Parent job orchestrating multiple child batch jobs</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#scalability","title":"Scalability","text":"<ul> <li>Asynchronous processing prevents HTTP timeouts: By decoupling file upload from data processing, large files can be processed without hitting typical 30-60 second HTTP timeout limits</li> <li>Batch processing handles large datasets efficiently: Configurable batch sizes allow tuning for optimal memory usage and processing speed</li> <li>Redis-backed queue supports horizontal scaling: Multiple worker processes can consume jobs from the same queue, enabling easy scaling as load increases</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#user-experience","title":"User Experience","text":"<ul> <li>Immediate upload confirmation: Users receive instant HTTP response confirming file acceptance, preventing perceived delays</li> <li>Real-time progress updates: WebSocket events provide continuous feedback on processing status, reducing user anxiety during long operations</li> <li>Comprehensive error reporting: Specific, actionable error messages at multiple validation levels help users correct issues quickly</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#reliability","title":"Reliability","text":"<ul> <li>Automatic resource cleanup: Temporary files are removed after processing (success or failure), preventing disk space exhaustion</li> <li>Robust error handling: Multi-level validation catches issues early; processing errors don't crash the system</li> <li>Job tracking and monitoring: Parent/child job structure enables detailed observability and debugging</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#security","title":"Security","text":"<ul> <li>JWT-based authentication for both protocols: Consistent authentication across HTTP and WebSocket prevents unauthorized access</li> <li>File type and size validation: Prevents malicious file uploads and resource exhaustion attacks</li> <li>User-specific data isolation: WebSocket rooms ensure users only receive updates for their own uploads</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#trade-offs-accepted","title":"Trade-offs Accepted","text":"<ul> <li>Increased system complexity: Three-layer architecture (HTTP/WebSocket/Queue) requires more infrastructure and monitoring compared to simple synchronous approach</li> <li>Redis dependency: Adds operational overhead of maintaining Redis instance</li> <li>WebSocket connection management: Requires handling connection lifecycle, reconnection logic, and authentication</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#consequences","title":"Consequences","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#positive","title":"Positive","text":"<ul> <li>Handles files of any size within limits: Large files with thousands of records process smoothly without timeouts</li> <li>Excellent user experience: Users can navigate away and return; progress is preserved and resumable</li> <li>Production-ready scalability: Can handle increased load by adding queue workers horizontally</li> <li>Detailed observability: Upload IDs correlate actions across all system layers for effective debugging</li> <li>Clean separation of concerns: HTTP handles upload, WebSocket handles communication, Queue handles processing</li> <li>Reusable pattern: Architecture can be applied to multiple modules requiring file upload functionality</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#negative","title":"Negative","text":"<ul> <li>Increased infrastructure complexity: Requires Redis server, WebSocket server, and queue workers in addition to standard HTTP API</li> <li>More difficult local development: Developers need Redis and multiple processes running to test the full workflow</li> <li>WebSocket connection management: Must handle disconnections, reconnections, and stale connections</li> <li>Eventual consistency model: Brief delay between upload confirmation and processing completion may confuse some users</li> <li>Higher operational overhead: More services to monitor, log aggregation across multiple components required</li> </ul>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#implementation-notes","title":"Implementation Notes","text":""},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#file-validation-pipeline","title":"File Validation Pipeline","text":"<pre><code>1. File type validation: .xlsx, .xls, .csv only\n2. File size validation: Configurable maximum (default 10MB)\n3. Header validation: Exact column order and count matching\n4. Data validation: At least 1 non-empty row after header\n5. Format validation: Excel date conversion, accounting number parsing\n</code></pre>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#response-schemas","title":"Response Schemas","text":"<p>HTTP Success Response: <pre><code>interface FileUploadResponseDto {\n  message: string;           // \"File accepted, split into batches\"\n  uploadId: string;         // \"{PREFIX}-{timestamp}-{uuid}\"\n  totalRecords: number;     // Total records processed\n  totalBatches: number;     // Number of batches created\n  parentJobId: string;      // Parent job ID for tracking\n  childJobIds: string[];    // Array of child job IDs\n}\n</code></pre></p> <p>WebSocket Batch Progress: <pre><code>{\n  \"event\": \"batch-progress\",\n  \"data\": {\n    \"uploadId\": \"UPLOAD-1234567890-uuid\",\n    \"batchId\": \"batch-1\",\n    \"status\": \"completed\",\n    \"processed\": 5,\n    \"errors\": 0,\n    \"warnings\": 0\n  }\n}\n</code></pre></p> <p>WebSocket Final Summary: <pre><code>{\n  \"event\": \"upload-complete\",\n  \"data\": {\n    \"uploadId\": \"UPLOAD-1234567890-uuid\",\n    \"summary\": {\n      \"totalProcessed\": 10,\n      \"successCount\": 7,\n      \"errorCount\": 2,\n      \"warningCount\": 1,\n      \"additionalMetrics\": {}  // Module-specific metrics\n    }\n  }\n}\n</code></pre></p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#error-response-types","title":"Error Response Types","text":"<p>File Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"Only Excel (.xlsx, .xls) and CSV files are allowed\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Header Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"Header mismatch at column 3. Expected 'COLUMN_NAME' but found 'ColumnName'\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Data Validation Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"File must contain at least 1 non-empty data row(s) after the header row. Found only 0 non-empty data row(s).\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p> <p>Authentication Error (400): <pre><code>{\n  \"statusCode\": 400,\n  \"message\": \"User authentication required\",\n  \"error\": \"Bad Request\"\n}\n</code></pre></p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>1. parseFile() - Extract data with proper formatting\n2. filterAndNormalizeRows() - Clean and filter empty rows\n3. chunkArray() - Split into configurable batches\n4. createQueueJobs() - Create parent/child job structure\n5. processInBackground() - Execute batch processing\n</code></pre>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<p>Logging Strategy: - Structured logging with upload IDs for correlation - Process duration tracking at each stage - Error logging with full stack traces</p> <p>Key Metrics: - Upload success/failure rates by module - Processing time per batch - Queue depth and processing rates - WebSocket connection count and stability</p> <p>Tracing: - Upload ID correlation across HTTP/WebSocket/Queue layers - Job ID tracking for debugging - User context preservation throughout workflow</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#module-specific-configuration","title":"Module-Specific Configuration","text":"<p>Each module can customize: - Upload ID prefix (e.g., \"CC\" for Clear Checks, \"INV\" for Invoices) - Expected file headers and column order - Batch size for processing - Module-specific validation rules - Custom business logic for data processing - Summary metrics format</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#related-decisions","title":"Related Decisions","text":"<p>[To be added as related ADRs are created for specific module implementations]</p>"},{"location":"Front%20End%20ADR%27s/File%20Upload%20Workflow/#references","title":"References","text":"<ul> <li>BullMQ Documentation: https://docs.bullmq.io/</li> <li>Socket.IO Documentation: https://socket.io/docs/</li> <li>Excel Serial Date Format: Microsoft Excel date system documentation</li> <li>Multipart Form Data Specification: RFC 7578</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/","title":"Architectural Design Record (ADR)","text":"<p>Title: Long-Running Operations (LRO) Pattern with HTTP/WebSocket/Queue Architecture Date: 2025-10-26 Status: Accepted</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#context","title":"Context","text":"<p>The application requires a pattern for handling long-running operations (LROs) that exceed typical HTTP timeout limits. These operations include file uploads, batch validations, data migrations, complex calculations, and other processing tasks that take more than a few seconds to complete.</p> <p>Key Requirements: - Execute operations that may take minutes to complete - Provide immediate acknowledgment to users when operations start - Deliver real-time progress updates during execution - Support operations across multiple modules (file uploads, validations, data processing, etc.) - Maintain security through proper authentication - Enable horizontal scaling for increased load</p> <p>Constraints: - HTTP request timeout limitations (typically 30-60 seconds) - Need for immediate user feedback while processing continues - Browser connection limitations for long-polling approaches - Resource cleanup requirements for failed operations - User expectation of real-time progress visibility</p> <p>Options Considered: 1. Synchronous HTTP-only processing - Simple but causes timeouts on long operations 2. HTTP with polling - Requires constant client requests, inefficient and increases server load 3. Server-Sent Events (SSE) - One-way communication, limited browser support, connection management complexity 4. Hybrid HTTP/WebSocket with queue-based processing - Balances immediate feedback with async processing and bidirectional communication</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#decision","title":"Decision","text":"<p>We will implement a Long-Running Operations (LRO) pattern using a hybrid HTTP/WebSocket architecture with asynchronous processing via BullMQ for any operation that exceeds typical HTTP timeout thresholds.</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#core-pattern-components","title":"Core Pattern Components","text":"<ol> <li>HTTP Layer (Synchronous Initiation)</li> <li>Endpoint Pattern: <code>POST /{module}/{operation}</code></li> <li>Protocol: HTTP (REST)</li> <li>Responsibility: Initiate operation, perform fast validations, return operation ID</li> <li> <p>Timeout: &lt; 5 seconds for initial response</p> </li> <li> <p>WebSocket Layer (Asynchronous Communication)</p> </li> <li>Protocol: WebSocket with Socket.IO</li> <li>Authentication: JWT token-based</li> <li>Responsibility: Real-time progress updates, completion notifications, error reporting</li> <li> <p>Connection Model: Persistent, bidirectional</p> </li> <li> <p>Queue Layer (Background Execution)</p> </li> <li>Technology: BullMQ with Redis</li> <li>Responsibility: Execute long-running operations asynchronously</li> <li>Job Model: Parent/child job hierarchy for complex operations</li> </ol>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#generic-lro-workflow","title":"Generic LRO Workflow","text":"<pre><code>Client Request (HTTP)\n   \u2193\n[Fast Validation &amp; Setup]\n   \u2193\nOperation ID Generated \u2500\u2500\u2500\u2500\u2500\u2500\u25ba Immediate HTTP Response (200 OK)\n   \u2193\nQueue Job Created\n   \u2193\n[Background Processing]\n   \u2193\nProgress Updates \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba WebSocket Events\n   \u2193\nCompletion/Failure \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba WebSocket Final Event\n   \u2193\n[Resource Cleanup]\n</code></pre>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#pattern-application-examples","title":"Pattern Application Examples","text":"<p>File Upload &amp; Processing: - HTTP: Accept file, validate format, return upload ID - Queue: Parse file, validate data, process batches - WebSocket: Batch progress, validation results, completion summary</p> <p>Bulk Data Validation: - HTTP: Accept validation request, return validation ID - Queue: Execute validation rules across datasets - WebSocket: Progress percentage, rule violations, final report</p> <p>Data Migration/Transformation: - HTTP: Initiate migration, return migration ID - Queue: Transform records, handle dependencies - WebSocket: Records processed, errors encountered, completion status</p> <p>Complex Calculations: - HTTP: Submit calculation request, return calculation ID - Queue: Execute computation, handle intermediate results - WebSocket: Calculation progress, partial results, final output</p>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#reasoning","title":"Reasoning","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#decoupling-scalability","title":"Decoupling &amp; Scalability","text":"<ul> <li>Decouples client from processing lifecycle: HTTP response returns immediately, allowing users to continue working while operation executes</li> <li>Prevents timeout cascades: Long operations don't hold HTTP connections open, avoiding timeout failures and connection pool exhaustion</li> <li>Horizontal scaling capability: Queue workers can be added independently of API servers, allowing targeted scaling for processing bottlenecks</li> <li>Load distribution: Redis-backed queue distributes work across multiple workers automatically</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#user-experience","title":"User Experience","text":"<ul> <li>Immediate acknowledgment: Users get instant feedback that their operation was accepted</li> <li>Real-time visibility: Continuous progress updates reduce anxiety and uncertainty during long operations</li> <li>Resumable operations: Users can disconnect and reconnect; progress state is maintained server-side</li> <li>Detailed feedback: Granular progress and error information helps users understand what's happening</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#reliability-resilience","title":"Reliability &amp; Resilience","text":"<ul> <li>Automatic retry mechanisms: BullMQ provides built-in retry logic for transient failures</li> <li>Job persistence: Operations survive server restarts; Redis stores job state</li> <li>Error isolation: Failed operations don't crash the API server</li> <li>Resource cleanup: Systematic cleanup on success or failure prevents resource leaks</li> <li>Observability: Operation IDs correlate events across all system layers</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#developer-experience","title":"Developer Experience","text":"<ul> <li>Consistent pattern: Same architecture applies to all LROs across modules</li> <li>Clear separation of concerns: HTTP for initiation, WebSocket for communication, Queue for execution</li> <li>Testable components: Each layer can be tested independently</li> <li>Reusable infrastructure: Queue workers, WebSocket handlers, and HTTP controllers follow standard patterns</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#trade-offs-accepted","title":"Trade-offs Accepted","text":"<ul> <li>Increased system complexity: Three-layer architecture requires more infrastructure than simple synchronous approach</li> <li>Eventual consistency: Brief delay between initiation and completion</li> <li>Infrastructure dependencies: Redis and WebSocket server required</li> <li>Connection management overhead: Must handle WebSocket lifecycle (connect, disconnect, reconnect)</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#consequences","title":"Consequences","text":""},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#positive","title":"Positive","text":"<ul> <li>Handles operations of any duration: No artificial limits imposed by HTTP timeouts</li> <li>Production-ready scalability: Proven pattern handles high load with horizontal scaling</li> <li>Enhanced user experience: Users stay informed throughout operation lifecycle</li> <li>Flexible application: Pattern works for uploads, validations, migrations, calculations, and more</li> <li>Operational visibility: Complete tracing from initiation through completion</li> <li>Clean architecture: Well-defined boundaries between layers</li> <li>Graceful degradation: Operations continue even if WebSocket temporarily disconnects</li> </ul>"},{"location":"Front%20End%20ADR%27s/LRO%20Long%20Running%20Operations/#negative","title":"Negative","text":"<ul> <li>Infrastructure complexity: Requires Redis, WebSocket server, and queue workers</li> <li>Development overhead: More moving parts to develop, test, and debug</li> <li>Operational burden: More services to monitor, maintain, and troubleshoot</li> <li>Learning curve: Developers must understand async patterns and queue mechanics</li> <li>Local development complexity: Full stack requires multiple services running locally</li> <li>Network reliability dependency: WebSocket connections can be disrupted by network issues</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/","title":"App Reports Structure","text":"<p>Title: Legacy AS/400 Reporting System Modernization Date: 2025-06-27 Status: In Progress  </p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#context","title":"Context","text":"<p>This decision is about how we implement global reporting structure in the new application while integrating with legacy system. We are modernizing a legacy AS/400 (IBM i) system that currently uses SpoolFlex, an add-on application providing a 5250 terminal interface for spool file management. The existing system allows users to view/work with AS/400 spool files, convert to PDF/Excel formats, email reports, save to shared drives, and automate report generation through pre-defined jobs. </p> <p>The modernization effort aims to replace the outdated 5250 terminal interface completely with a modern web-based user interface while maintaining equivalent functionality.</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#decision","title":"Decision","text":"<p>We will implement a hybrid reporting architecture that bridges the legacy AS/400 system with modern web technologies. We will keep all existing functionality in-place and simply wrap the current system. This structure will also support our second phase of re-writing the existing RPG reports in a modern framework within our Node.js REST API backend.</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#core-components","title":"Core Components:","text":"<ol> <li>Tabular Report Log SQL Table - Central metadata repository:</li> <li> <p>Fields: ReportName, CreatedBy, Timestamp,FileName, LocationPath.</p> </li> <li> <p>Shared Drive Storage - File system location for converted report outputs</p> </li> <li> <p>*Note: the client and the applications moving reports to the share will need the appropriate security access.</p> </li> <li> <p>Legacy System Wrapper Program - Bridge component that accepts parameters (SpoolFileLocation, User, OutputPath, ConversionType), retrieves spool files, converts to desired format, generates timestamped filenames, saves to output path, and records metadata in SQL table</p> </li> <li> <p>Modern Backend API - RESTful service with GET endpoints for report log retrieval, filtering by report type/user permissions, and shared/private access control</p> </li> <li> <p>Web Client Interface - Browser-based UI component that displays available reports and opens them directly from shared drive locations. This should allow user to open the file folder location, open the report in a separate window, or popup download the report from the browser.</p> </li> </ol>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#reasoning","title":"Reasoning","text":""},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#report-log-benefits","title":"Report Log Benefits","text":"<p>The centralized report log provides a scalable foundation for future modernization. As we gradually migrate reports from the legacy system to native backend implementations, we can maintain the same process: write to the report log and save files to shared drive locations. This consistent approach ensures:</p> <ul> <li>Unified reporting interface regardless of report source (legacy or modern)</li> <li>Error tracking capabilities - failed report generations can be logged with error details</li> <li>Audit trail for all report activities across the entire system</li> <li>Seamless migration path without disrupting user workflows</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#alternatives-considered","title":"Alternatives Considered","text":"<p>Alternative 1: Direct File System Access - Pros: Users work directly with output files in storage locations - Cons: Requires page-specific mappings to share drive locations (not scalable), vulnerable to storage location changes, no global repository of report activities - Rejected: Duplicates Windows File Explorer functionality without added value</p>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#consequences","title":"Consequences","text":""},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#positive","title":"Positive","text":"<ul> <li>Gradual migration path without operational disruption</li> <li>Preserved functionality with modern user experience</li> <li>Scalable architecture supporting future report implementations</li> <li>Centralized error handling and audit capabilities</li> <li>Flexible output formats (PDF, HTML, CSV)</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#negative","title":"Negative","text":"<ul> <li>Maintains dependency on legacy AS/400 system</li> <li>Additional system complexity and potential failure points</li> <li>Ongoing shared drive storage management requirements</li> </ul>"},{"location":"REST%20API%20ADR%27s/Reporting%20System%20Modernization/#implementation-notes","title":"Implementation Notes","text":"<ol> <li>Implement Report Log SQL table and wrapper program</li> <li>Build RESTful backend service with filtering capabilities</li> <li>Develop web client interface</li> <li>Migrate automated jobs and optimize performance</li> </ol> <p>Review Date: Now</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/","title":"RPG Program and Database File Development Standards","text":"<p>Title: RPG Program and Database File Development Standards Date: 2025-07-14 Status: In Progress  </p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#context","title":"Context","text":"<p>Architectural Problem: Our current RPG application system relies on temporary flat files that need to be converted to externally defined permanent tables as part of our migration to a Power 10 server architecture. The existing system uses two distinct types of temporary files that require different handling approaches.</p> <p>Key Factors and Constraints: - Migration from temporary flat files to permanent database tables - Two types of temporary files requiring different conversion strategies:   1. Job-scoped tables (implementation approach TBD)   2. User-scoped temporary files (specific conversion requirements defined) - Multi-environment deployment across Power 8 (dev/test/UAT) and Power 10 (production) servers - Need for proper library management and naming conventions - Requirement to maintain data isolation and security</p> <p>Options Considered: - Continue with temporary files (rejected due to data persistence needs) - Convert all temporary files to permanent tables with same structure (rejected due to security concerns) - Implement differentiated approach based on file scope (selected)</p> <p>System Context: IBM i (AS/400) environment with RPG programs accessing database files across multiple environments, with production running on Power 10 and lower environments on Power 8.</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#decision","title":"Decision","text":"<p>Architectural Decision: Implement a tiered database architecture with externally defined permanent tables, environment-specific library management, and differentiated handling of temporary file types.</p> <p>Implementation Approach:</p> <p>User-Scoped Temporary Files Conversion: - Convert to externally defined permanent tables - Add USER_NAME field to enable user-based data isolation - Store physical files in Working Data Library with \"G\" prefix - Create logical files/views in Data Library without prefix for RPG program access - Require user filtering in all INSERT/DELETE operations</p> <p>Job-Scoped Tables: - Status: To Be Determined - Permanent data sorts will be replaced with logical files (SQL views)</p> <p>Library Structure by Environment:</p> Environment Data Library Working Data Library Stored Procedure Library Server Development DATADEV QS36FDEV GSSLIBDEV Power 8 Testing DATATEST QS36FTEST GSSLIBTEST Power 8 UAT DATAUAT QS36FUAT GSSLIBUAT Power 8 Production DATA QS36F GSSLIB Power 10 <p>Technical Requirements: - All database files must be externally defined - RPG programs reference logical files in Data Library only - Physical files stored in Working Data Library with \"G\" prefix - User filtering mandatory for user-scoped table access</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#reasoning","title":"Reasoning","text":"<p>Data Persistence and Integrity - Permanent tables provide data persistence beyond job/session scope - Externally defined tables ensure better data integrity and documentation - Structured approach to user data isolation improves security</p> <p>Maintainability and Scalability - Logical files provide abstraction layer for easier future modifications - Environment-specific libraries enable proper deployment control - Standardized naming conventions improve system maintainability - Power 10 architecture provides improved performance and capacity</p> <p>Security and Data Isolation - User-based filtering prevents cross-user data access - Separate library structure ensures environment isolation - Controlled access patterns through logical file abstraction</p> <p>Trade-offs Considered: - Increased storage requirements vs. improved data persistence - Additional development effort vs. long-term maintainability benefits - User filtering complexity vs. enhanced security</p> <p>Risk Mitigation: - Phased approach allows for testing and refinement - Deferred decision on job-scoped tables reduces immediate complexity - Environment separation minimizes deployment risks</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#consequences","title":"Consequences","text":""},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#positive","title":"Positive","text":"<ul> <li>Data Persistence: User-scoped data persists beyond individual sessions</li> <li>Improved Data Management: Externally defined tables provide better integrity and documentation</li> <li>Environment Isolation: Separate libraries ensure clean environment management and deployment control</li> <li>Enhanced Performance: Power 10 server provides improved performance and capacity for production</li> <li>Better Maintainability: Logical files provide abstraction layer for easier future modifications</li> <li>Enhanced Security: User-based filtering prevents unauthorized data access</li> </ul>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#negative","title":"Negative","text":"<ul> <li>Development Effort: All programs referencing temporary files require modification for user filtering</li> <li>Storage Requirements: Permanent tables consume more storage than temporary files</li> <li>Increased Complexity: Additional user filtering logic required in all affected programs</li> <li>Migration Risk: Potential for data inconsistency during transition period</li> <li>Cross-Server Complexity: Different server architectures for development vs. production environments</li> </ul>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#implementation-notes","title":"Implementation Notes","text":"<p>User-Scoped Table Conversion Process: 1. Create physical table in Working Data Library with \"G\" prefix (QS36FDEV/QS36FTEST/QS36FUAT/QS36F) 2. Add USER_NAME field to table structure 3. Create corresponding logical file/view in Data Library (DATADEV/DATATEST/DATAUAT/DATA) 4. Identify all programs referencing the temporary file 5. Modify programs to include user filtering logic 6. Test data access and filtering functionality 7. Deploy to appropriate environment libraries following promotion process</p> <p>RPG Program Modification Pattern: <pre><code>// Before (temporary file access)\nREAD TEMPFILE;\n\n// After (permanent table with user filtering)\nCLEAR CUSTFILE;\nUSER_NAME = %USERID();\nSETLL (USER_NAME) CUSTFILE;\nREAD CUSTFILE;\n</code></pre></p> <p>Library Usage Guidelines: - Data Library: Contains logical files/views (non-prefixed names) that RPG programs reference - Working Data Library: Contains physical tables with \"G\" prefix for actual data storage - Stored Procedure Library: Contains database stored procedures and functions</p> <p>Server Infrastructure: - Power 8 Server: Hosts Development, Testing, and UAT environments - Power 10 Server: Hosts Production environment only - Cross-Server Considerations: Development and testing occur on Power 8, with final deployment to Power 10 for production</p> <p>Deployment Process: - New programs must be developed with the new library structure - Existing programs migrated to appropriate environment libraries during deployment - Cross-environment consistency maintained through standardized naming conventions</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#related-decisions","title":"Related Decisions","text":"<p>[To be added as additional ADRs are created]</p>"},{"location":"RPG%20Legacy%20ADR%27s/RPG%20Development%20Standards/#references","title":"References","text":"<p>[To be added as external documentation and standards are referenced]</p> <p>Future Considerations: - Job-scoped table implementation strategy needs definition - Performance monitoring requirements for user-filtered queries - Indexing strategy for USER_NAME fields - Data archiving approach for user-scoped permanent tables - Backup and recovery procedure updates for new table structure</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/","title":"Authentication and Authorization","text":"<p>Title: Authentication and Authorization Security Standards Date: 2025-06-16 Status: Accepted  </p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#context","title":"Context","text":"<p>This decision addresses the approach to managing grid-level data interactions \u2014 specifically filtering, sorting, and pagination \u2014 for data-driven UI components.</p> <p>Two architectural options were considered for handling these features:</p> <p>Client-Side Handling \u2013 Load the dataset once from the API and allow all interactions (filtering, sorting, pagination) to occur entirely in the browser. Server-Side Handling \u2013 Rely on backend API endpoints to execute filtering, sorting, and pagination for every user interaction. The system in question needs to handle datasets that may be large in some cases, but are typically manageable in size after user-driven filtering.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#decision","title":"Decision","text":"<p>The architecture will implement client-side filtering, sorting, and pagination for grid components.</p> <p>To support this:</p> <p>The UI will include filter inputs and a \"Search\" button that users must use to initiate data retrieval. Upon clicking \"Search\", the frontend will send a request to the API including any provided filter criteria. The API will return a page of data (default size of 500 records or fewer) along with offset-based pagination metadata. All grid operations (filtering, sorting, pagination) will then be handled entirely client-side using the retrieved data. In addition:</p> <p>The API will implement standardized filtering and sorting behavior for all relevant endpoints. This ensures consistency and reusability of query logic across the application. Every API endpoint will support: Sorting via query parameters in ascending/descending order. Filtering on text fields (equality) and numeric fields (equality, greater than, less than, and range). If the total number of matching records exceeds the initial page size (default limit = 500), the API will not return an error. Instead, it will return a nextPageUrl that can be used to fetch additional pages of data.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#reasoning","title":"Reasoning","text":""},{"location":"Security%20ADR%27s/Security%20End%202%20End/#performance-and-user-experience","title":"Performance and User Experience","text":"<p>Interactions like filtering and sorting are executed instantly in the browser, eliminating round trips to the server and improving responsiveness. Users control when and how data is retrieved, reducing backend load and improving perceived speed.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#simplicity","title":"Simplicity","text":"<p>Reduces complexity in the frontend-to-backend interaction model. Decreases backend logic for grid state handling. Encourages standardization of query patterns across endpoints.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#scalable-pagination","title":"Scalable Pagination","text":"<p>Using offset and limit enables scalable data retrieval while avoiding hard failures due to record caps. Providing metadata like nextPageUrl and totalRecords gives clients full control if they need to fetch more data explicitly.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#rest-api-capabilities","title":"REST API Capabilities","text":"<p>Even though grid operations are handled client-side after data retrieval, the REST API will support consistent and standardized pagination, filtering, and sorting behavior:</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#filtering","title":"Filtering","text":"<p>Text fields: Supports equality match (eq) Numeric fields: Supports: Equality (eq) Greater than (gt) Less than (lt) Range filtering (e.g., min/max or between operators)</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#sorting","title":"Sorting","text":"<p>Query parameter format: ?sort=field.asc or ?sort=field.desc Multiple sort keys may be supported where applicable.</p>"},{"location":"Security%20ADR%27s/Security%20End%202%20End/#pagination-offset-based","title":"Pagination (Offset-Based)","text":"<p>Default limit: 500 Supports limit and offset parameters The API response includes: ```json {   \"data\": [ / array of records / ],   \"offset\": 0,   \"limit\": 500,   \"totalRecords\": 1342,   \"hasMore\": true,   \"nextPageUrl\": \"/api/items?offset=500&amp;limit=500\",   \"previousPageUrl\": null,   \"filtersApplied\": {     \"status\": \"active\"   },   \"sort\": \"createdAt.desc\" }</p> <p>Edit</p> <p>ADR Template Document  I've created a comprehensive ADR template based on the structure from your example. The template includes all the key sections and provides guidance on what to include in each part:</p> <p>Header section with title, date, and status Context for describing the problem and options considered Decision stating what was chosen Reasoning explaining why (with subsections for different benefit categories) Consequences covering both positive and negative impacts Implementation Notes for technical details Related Decisions for linking to other ADRs References for external sources The template uses placeholder text in brackets to guide you on what content to include in each section. You can customize it further based on your team's specific needs or standards.</p> <p>Architectural Design Record (ADR) Title: [Brief descriptive title of the architectural decision] Date: [YYYY-MM-DD] Status: [Proposed | Accepted | Rejected | Deprecated | Superseded]</p> <p>Context [Describe the situation that requires a decision to be made. Include:]</p> <p>[What architectural problem or question needs to be addressed?] [What are the key factors, constraints, or requirements driving this decision?] [What options were considered?] [Any relevant background information or system context] Decision [State the architectural decision that has been made.]</p> <p>[Include specific details about:]</p> <p>[What approach will be implemented] [Key components or patterns involved] [Any specific technical requirements or constraints] [Implementation details that are crucial to the decision] Reasoning [Explain why this decision was made. Include:]</p> <p>[Benefit Category 1 (e.g., Performance, Maintainability, etc.)] [Specific reasoning points] [How this decision addresses the identified concerns] [Benefit Category 2] [Additional reasoning points] [Trade-offs considered] [Additional Categories as needed] [Other factors that influenced the decision] [Risks mitigated or accepted] Consequences [Describe the positive and negative consequences of this decision]</p> <p>Positive [Benefits gained from this approach] [Problems solved] [Improvements achieved] Negative [Trade-offs or limitations accepted] [New complexities introduced] [Technical debt or future considerations] Implementation Notes [Optional section for specific implementation details, if relevant:]</p> <p>[Technical specifications] [API contracts or interfaces] [Configuration requirements] [Migration considerations] [Example JSON schema, code snippets, or configuration examples can be included here]</p> <p>Related Decisions [Optional section linking to other ADRs:]</p> <p>[ADR-XXX: Related decision that this builds upon] [ADR-YYY: Decision that this supersedes or conflicts with] References [Optional section for external references:]</p> <p>[Documentation links] [Industry standards or patterns referenced] [Research or articles that influenced the decision]</p>"},{"location":"Testing%20ARD%27s/UAT%20Process/","title":"User Acceptance Testing (UAT) Process","text":""},{"location":"Testing%20ARD%27s/UAT%20Process/#objective","title":"Objective","text":"<p>Ensure delivered functionality meets the Business Requirements Document (BRD) and is free from critical defects before go-live.</p>"},{"location":"Testing%20ARD%27s/UAT%20Process/#roles-responsibilities","title":"Roles &amp; Responsibilities","text":"<ul> <li>Business Users \u2013 Execute test cases, report issues, provide feedback.</li> <li>Internal ARG-QA \u2013 Bridge between Users and Consultant; verifies features before UAT, filters bugs, manages communication, and performs triage classification.</li> <li>Consultant QA (DAMCO) \u2013 Logs validated bugs into JIRA, investigates, resolves, and updates status.</li> </ul>"},{"location":"Testing%20ARD%27s/UAT%20Process/#process-flow","title":"Process Flow","text":"<ol> <li> <p>Smoke Test (DAMCO QA)    DAMCO QA verifies the environment is stable and major functionality works. Critical blockers are fixed before proceeding.</p> </li> <li> <p>Pre-UAT Verification (ARG-QA)    ARG-QA performs a quick functional check to confirm:  </p> </li> <li>Feature is accessible in UAT environment  </li> <li>Main buttons/links work  </li> <li>No blocking errors occur  </li> <li> <p>Core workflows are usable for testing  </p> </li> <li> <p>User Testing &amp; Logging Issues    Users execute test scenarios and log all issues (bugs, usability concerns, questions) into a shared OneDrive spreadsheet with:  </p> </li> <li>Steps taken  </li> <li>Expected vs. actual results  </li> <li> <p>Screenshots/data references</p> </li> <li> <p>ARG-QA Review &amp; Triage    ARG-QA reviews entries to:  </p> </li> <li>Remove duplicates  </li> <li>Identify misunderstandings  </li> <li> <p>Classify using the Triage Classification System (see below)</p> </li> <li> <p>UAT Review Meeting </p> </li> <li> <p>ARG-QA and Users meet to confirm classifications and finalize the issue list based on triage categories.</p> </li> <li> <p>Issue Resolution Based on Triage    Issues are handled according to their triage classification:</p> </li> <li>Clear and Communicate Immediately: <ul> <li>Category 1 (User Mis-Interpretation)</li> </ul> </li> <li>Send Directly to DAMCO: <ul> <li>Category 2 (Design Adherence)</li> <li>Category 3 (Clear Defect)</li> </ul> </li> <li> <p>Requires ARG Internal Review: </p> <ul> <li>Category 4 (Business Rule Implementation Error)</li> <li>Category 5 (Enhancement Request)</li> </ul> </li> <li> <p>Bug Resolution &amp; Tracking    DAMCO QA fixes issues, updates JIRA status, and coordinates with ARG-QA for validation.</p> </li> <li> <p>Bug Validation &amp; UAT Sign-Off    ARG-QA and Users re-test resolved items. UAT completes when:  </p> </li> <li>All critical/high defects are fixed and verified  </li> <li>No open medium/low defects block go-live  </li> <li>User sign-off is received</li> </ol>"},{"location":"Testing%20ARD%27s/UAT%20Process/#triage-classification-system","title":"Triage Classification System","text":"Classification Description Action Required 1 - User Mis-Interpretation User misunderstood functionality or process Clear Immediately with user education 2 - Design Adherence System not following approved design specifications Send Directly to DAMCO 3 - Clear Defect Obvious system malfunction or error Send Directly to DAMCO 4 - Business Rule Implementation Error System not implementing business rules correctly Requires ARG Internal Review 5 - Enhancement Request New functionality or improvement request Requires ARG Internal Review"},{"location":"Testing%20ARD%27s/UAT%20Process/#jira-status-tracking","title":"JIRA Status Tracking","text":"JIRA Status Description Required Action In-Progress DAMCO actively working on the issue Monitor progress ARG-QA Input Required DAMCO requires clarification from ARG ARG provides requested information Implemented Fix deployed to UAT environment REQUIRES ACTION: - ARG-QA Re-test and validate Resolved No Implementation Issue resolved without code changes ARG-QA Needs to confirm and mark done Done ARG has validated and closed the issue Update tracking spreadsheet"},{"location":"Testing%20ARD%27s/UAT%20Process/#tools-communication","title":"Tools &amp; Communication","text":"<ul> <li>OneDrive Spreadsheet \u2013 Centralized logging of all user-reported issues with triage classifications.</li> <li>JIRA \u2013 DAMCO QA's bug tracking system with standardized status workflow.</li> <li>QA Meetings / Teams \u2013 Communication between ARG-QA, Users, and DAMCO QA.</li> </ul>"},{"location":"Testing%20ARD%27s/UAT%20Process/#workflow-summary-by-triage-category","title":"Workflow Summary by Triage Category","text":"<p>Category 1 (User Mis-Interpretation): - OneDrive Entry \u2192 ARG-QA Review \u2192 Clear Immediately \u2192 User Communication</p> <p>Categories 2 &amp; 3 (Design Adherence / Clear Defect): - OneDrive Entry \u2192 ARG-QA Review \u2192 Direct DAMCO Submission \u2192 JIRA Tracking \u2192 Resolution \u2192 User Validation</p> <p>Categories 4 &amp; 5 (Business Rule Error / Enhancement):  - OneDrive Entry \u2192 ARG-QA Review \u2192 ARG Internal Review \u2192 Decision \u2192 Potential DAMCO Submission \u2192 JIRA Tracking (if applicable)</p>"}]}